h4. *How do I validate a record with default values filled in?*

```java
Foo foo = new Foo();
ValidationResult result = ValidateDataAgainstSchema.validate(foo.data(), foo.schema(), new ValidationOptions(RequiredMode.FIXUP_ABSENT_WITH_DEFAULT));
assert(result.isValid());
```

This will fail if the underlying data is read-only and default values cannot be set for absent fields.

This will also work for partially filled in records. It will only add default values to fields that are absent.

h4. How do I convert a Pegasus data schema to an Avro data schema

It requires the data-avro module, or data-avro-*.jar.

At runtime, use:

```java
Foo foo = new Foo();
DataSchema pegasusSchema = foo.schema();
org.apache.avro.Schema avroSchema = SchemaTranslator.dataToAvroSchema(pegasusSchema);
String avsoSchemaInJson = avroSchema.toString();
```

From the command line, to create text files with Avro schema from pdsc files (version 0.17.1 or higher):

<pre><code># syntax:
# java [-Dgenerator.resolver.path=<path>] com.linkedin.data.avro.AvroSchemaGenerator <targetDir> [<pdscFile|schemaFullName>]...
java -Dgenerator.resolver.path=src/main/pegasus com.linkedin.data.avro.generator.AvroSchemaGenerator ../build/main/codegen/avro src/main/pegasus/com/linkedin/foo/*.pdsc
# or
java -Dgenerator.resolver.path=src/main/pegasus com.linkedin.data.avro.generator.AvroSchemaGenerator ../build/main/codegen/avro com.linkedin.foo.Foo
</code></pre>

Classpath must be setup to include data-avro.jar and its dependencies.

h4. How do I use a data type embedded inside a pdsc file?

You may experience errors, such as the following:

<pre><code>Type cannot be resolved: 1,1: "a.b.D" cannot be resolved.</code></pre>

when <code>a.b.D</code>'s definition is embedded in another type, for example, <code>a.b.C</code>.

Embedded data types do not have their own pdsc file. As a result, such data type can only be referenced within the containing pdsc file. It can not be referenced externally. To change this behavior, pull out the definition of the internal type to a separate pdsc file.

Internally, this behavior is due to reason that the schema parser references the data types by filenames. pdsc file for <code>a.b.C</code> should be expected at <code>pegasus/a/b/C.pdsc</code>. Since Rest.li does not prefix the containing data type's name to the embedded type's name, when code>a.b.D</code> is embedded in <code>C.pdsc</code>, the schema parser will not be able to find <code>pegasus/a/b/D.pdsc</code>.

h4. Why is my Java 8 build generating all sorts of Javadoc warnings/errors due to doclint?

When using Java 8, you may experience build failures due to the following:

<pre><code>[error] /home/myuser/data-template/src/main/codegen/com/linkedin/PegasusFormSchema.java:192: error: malformed HTML
* return type: java.util.Set<java.lang.Integer>
[error] /home/myuser/data-template/src/main/codegen/com/linkedin/PegasusFormSchema.java:192: error: bad use of '>'
* return type: java.util.Set<java.lang.Integer>
</code></pre>

The root cause is incorrectly formatted string values for anything within the @doc@ or @symbolDocs@ attributes within your pdscs. Due to the addition of doclint in JDK8, anything under the doc or symbolDocs attribute must be W3C HTML 4.01 compliant. This is because the contents of this string will appear as Javadocs in the generated Java ‘data template’ classes later. Please take this into consideration when writing your documentation. 

The alternative is to disable doclint altogether:
<pre><code>  
if (JavaVersion.current().isJava8Compatible()) {
  allprojects {
    tasks.withType(Javadoc) {
      options.addStringOption('Xdoclint:none', '-quiet')
    }
  }
}
</code></pre>

More details on doclint can be found here: "Javadoc has become very strict":http://stackoverflow.com/questions/22528767/jdk8-and-javadoc-has-become-very-strict

More details on @doc@ and @symbolDocs@ can be found here: "Data Schemas and Templates":https://github.com/linkedin/rest.li/wiki/DATA-Data-Schema-and-Templates

h4. How does rest.li full update method work with schema evolution?

There is a potential pitfall to be aware of. Consider the following scenario: 

# Suppose a schema begins on version 0 with a single optional field A. We write a client that sends an update request with the field A populated.
# At some point, we add an optional field B in version 1.
# Our old client still sends a request with A populated, but it is not clear how to interpret its request. From the server's perspective, we cannot distinguish a new client that wants to null out field B (e.g. schema version 1) vs. a client that is not aware of field B (e.g. version 0).