/*
 * This script performs the search functionality of the site.
 * It should only be executed on the actual search page, as executing this
 * will load the entire site's text content into memory.
 */

var index = lunr(function () {
  this.field('title')
  this.field('content', { boost: 10 })
  this.field('category')
  this.field('tags')
  this.ref('id')
});



  

  
    index.add({
      title: "Asynchronous Servers and Clients In Rest.li",
      category: null,
      content: "Asynchronous Servers And Clients In Rest.li\n\nContents\n\n\n  Introduction\n  Async Server Implementations\n  Async Client Implementations\n\n\nThis section assumes you are familiar with ParSeq and its key concepts.\n\nIntroduction\n\nRest.li is asynchronous and non-blocking under the hood:\n\n\n  R2 - On the client side, R2 uses a Netty based asynchronous client. On the server side if you are using our experimental Netty server, it is async and non-blocking. If you are using Jetty, you need to configure it to run in async mode.\n  D2 - All communication with ZooKeeper uses the async APIs.\n  Rest.li - Rest.li does not handle I/O. All I/O work is done by R2, which is async and non-blocking as explained above. Rest.li uses ParSeq to interact with and delegate to server application code. The RestClient used to make Rest.li requests on the client-side has several options in order to write async non-blocking code.\n\n\nAsync Server Implementations\n\nAs shown above the Rest.li framework is async and non-blocking under the hood. R2 uses non-blocking I/O via both Netty and Jetty to serve client traffic using Java NIO. Resource methods are invoked in an event driven fashion.\n\nAs a result of this, if you do any blocking work in your method implementation, it can negatively impact your application throughput as threads are held up by your application that are needed by Rest.li (if you enable async mode in your Rest.li server). It is the responsibility of application developers to ensure that their method implementations are async and use non-blocking I/O.\n\nThere are two main options available to write async server implementations in Rest.li:\n\n\n  Using a com.linkedin.common.Callback (included in Rest.li)\n  Using ParSeq\n\n\nAsync Server Templates\n\nRest.li also includes templates to make writing async resources slightly easier. There are templates that use Callbacks and Tasks for each type of Rest.li resource. For example, for collection resources with primitive keys we have com.linkedin.restli.server.resources.CollectionResourceAsyncTemplate and com.linkedin.restli.server.resources.CollectionResourceTaskTemplate. There are similar templates for complex key resources, association resources, and simple resources.\n\nServer Configuration\n\nAsync request handling is available for Servlet API 3.0 or greater and is enabled by default for 3.0+.\n\nTo us async mode in Servlet containers, async-supported must be set to true in your web.xml.\n\nExample:\n\n&lt;servlet&gt;\n  ...\n  &lt;async-supported&gt;true&lt;/async-supported&gt;\n  ...\n&lt;/servlet&gt;\n\n\nHere is an example configuration for our quick start example server.\n\nAsync can be further configured by the Rest.li uscAsync servlet param (which defaults to true for servlet API 3.0+ servlet containers). The asyncTimeout param can be set to a desired maximum timeout value in milliseconds.\n\nExample:\n\n&lt;init-param&gt;\n  &lt;param-name&gt;useAysnc&lt;/param-name&gt;\n  &lt;param-value&gt;true&lt;/param-value&gt;\n&lt;/init-param&gt;\n&lt;init-param&gt;\n  &lt;param-name&gt;asyncTimeout&lt;/param-name&gt;\n  &lt;param-value&gt;30000&lt;/param-value&gt;\n&lt;/init-param&gt;\n\n\nUsing Callbacks\n\nConsider the following implementation of an async GET method using a Callback. In this example, we fetch data from ZooKeeper asynchronously. Based on the data that we get back, we either return a 404 to the user or build a Greeting RecordTemplate. We will use this example to understand how to write Callback based Rest.li async method implementations.\n\nExample:\n\n@RestMethod.Get\npublic void get(final Long id, @CallbackParam final Callback&lt;Greeting&gt; callback) {\n  String path = \"/data/\" + id;\n  // _zkClient is a regular ZooKeeper client\n  _zkClient.getData(path, false, new DataCallback() {\n    public void processResult(int i, String s, Object o, byte[] b, Stat st) {\n      if (b.length == 0) {\n        callback.onError(new RestLiServiceException(HttpStatus.S_404_NOT_FOUND));\n      }\n      else {\n        callback.onSuccess(buildGreeting(b));\n      }\n    }\n  }, null);\n}\n\n\nSignature\n\n@RestMethod.Get\npublic void get(final Long id, @CallbackParam final Callback&lt;Greeting&gt; callback) {\n\n\nIn order to use a Callback, we need to set the return type of the function to be void and pass in a Callback&lt;T&gt; as a parameter to the function. T here is whatever type you would have returned from a synchronous implementation of the same function. In this case, the synchronous implementation would have returned a Greeting, which is why we are returning a Callback&lt;Greeting&gt; here. The @CallbackParam is required.\n\nFunction Body\n\n_zkClient.getData(path, false, new DataCallback() {\n  public void processResult(int i, String s, Object o, byte[] b, Stat st) {\n    if (b.length == 0) {\n      callback.onError(new RestLiServiceException(HttpStatus.S_404_NOT_FOUND));\n    }\n    else {\n      callback.onSuccess(buildGreeting(b));\n    }\n  }\n}, null);\n\n\nWe use the async ZooKeeper getData API call to fetch data from ZooKeeper. Based on the data we get back from ZooKeeper in the DataCallback (which is a ZooKeeper construct), we invoke either the onError or the onSuccess method on the Callback interface.\n\nonError is used to signify that something went wrong. In this case, we invoke onError with a RestliServiceException when the length of data that we get back from ZooKeeper is 0. The Rest.li framework translates this Exception into an appropriate REST response to send back to the client.\n\nIn case we get back data that has non-zero length, we build a Greeting object from it in the buildGreeting method (not shown here) and return that to the client by invoking the onSuccess method.\n\nAll you have to do is invoke the onError or onSuccess method within your method, and the Rest.li framework will return data back to the client. This is why the return type for the method is void, since the Callback is used to return values back to the client.\n\nCallback Execution\n\nIt is up to the application developer to execute the Callback. Rest.li does not execute the Callback for you. In the above example, the Callback was executed by the ZooKeeper thread, which is why we didn’t have to explicitly execute it. However, once the onError or onSuccess method has been invoked in the Callback, Rest.li will translate that into a response to send back to the client. In other words, Callback execution is up to the application developer, but sending a response back once the Callback has been executed is handled by Rest.li.\n\nUsing ParSeq\n\nConsider the following example of an async GET implementation that uses ParSeq:\n\n@RestMethod.Get\npublic Task&lt;Greeting&gt; get(final Long id) {\n  final Task&lt;FileData&gt; fileDataTask = buildFileDataTask();\n  final Task&lt;Greeting&gt; mainTask = Tasks.callable(\"main\", new Callable&lt;Greeting&gt;() {\n    @Override\n    public Greeting call() throws Exception {\n      FileData fileData = fileDataTask.get();\n      return buildGreetingFromFileData(id, fileData);\n    }\n  });\n  return Tasks.seq(fileDataTask, mainTask);\n}\n\n\nbuildFileDataTask (implementation not shown here) reads some file on disk using async I/O and returns a Task&lt;FileData&gt;, where FileData (implementation not shown here) is some abstraction for the data being read. We use FileData to build a Greeting to return to the client.\n\nSignature\n\n@RestMethod.Get\npublic Task&lt;Greeting&gt; get(final Long id) {\n\n\nIn order to use ParSeq the function implementation must return a ParSeq Task&lt;T&gt;. T here is whatever type you would have returned from a synchronous implementation of the same function. In this case the synchronous implementation would have returned a Greeting, which is why we are returning a Task&lt;Greeting&gt; here.\n\nFunction Body\n\nfinal Task&lt;FileData&gt; fileDataTask = buildFileDataTask();\nfinal Task&lt;Greeting&gt; mainTask = Tasks.callable(\"main\", new Callable&lt;Greeting&gt;() {\n  @Override\n  public Greeting call() throws Exception {\n    FileData fileData = fileDataTask.get();\n    return buildGreetingFromFileData(id, fileData);\n  }\n});\nreturn Tasks.seq(fileDataTask, mainTask);\n\n\nThe basic idea to use ParSeq for Rest.li async method implementations is to return a Task.\n\nfileDataTask is a Task for the FileData that we read from disk. We want to transform this FileData into a Greeting to return to the user. We define a new Task, called mainTask in the example above, to do this.\n\nWithin the call method of mainTask, we obtain the FileData from the fileDataTask. This is a non-blocking call because of the way we assemble our final call graph (more on this in a bit). Finally, we build a Greeting in the buildGreetingFromFileData (implementation not shown here) method.\n\nSo we have two Tasks now, fileDataTask and mainTask, with mainTask depending on the result of fileDataTask. mainTask also builds the Greeting object that we want to return to the client. In order to build this dependency between the two Tasks, we use the Tasks.seq method.\n\nTask Execution\n\nIt is suggested to return a Task from your method. In the above example, Task.seq(fileDataTask, mainTask) returns a new Task that is executed for you automatically using the ParSeq engine within Rest.li. In other words, you do not have to provide a separate ParSeq execution engine to run this Task. Rest.li runs the Task for you and returns an appropriate response to the client.\n\nPromise Execution\n\nEven though the example above returns a Task, it is worth pointing out the difference between returning a Task and returning a Promise from an async method implementation. If you return a Promise from a resource method, Rest.li creates a Task for you to resolve the Promise. This Task is then run on the underlying ParSeq engine within Rest.li.\n\nOne thing to keep in mind is that if you are returning a Task from your method you should never return it as a Promise! This is because the Rest.li framework will wrap this Promise in a new Task to execute it, and thus two ParSeq Tasks are created. In our performance benchmarks, we have noticed that this one extra Task can lead to a noticeable increase in latency, especially in high QPS scenarios.\n\nPromise or Task?\n\nIf you are using ParSeq, you can return either a Promise or a Task from your resource method implementation. In general, you should return a Task. The Promise templates are deprecated and should not be used for new development.\n\nAsync Client Implementations\n\nThere are two main options available to make async requests using Rest.li:\n\n\n  Using a com.linkedin.common.Callback\n  Using a com.linkedin.restli.client.ParSeqRestClient (included in Rest.li)\n\n\nUsing Callbacks\n\nHere is a partial example of making a GET request to the /greetings resource and then using the result asynchronously:\n\nCallback&lt;Response&lt;Greeting&gt;&gt; cb = new Callback() {\n  void onSuccess(Response&lt;Greeting&gt; response) {\n    // do something with the returned Greeting\n  }\n  void onError(Throwable e) {\n    // whoops\n  }\n}\n\nRequest&lt;Greeting&gt; getRequest = BUILDERS.get().id(1L).build();\n\n_restClient.sendRequest(getRequest, new RequestContext(), cb);\n\n\n\nDefining the Callback\n\nCallback&lt;Response&lt;Greeting&gt;&gt; cb = new Callback() {\n  void onSuccess(Response&lt;Greeting&gt; response) {\n    // do something with the returned Greeting\n  }\n  void onError(Throwable e) {\n    // whoops\n  }\n}\n\n\nWe need to define the Callback that will be executed when we get back a Response from the server. onSuccess will be invoked by Rest.li on getting a non-exception result (i.e. a Response), while onError will be invoked by Rest.li in case an Exception is thrown. The key concept to note here is that Rest.li invokes the Callback from you once we get a Response or an Exception is thrown.\n\nSending the Request\n\nRequest&lt;Greeting&gt; getRequest = BUILDERS.get().id(1L).build();\n_restClient.sendRequest(getRequest, new RequestContext(), cb);\n\n\nWe pass the Callback we defined previously as the last parameter of the sendRequest call. This calls returns right away, because as stated above, Rest.li invokes the Callback for you appropriately.\n\nUsing a ParSeqRestClient\n\nA ParSeqRestclient is simply a wrapper around the standard RestClient that returns a ParSeq Task or Promise. Here is an example that uses a ParSeqRestClient to make two asynchronous Rest.li requests in parallel and then print out the results:\n\nRequest&lt;Greeting&gt; greetingsRequest = GREETINGS_BUILDERS.get().id(1).build();\nRequest&lt;Fortune&gt; fortunesRequest = FORTUNES_BUILDERS.get().id(1).build();\n\nTask&lt;Response&lt;Greeting&gt;&gt; greetingsResponseTask = _parseqRestClient.createTask(greetingsRequest);\nTask&lt;Response&lt;Fortune&gt;&gt; fortunesResponseTask = _parseqRestClient.createTask(fortunesRequest);\n\nTask&lt;Void&gt; printTask = Tasks.callable(\"printTask\", new Callable&lt;Void&gt; {\n  @Override\n  public Void call() throws Exception {\n    Greeting greeting = greetingsResponseTask.get().getEntity();\n    Fortune fortune = fotunesResponseTask.get().getEntity();\n    System.out.println(greeting + \", \" + fortune);\n    return null;\n  }\n});\n\n_engine.run(Tasks.seq(Tasks.par(greetingsResponseTask, fortunesResponseTask), printTask));\n\n\nTask Creation\n\nThe ParSeqRestClient includes APIs to get back a Task or Promise corresponding to the Response of a Request. In the example above, we build out a Request for a Greeting and Fortune using the standard generated builders. We then use the createTask API to get back a Task&lt;Response&lt;Greeting&gt;&gt; and Task&lt;Response&lt;Fortune&gt;&gt;. This does not send the request out over the network! It simply gives you back a Task, which, when run on a ParSeq engine, would give you back a Response&lt;Greeting&gt; or Response&lt;Fortune&gt;.\n\nTo print out the result we get back from the server, we define a third Task named printTask. In this Task, we get the Greeting and Fortune from the previous Tasks and print it out.\n\nBuilding the Call Graph and Running the Tasks\n\nNow that we have the Tasks that we want to run, we need to build the call graph and execute the Tasks on a ParSeq engine.\n\nWe want to make our Rest.li requests in parallel. To do this we use the Tasks.par method to run greetingsResponseTask and fortunesResponseTask in parallel. We want to run printTask after both these Tasks have completed, which is why we use the Tasks.seq method Tasks.seq(Tasks.par(greetingsResponseTask, fortunesResponseTask), printTask). This gives us the final Task that we run on our ParSeq engine using the run method.\n\nUsing Promises\n\nParSeqRestClient also has a Promise based API. It is very similar to the Callback based approach of sending a request. The main difference is that instead of passing in a Callback to the sendRequest call we attach a PromiseListener that is invoked asynchronously when we get a result from the server. Here is the Callback example implemented using the Promise API:\n\nRequest&lt;Greeting&gt; getRequest = BUILDERS.get().id(1L).build();\n\n// this call sends the request over the wire and returns a Promise\nPromise&lt;Response&lt;Greeting&gt;&gt; greetingPromise = _parseqRestClient.sendRequest(getRequest);\n\ngreetingPromise.addListener(new PromiseListener&lt;Response&lt;Greeting&gt;&gt;() {\n  @Override\n  public void onResolved(Promise&lt;Response&lt;Greeting&gt;&gt; promise) {\n    if (promise.isFailed()) {\n      Throwable t = promise.getError();\n      // something went wrong\n    }\n    else {\n      Response&lt;Greeting&gt; response = promise.get();\n      // do something with the returned Greeting\n    }\n  }\n});\n\n",
      tags: null,
      id: 0
    });
    
    

  
    index.add({
      title: "Attachment Streaming",
      category: null,
      content: "Attachment Streaming\n\nThis is an experimental feature at this point. Please consult Rest.li\nteam before using it.\n\nContents\n\n\n  Goals\n  Inspiration\n  New Wire Format\n  Data Modeling\n  Creating Attachments\n  Reading Attachments\n  Chaining (Proxying) Attachments Across Services\n  Client Streaming APIs\n  Server Streaming APIs\n  Good Programming Practice\n  Additional Developer Notes\n  Attachment Streaming without Rest.li\n  Future Enhancements\n\n\nGoals\n\nRest.li is the high performance platform on which web services operate\nat LinkedIn. As our company moved closer to the formal adoption of large\nunstructured blobs of data, such as media, we needed a highly performant\nway to move all this data around. Therefore it became apparent that we\nneeded to perform an overhaul of our existing service-to-service\narchitecture.\n\nThe goals of Rest.li attachment streaming therefore are the following:\n\n\n  Provide the ability to pass large blobs of bytes around our data\ncenters between multiple services seamlessly\n  No one service should hold the entire payload in memory at once\n  Fully asynchronous and event driven\n  Zero copy write and read for high performance\n  Leverage existing web services infrastructure, namely R2/D2/Rest.li\n  Allow multiple blobs to be sent in a single request or response\n  Establish a wire format that can foster a high adoption rate for\nexternal members and platforms\n  Solve the immediate business need of our services interacting with\nour custom distributed object store - LinkedIn’s version of S3 -\nAmbry\n  Provide clean and intuitive async APIs for our engineers\n\n\nInspiration\n\nRest.li attachment streaming is inspired by the Reactive Streaming\nManifesto\n\nTherefore attachment streaming in Rest.li, from the bottom (R2) to the\ntop, is based on the following:\n\n\n  Allow processing a potentially unbounded number of elements\n  Ensure that data elements are handled in sequence\n  Asynchronously pass elements between components\n  Mandatory non-blocking backpressure from the bottom (TCP) up\n  The reader should never be forced to buffer data\n\n\nNew Wire Format\n\nIn order to support attachment streaming, Rest.li leverages a different\nwire format then what traditional Rest.li traffic uses. This is for a\nnumber of reasons:\n\n\n  The current wire formats JSON and PSON do not work well for\nexpressing binary attachments. They were meant for structured data,\nso providing big blobs of bytes in them violates the spirit of their\noriginal intent.\n  Multiple attachments need to be supported, which is not possible in\nthe current state of Rest.li.\n  The regular Rest.li payload (JSON/PSON) needs to continue to be\nsupported and to be fully read/written before attachments are\nread/written by developer code.\n  Arbitrarily large attachments need to be supported. This requires a\nformat that is welcoming to very large blobs of bytes.\n  Each attachment needs some metadata associated with it as well. This\nis outside of the regular Rest.li payload.\n  It is desirable to have a wire format that is set up for easy\nadoption by external consumers and platforms.\n\n\nIn order to satisfy these requirements, Rest.li makes use of\nmultipart/mime as the streaming wire format. Therefore\nRest.li attachment streaming’s implementation is build upon the formal\nadoption of the RFC.\n\nWithin multipart/mime there are several mime\nsubtypes that can be chosen. Rest.li attachment streaming uses\nmultipart/related as the value for the\nContent-Type header since the JSON/PSON payload will be the\nfirst part in the mime envelope. A more detailed explanation as to the\njustification of multipart/related is explained\nhere.\n\nSample Traditional Rest.li Wire Format\n\nFor reference, here are examples of sample regular Rest.li payloads.\n\nPOST /widgets?action=purge HTTP/1.1\nContent-Type: application/json\n\n{\n  \"reason\": \"spam\",\n  \"purgedByAdminId\": 1\n}\n\n\nPOST /widgets?ids=List(1,2) HTTP/1.1\nContent-Type: application/json\nX-RestLi-Method: BATCH_PARTIAL_UPDATE\n\n{\n  \"entities\": {\n    \"1\": {\"patch\": { \"$set\": { \"name\":\"Sam\"}}},\n    \"2\": {\"patch\": { \"$delete\": [\"name\"]}}\n  }\n}\n\n\nWire Format for Attachment Streaming\n\nNote that the current wire format described above will still be\nsupported as there are no backward incompatible changes being made.\n\nHowever for clients sending requests with attachments present or for\nservers responding with attachments, the wire protocol will change.\n\nIf attachments are present in either a request or a response, the\ncontent type becomes multipart/related. If a client can\nhandle attachments back from a server, then an accept type of\nmultipart/related is also added to the Accept\nheader.\n\nFor example:\n\nPUT /widgets?ids=List(1,2,3) HTTP/1.1\nX-RestLi-Method: BATCH_UPDATE\nContent-Type: multipart/related; boundary=--km6cltxBQgkYRIwT8lAgFGfNV0AmQFwDB\nAccept: multipart/related; application/json\n\n--km6cltxBQgkYRIwT8lAgFGfNV0AmQFwDB\nContent-Type: application/json\n{\n  \"entities\": {\n    \"1\": {\n      \"widgetName\": \"Trebuchet\",\n      \"myVideo\": \"cid:725c0319-b1f1-4b9c-b618-7ee9468870f0\"\n    },\n    \"2\": {\n      \"widgetName\": \"Gear\",\n      \"myVideo\": \"cid:a4d4133b-0546-4f7b-8104-ffdd644168c6\"\n    }\n    \"3\": {\n      \"widgetName\": \"Slider\",\n      \"myVideo\": \"cid:725c0319-b1f1-4b9c-b618-7ee9468870f0\"\n    }\n  }\n}\n--km6cltxBQgkYRIwT8lAgFGfNV0AmQFwDB\nContent-ID: &lt;725c0319-b1f1-4b9c-b618-7ee9468870f0&gt;\nbinary data...\n--km6cltxBQgkYRIwT8lAgFGfNV0AmQFwDB\nContent-ID: &lt;a4d4133b-0546-4f7b-8104-ffdd644168c6&gt;\nbinary data...\n--km6cltxBQgkYRIwT8lAgFGfNV0AmQFwDB--\n\n\nNote that the regular Rest.li payload becomes the first part in a\nmultipart/related envelope. Each attachment then becomes\nits own subsequent part separated by the multipart boundary. Since each\npart in a multipart envelope can have its own headers, the\nContent-Type header from the regular payload now appears as\na header in the first part. Both JSON and PSON are supported as valid\nContent-Types for the first part.\n\nEach part after the regular Rest.li payload represents a blob of data\nattached to the request or response. Each attachment part has a\nContent-ID header which uniquely identifies that attachment\nin the payload. References to this unique identifier should then be\nplaced as fields in the JSON (RecordTemplate backing) payload as\npointers to the blobs in the attachments. In this particular example we\nhave two attachments. Trebuchet and Slider\nboth point to the same attachment while Gear points to the\nother attachment.\n\nData Modeling\n\nData modeling is no different for streaming, with the exception of the\nfollowing recommendation. This recommendation is simply to serve as a\nvisual cue and does not impact the generated RecordTemplates or the\nprocessing of a streaming request or response.\n\nOur recommendation is that anytime you have a field in a schema\nreferencing an attachment, that an @attachment annotation is present.\nThis should further be expanded to include documentation mentioning that\nthis field represents a pointer to an attachment. Once again it is important\nto note that the purpose of these fields is simply to convey that an\nattachment could be present.\n\nnamespace com.linkedin.greetings.api\n\n/**\n * A greeting\n */\nrecord Greeting {\n  id: long\n\n  /**\n   * Type 1 UUID representing a video attachment\n   */\n  @attachment\n  content: string\n}\n\n\nIn terms of the actual data supplied at runtime we suggest using Type 1\nUUIDs.\n\nTechnical Details on Type 1\nUUIDs\n\nThe reason for suggesting Type 1 UUIDs is because it provides the best\nguarantee of producing a globally unique identifier. This is important\nsince, as shown further below, attachments can be coalesced from\ndifferent machines/services which may lead to an identifier collision.\n\nCreating Attachments\n\nIn order to create an attachment, developers must implement the\nfollowing interface(s):\n\n/**\n * Represents a custom data source that can serve as an attachment.\n */\npublic interface RestLiAttachmentDataSourceWriter extends Writer\n{\n  /**\n   * Denotes a unique identifier for this attachment. It is recommended to choose \n   * identifiers with a high degree of uniqueness, such as Type 1 UUIDs. \n   * For most use cases there should be a corresponding String field in a PDSC\n   * to indicate affiliation.\n   *\n   * @return the {@link java.lang.String} representing this attachment.\n   */\n  public String getAttachmentID();\n}\n\n\nYou’ll notice this extends Writer which is defined as the\nfollowing:\n\n/**\n * Writer is the producer of data for an EntityStream.\n */\npublic interface Writer\n{\n  /**\n   * This is called when a Reader is set for the EntityStream.\n   *\n   * @param wh the handle to write data to the EntityStream.\n   */\n  void onInit(final WriteHandle wh);\n\n  /**\n   * Invoked when it it possible to write data.\n   *\n   * This method will be invoked the first time as soon as data can be written to the WriteHandle.\n   * Subsequent invocations will only occur if a call to {@link WriteHandle#remaining()} has returned 0\n   * and it has since become possible to write data.\n   */\n  void onWritePossible();\n\n  /**\n   * Invoked when the entity stream is aborted.\n   * Usually writer could do clean up to release any resource it has acquired.\n   *\n   * @param e the throwable that caused the entity stream to abort\n   */\n  void onAbort(Throwable e);\n}\n\n\nThe Writer class leverages an interface called\nWriteHandle which is defined as follows:\n\n/**\n * This is the handle to write data to an EntityStream.\n */\npublic interface WriteHandle\n{\n  /**\n   * This writes data into the EntityStream. This call may have no effect if the stream has been aborted\n   * @param data the data chunk to be written\n   * @throws java.lang.IllegalStateException if remaining capacity is 0, or done() or error() has been called\n   * @throws java.lang.IllegalStateException if called after done() or error() has been called\n   */\n  void write(final ByteString data);\n\n  /**\n   * Signals that Writer has finished writing.\n   * This call has no effect if the stream has been aborted or done() or error() has been called\n   */\n  void done();\n\n  /**\n   * Signals that the Writer has encountered an error.\n   * This call has no effect if the stream has been aborted or done() or error() has been called\n   * @param throwable the cause of the error.\n   */\n  void error(final Throwable throwable);\n\n  /**\n   * Returns the remaining capacity in number of data chunks. Always returns 0 if the stream is aborted or \n   * finished with done() or error()\n   *\n   * @return the remaining capacity in number of data chunks\n   */\n  int remaining();\n}\n\n\nThese are the essential interfaces to keep in mind when defining an\nattachment as they represent how your custom data source will be asked\nto produce both the metadata as well as the raw bytes for your\nattachment.\n\nWhen it is time for a RestLiAttachmentDataSourceWriter to\nproduce data, it will first be invoked on\nRestLiAttachmentDataSourceWriter\\#getAttachmentID().\nImplementations should return a unique identifier that should be the\nsame identifier placed in the strongly typed RecordTemplate\npayload as described earlier.\n\nNext, the attachment will be invoked on\nonInit(WriteHandle). The provided WriteHandle\nis the object that will be used to perform the actual writing of bytes\nlater, so implementations should save a reference to it.\n\nSubsequently, at some point in time in the future, the attachment will\nbe invoked on Writer\\#onWritePossible(). It is at this\npoint that implementations should write raw bytes on\nWriteHandle\\#write(ByteString). The amount of times that\nthe writer may write will be based on what is returned from\nWriteHandle\\#remaining(). Once the number of writes\nremaining has been honored, then again at some time in the future, the\nattachment will be invoked again on\nWriter\\#onWritePossible(). Then the attachment simply\nrepeats the logic above. The Javadoc is clear about this behavior for\ndevelopers to follow.\n\nThe size of the chunk written is up to the developer but keep in mind\nthat the larger the chunks written, the more memory that may be used by\nthe application at any given time. Furthermore, in order to minimize\ncopies, developers should use\nByteString\\#unsafeWrap(byte\\[\\]) to wrap byte arrays that\nneed to be written out.\n\nReading Attachments\n\nReading attachments is a multi-step callback driven process which allows\ndevelopers to asynchronously walk through each attachment. It begins\nwith a top level RestLiAttachmentReader and a\nSingleRestLiAttachmentReader for each individual attachment\nencountered. This applies whether a client is reading a server’s\nresponse attachments or a server reading a client’s incoming request\nattachments.\n\nThere are two callbacks involved, one for the\nRestLiAttachmentReader and one for the\nSingleRestLiAttachmentReader. The relevant interfaces are\nas follows:\n\n/**\n * Used to register with {@link com.linkedin.restli.common.attachments.RestLiAttachmentReader}\n * to asynchronously drive through the reading of multiple attachments.\n */\npublic interface RestLiAttachmentReaderCallback\n{\n  /**\n   * Invoked (at some time in the future) upon a registration with a {@link RestLiAttachmentReader}.\n   * Also invoked when previous attachments are finished and new attachments are available.\n   *\n   * @param singleRestLiAttachmentReader the {@link RestLiAttachmentReader.SingleRestLiAttachmentReader}\n   *                                     which can be used to walk through this attachment.\n   */\n  public void onNewAttachment(RestLiAttachmentReader.SingleRestLiAttachmentReader singleRestLiAttachmentReader);\n\n  /**\n   * Invoked when this reader is finished which means all attachments have been consumed.\n   */\n  public void onFinished();\n\n  /**\n   * Invoked as a result of calling {@link RestLiAttachmentReader#drainAllAttachments()}.\n   * This will be invoked at some time in the future when all the attachments in this\n   * reader have been drained.\n   */\n  public void onDrainComplete();\n\n  /**\n   * Invoked when there was an error reading attachments.\n   *\n   * @param throwable the Throwable that caused this to happen.\n   */\n  public void onStreamError(Throwable throwable);\n}\n\n\n/**\n * Used to register with\n * {@link com.linkedin.restli.common.attachments.RestLiAttachmentReader.SingleRestLiAttachmentReader}\n * to asynchronously drive through the reading of a single attachment.\n */\npublic interface SingleRestLiAttachmentReaderCallback\n{\n  /**\n   * Invoked when data is available to be read on the attachment.\n   *\n   * @param attachmentData the {@link com.linkedin.data.ByteString} representing the current\n   *                       window of attachment data.\n   */\n  public void onAttachmentDataAvailable(ByteString attachmentData);\n\n  /**\n   * Invoked when the current attachment is finished being read.\n   */\n  public void onFinished();\n\n  /**\n   * Invoked when the current attachment is finished being drained.\n   */\n  public void onDrainComplete();\n\n  /**\n   * Invoked when there was an error reading the attachments.\n   *\n   * @param throwable the Throwable that caused this to happen.\n   */\n  public void onAttachmentError(Throwable throwable);\n}\n\n\nThe process begins by registering a callback of type\nRestLiAttachmentReaderCallback as shown above with the\nprovided RestLiAttachmentReader. At some point in time in\nthe future, the RestLiAttachmentReaderCallback will be\ninvoked on\nRestLiAttachmentReaderCallback\\#onNewAttachment(SingleRestLiAttachmentReader).\nThis SingleRestLiAttachmentReader is what is used to\ntraverse through each individual attachment.\n\nThe process then continues for each attachment as developers must\nregister a SingleRestLiAttachmentReaderCallback with the\nprovided SingleRestLiAttachmentReader. Once registered, the\nSingleRestLiAttachmentReader can then be told to produce\nattachment data via\nSingleRestLiAttachmentReader\\#requestAttachmentData(). Once\nthis is invoked, at some point in time in the future, the\nSingleRestLiAttachmentReaderCallback will be invoked on\nSingleRestLiAttachmentReaderCallback\\#onAttachmentDataAvailable(ByteString)\nrepresenting the data for the reader to consume. Once the attachment\ndata is consumed, another call may be made to\nSingleRestLiAttachmentReader\\#requestAttachmentData()\nthereby driving through all the data in that attachment.\n\nRefer to the Javadocs provided for each class to obtain technical\ndetails as to how to use each API. Additional features, such as\nattachment draining and exception handling, are also described.\n\nChaining (Proxying) Attachments Across Services\n\nRest.li streaming supports the ability to proxy attachments meaning\nthat:\n\n\n  A server can take an incoming request and send one or more of its\nattachments as a request further downstream.\n  A server can take one or more attachments from a response to a\ndownstream request, and then send them back to the original request.\n  Clients and servers can coalesce multiple attachments from different\nsources.\n  This then becomes useful for observer or authentication patterns.\n\n\nHere is an outline of what it may look like across multiple\nservices:\n\n\n\nNote how these big blobs of data move seamlessly between multiple\nservices without any one service holding the entire blob in memory at\nonce.\n\nThe APIs listed below in the client and server API sections highlight\nhow to perform attachment chaining across multiple services.\n\nClient Streaming APIs\n\nSpecifying Outgoing Attachments on the Client\n\nThe generated request builders have been augmented to allow developers\nto append attachments to outgoing requests. These APIs are:\n\n1.\nappendSingleAttachment(RestLiAttachmentDataSourceWriter) to\nappend a single attachment or to proxy an incoming\nSingleRestLiAttachmentReader further downstream.\n2. appendMultipleAttachments(RestLiDataSourceIterator) to\nappend multiple attachments (as defined by the\nRestLiDataSourceIterator interface) or to proxy an incoming\nRestLiAttachmentReader further downstream.\n\nHere is sample code on the client side on what attachment creation would\nlook like when constructing a request:\n\nfinal byte[] clientSuppliedBytes = \"ClientSupplied\".getBytes();\nfinal GreetingWriter greetingAttachment = new GreetingWriter(ByteString.copy(clientSuppliedBytes));\nfinal StreamingGreetingsCreateBuilder createBuilder = new StreamingGreetingsBuilders().create();\ncreateBuilder.appendSingleAttachment(greetingAttachment);\nfinal Greeting greeting = new Greeting().setMessage(\"A greeting with an attachment\");\nfinal Request&lt;EmptyRecord&gt; createRequest = createBuilder.input(greeting).build();\ntry\n{\n  final Response&lt;EmptyRecord&gt; createResponse = getClient().sendRequest(createRequest).getResponse();\n  Assert.assertEquals(createResponse.getStatus(), 201);\n}\ncatch (final RestLiResponseException responseException)\n{\n  Assert.fail(\"We should not reach here!\", responseException);\n}\n\n\nThe implementation of GreetingWriter has not been provided\nbut it is a trivial implementation of\nRestLiAttachmentDataSourceWriter.\n\nAccessing Response Attachments on the Client\n\nBy default the server will not send response attachments back unless a\nclient specifies it can handle them. To explicitly allow response\nattachments to come back, developers will need to use\nRestLiRequestOptions as follows when constructing the\nrequest builders:\n\nfinal RestliRequestOptions defaultOptions = new RestliRequestOptionsBuilder()\n  .setProtocolVersionOption(ProtocolVersionOption.USE_LATEST_IF_AVAILABLE)\n  .setAcceptResponseAttachments(true)\n  .build();\nfinal StreamingGreetingsBuilders builders = new StreamingGreetingsBuilders(defaultOptions);\n\n\nWithout specifying this explicitly, a server will not be able to send\nany attachments back. This is because by default the\nAccept-Type header will not include\nmultipart/related as a valid accept type for the client to\nhandle.\n\nSubsequently, in order for the client to access any response attachments\nfrom the server, the Response object has been augmented\nwith two new APIs, Response\\#hasAttachments() and\nResponse\\#getAttachmentReader().\n\nClients should first see if the response has any attachments, and if so,\ncall getAttachmentReader() to return the\nRestLiAttachmentReader to walk through all the attachment\ndata.\n\nServer Streaming APIs\n\nSpecifying Response Attachments on the Server\n\nEven before assigning response attachments, the resource method should\nfirst check to see if the client can handle any response attachments.\nThis is done via:\n\nif (getContext().responseAttachmentsSupported())\n{\n   //Client can handle response attachments\n}\n\n\nSince the server does not have response builders or any opposite\nequivalent of the client side, resource methods must assign response\nattachments using an instance of RestLiResponseAttachments.\nThe APIs exposed by RestLiResponseAttachments are very\nsimilar to the client side request builders:\n\n/**\n * Append a {@link com.linkedin.restli.common.attachments.RestLiAttachmentDataSourceWriter}\n * to be placed as an attachment.\n *\n * @param dataSource the data source to be added.\n * @return the builder to continue building.\n */\npublic Builder appendSingleAttachment(final RestLiAttachmentDataSourceWriter dataSource)\n{\n  AttachmentUtils.appendSingleAttachmentToBuilder(_responseAttachmentsBuilder, dataSource);\n  return this;\n}\n\n/**\n * Append a {@link com.linkedin.restli.common.attachments.RestLiDataSourceIterator} to be used\n * as a data source within the newly constructed attachment list. All the individual attachments\n * produced from the {@link com.linkedin.restli.common.attachments.RestLiDataSourceIterator}\n * will be chained and placed as attachments in the new attachment list.\n *\n * @param dataSourceIterator\n * @return the builder to continue building.\n */\npublic Builder appendMultipleAttachments(final RestLiDataSourceIterator dataSourceIterator)\n{\n  AttachmentUtils.appendMultipleAttachmentsToBuilder(_responseAttachmentsBuilder, dataSourceIterator);\n  return this;\n}\n\n\nOnce this is created, the RestLiResponseAttachments can be\nassigned to the response via\nResourceContext\\#setResponseAttachments.\n\nHere is a complete example on how a server might respond with\nattachments:\n\nif (getContext().responseAttachmentsSupported())\n{\n    final GreetingWriter greetingWriter = new GreetingWriter(ByteString.copy(greetingBytes));\n    final RestLiResponseAttachments streamingAttachments = new RestLiResponseAttachments.Builder()\n      .appendSingleAttachment(greetingWriter)\n      .build();\n    getContext().setResponseAttachments(streamingAttachments);\n    callback.onSuccess(new Greeting().setMessage(\"Your greeting has an attachment since you were kind and \"\n      + \"decided you wanted to read it!\").setId(key));\n}\n\n\nOnce again, the implementation of GreetingWriter has not\nbeen provided but it is a trivial implementation of\nRestLiAttachmentDataSourceWriter.\n\nAccepting Request Attachments on the Server\n\nA resource method expresses that it can accept request attachments by\ndeclaring RestLiAttachmentReader as a parameter in it’s\nmethod signature. This parameter must be accompanied by the presence of\na new annotation: RestLiAttachmentsParam. Note that only\none parameter of this type can be declared and the\nRestLiAttachmentsParam cannot be used with any other\nparameter type.\n\nHere is an example of what a method signature may look like:\n\npublic void create(Greeting greeting, @CallbackParam Callback&lt;CreateResponse&gt; callback,\n                   @RestLiAttachmentsParam RestLiAttachmentReader attachmentReader)\n\n\nIf the provided RestLiAttachmentReader is not null, the\nresource method may walk through it absorbing attachment data as\ndescribed earlier above.\n\nIt is important to note that attachments cannot be sent to the server\nfor that endpoint if the resource method does explicitly allow for them\nvia the parameter declaration shown above. Therefore:\n1. If resource method asks for them and client sends them, then there\nis no problem and this is completely normal.\n2. If resource method asks for them and client does not send them, the\nresource method will see null for the\nRestLiAttachmentReader.\n3. If resource method does not ask for them and client sends them, a\nbad request is sent back and the attachments are drained by the Rest.li\nframework.\n4. If resource method does not ask for them and client does not send\nthem, then there is no problem and this is completely normal.\n\nGood Programming Practice\n\nGiven that attachment streaming will be a new experience for Rest.li\ndevelopers, we encourage all engineers to consider the following when\nwriting their services:\n\n\n  On the server side, application developers need to fully drain any\nincoming multipart requests. The Rest.li framework will only abosrb\nthe first part of the incoming request (the regular Rest.li\npayload). The rest of the payload will need to be consumed by the\nresource method. Failure to do so can lead to resource leaks (open\nconnections) which can cause server instability.\n  Similarly on the client side, application developers should fully\ndrain response attachments, otherwise the connection pool on the\nclient side will hit its limit.\n  In the event of an exception thrown by the resource method, the\nRest.li framework will attempt to drain all the request bytes.\nHowever such behavior should not be relied upon. Server developers\nshould make every attempt to fully drain the incoming request even\nin the face of an exception.\n  If an exception is thrown (i.e bad URL in the request) before a\nresource method is invoked, Rest.li will fully absorb and drop the\npayload on the ground.\n  Reduce bandwidth when possible; i.e use partial update to avoid\nresending unnecessary data across the wire.\n\n\nAdditional Developer Notes\n\n\n  There is an InputStream wrapper available that can\nallow a user specified Java InputStream to function as\na data source for a RestLiAttachmentDataSourceWriter.\nAlthough its use is discouraged since it is an async wrapper around\na synchronous library.\n  For clients issuing requests, only HTTP PUT or POST methods are\nallowed for attachment streaming:\n    \n      Create\n      Batch Create\n      Update\n      Batch Update\n      Partial Update\n      Batch Partial Update\n      Actions\n    \n  \n  Servers are allowed to send back attachments for ALL types of\nendpoints\n  A streaming supported server will need to be deployed\n  Client must explicitly specify that they can handle response\nattachments. Request builders by default will not specify\nattachments should be sent back.\n\n\nAttachment Streaming without Rest.li\n\nRest.li attachment streaming is built atop of a general purpose\nmultipart/mime layer. This layer allows application developers to:\n\n\n  Deploy a service to receive and send back attachments directly on\ntop of R2\n  Deploy clients to send and receive attachments directly on top of R2\n  Create multipart/mime requests (client) and responses (server) that\nconform to the RFC\n  Read multipart/mime responses (client) and requests (server) that\nconform to the RFC\n\n\nPlease see source, tests and examples in the multipart-mime\nmodule for more details.\n\nFuture Enhancements\n\nLinkedIn will continue to improve and enhance the Rest.li streaming\nexperience. We have plans for:\n\n\n  Full parseq integration - The current APIs are callback centric\n  Streaming support for Rest.li filters - Allow the filters to view\nwindows of bytes as attachments flow in an out of the server\n  IDL integration + request builders - Elevating the\nRestLiAttachmentReader parameter on the server side to\nthe IDL and providing a request builder specifically targeted for\nthis parameter. This would require the compat checker to detect any\nbackward incompatible changes as well.\n\n",
      tags: null,
      id: 1
    });
    
    

  
    index.add({
      title: "Rest.li Components",
      category: null,
      content: "Rest.li Components\n\nContents\n\nRest.li is the top layer of a larger software stack code named ‘pegasus’.  Pegasus is comprised of the following major components:\n\n\n  data/\n  generator/\n  r2/\n  d2/\n  restli-*/\n\n\ndata/\n\nThe pegasus data layer provides an in-memory data\nrepresentation structurally equivalent to JSON, serialization to/from JSON\nformat, and a schema-definition language for specifying data format.\n\ngenerator/\n\nThe pegasus data template code generation tool generates\ntype-safe Java APIs for manipulating pegasus data objects.\n\nr2/\n\nThe pegasus request/response layer provides fully asynchronous\nabstractions for transport protocols, along with implementations\nfor HTTP based on Netty and Jetty.\n\nd2/\n\nThe pegasus dynamic discovery layer uses Apache Zookeeper to\nmaintain cluster information.  D2 provides client-side software load balancing.\n\nrestli-*/\n\nThe pegasus Rest.li framework provides a simple framework\nfor building and consuming RESTful resources.  Rest.li provides resource\npatterns that streamline common CRUD use cases for collections of entities and\nassociations between entities.\n",
      tags: null,
      id: 2
    });
    
    

  
    index.add({
      title: "Rest.li compression",
      category: null,
      content: "Rest.li Compression\n\nContents\n\n  Supported Algorithms\n  Using Guice\n  Using Spring\n\n\nSupported Algorithms\n\nThe list of supported algorithms are posted here EncodingType.\n\nNote: There is no hand-shake protocol between the server and the client to make sure that the server supports a particular compression algorithm. If the client sends a request compressed using an algorithm that the server is not configured with, then the client request will fail as the server will not be able to decompress it.\n\nUsing Guice\n\nServer Configuration\n\nTo see how the server must be configured, see FortunesGuiceServletConfig.\n\nIn particular:\nFilterChain filterChain = FilterChains.create(\n    new ServerCompressionFilter(new EncodingType[] { EncodingType.SNAPPY }),\n    new SimpleLoggingFilter());\n\n\nThis instantiates a Rest.li server that supports SNAPPY compression. To support SNAPPY and GZIP, for example, you would have to instantiate your server as follows:\nFilterChain filterChain = FilterChains.create(\n    new ServerCompressionFilter(new EncodingType[] { EncodingType.SNAPPY, EncodingType.GZIP }),\n    new SimpleLoggingFilter());\n\n\nWhen we say that a server supports a particular algorithm for compression, it means that the server can compress and decompress using that algorithm. When to compress and decompress is governed by the client request.\n\nClient Configuration\n\nTo see how the client must be configured, see RestLiFortunesClient.\n\nIn particular:\n\n    final HttpClientFactory http = new HttpClientFactory(FilterChains.create(\n        new ClientCompressionFilter(EncodingType.IDENTITY, new EncodingType[]{ EncodingType.SNAPPY}, \"*\")\n    ));\n\n\nThis snippet instructs Rest.li to use IDENTITY (also known as no compression) for requests to the server. In other words, the HTTP “Content-Encoding” header will be set to “identity”. Thus, the first argument dictates the value of the “Content-Encoding” header.\n\nThe second argument dictates the value of the “Accept-Encoding” header. In the snippet above, each request to the server will have “Accept-Encoding” set to “snappy”. Why every request? Because the third argument is set to “*”. “*” means that each and every request to the server will have an “Accept-Encoding” header present. The third argument is a String of comma separated values, where the values can be “*”, a method name, or a family.\n\n\n  “*”: add the “Accept-Encoding” header to all outgoing requests.\n  a method name: add the “Accept-Encoding” header only for requests for the particular method. E.g. “get” would add it for get requests, “get, batch_get” would add it for get and batch_get requests etc.\n  a family: add the “Accept-Encoding” header only for requests for a method that matches the family. A family is specified as method:*. E.g. “action:*” would add it for all actions, “finder:*” would add it for all finder requests etc.\n\n\nThe above three can be mixed. For example, to have compression on get, batch_get and all finders, the third argument would be “get, batch_get, finder:*”\n\nUsing Spring\n\nServer Configuration\n\nTo see how the server must be configured, see https://github.com/linkedin/rest.li/blob/master/examples/spring-server/server/src/main/webapp/WEB-INF/beans.xml.\n\nThe following snippet from the above file configures the server side compression filter\n&lt;bean id=\"compressionFilter\" class=\"com.linkedin.r2.filter.compression.ServerCompressionFilter\" &gt;\n    &lt;constructor-arg value=\"snappy\" /&gt;\n&lt;/bean&gt;\n\n\nThe constructor-arg is a comma separated list of algorithms you want to support on the server. For example to support gzip and snappy, you would do the following:\n\n&lt;bean id=\"compressionFilter\" class=\"com.linkedin.r2.filter.compression.ServerCompressionFilter\" &gt;\n    &lt;constructor-arg value=\"snappy, gzip\" /&gt;\n&lt;/bean&gt;\n\n\nClient Configuration\n\nIt’s the same as the configuration for Guice.\n",
      tags: null,
      id: 3
    });
    
    

  
    index.add({
      title: "D2 Zookeeper configuration properties",
      category: null,
      content: "D2 Zookeeper Configuration Properties\n\nContents\n\n\n  Tiers of Configuration\n  Cluster Level Properties\n  partitionProperties Level Properties\n  Service Level Properties\n  transportClient Level Properties\n  degraderProperties Level Properties\n  loadBalancerStrategy Level Properties\n\n\nTiers of Configuration\n\n\n    There are tiers of configuration in D2. This is how we structure our\n    configuration.\n    \n      List of all clusters\n        \n            cluster A\n            \n                cluster level configuration (see below for more info)\n                services (all the services that belong to cluster A)\n                  \n                      service A-1\n                      \n                          service level properties (see below for more info)\n                          loadBalancerStrategyProperties\n                            \n                                \"loadBalancerStrategy\" level properties\n                                http.loadBalancer.updateIntervalMs\n                                http.loadBalancer.globalStepDown\n                                other load balancer properties\n                            \n                          transportClientProperties\n                          \n                              \"transportClient\" level properties\n                              http.maxResponseSize\n                              http.shutdownTimeout\n                              other transport client properties\n                          \n                          degraderProperties\n                          \n                              \"degraderProperties\" level properties\n                              degrader.lowLatency\n                              degrader.maxDropDuration\n                              other degrader properties\n                          \n                      \n                      service A-2\n                      \n                          service level properties\n                      \n                      other services under cluster A\n                  \n                partitionProperties for cluster A\n                \n                    \"partitionProperties\" level properties\n                    partitionType\n                    partitionKeyRegex\n                    other partitionProperties level properties\n                \n            \n            cluster B\n            \n                partitionProperties for cluster B (optional)\n                \n                    etc\n                \n                services (all the services that belong to cluster B)\n                \n                    service B-1\n                    service B-2\n                    etc\n                \n            \n            cluster C\n            cluster D\n            etc\n        \n    \n\n\nAs you can see, there are multiple tiers for configuration. Next we'll enumerate all the\nlevels and the configurations that belong to that level.\n\n\nCluster Level Properties\n\n\n\n    \n        Property Name\n    \n    \n        Description\n    \n\n\n    \n        partitionProperties\n    \n    \n        A map containing all the properties to partition the cluster. (See below for more details) \n    \n\n\n    \n        services\n    \n    \n        A list of d2 services that belong to this cluster.\n    \n\n\n\n\npartitionProperties Level Properties\n\n\n\n\n    \n        Property Name\n    \n    \n        Description\n    \n\n\n    \n        partitionType\n    \n    \n        The type partitioning your cluster use. Valid values are RANGE and HASH.\n    \n\n\n    \n        partitionKeyRegex\n    \n    \n        The regex pattern used to extract key out of URI.\n    \n\n\n    \n        partitionSize\n    \n    \n        Only if you choose partitionType RANGE. The size of the partition i.e. what the is the size of the RANGE in one partition\n    \n\n\n    \n        partitionCount\n    \n    \n        How many partition in the clusters\n    \n\n\n    \n        keyRangeStart\n    \n    \n        Only if you choose partitionType RANGE. This is the number where the key starts. Normally we start at 0. \n    \n\n\n    \n        hashAlgorithm\n    \n    \n        Only if you choose partitionType HASH. You have to give the type of hash. Valid values are MODULE and MD5.\n    \n\n\n\n\nService Level Properties\n\n\n\n\n    \n        Property Name\n    \n    \n        Description\n    \n\n\n    \n        loadBalancerStrategyList\n    \n    \n        The list of Strategies that you want to use in your LoadBalancer. Valid values are random, degraderV2, degraderV3. Only degraderV3 support partitioning.\n        Random load balancer just choose any random server to send the request to. So you can't do sticky routing if you choose random load balancer. \n    \n\n\n    \n        path\n    \n    \n        The context path of your service\n    \n\n\n    \n        loadBalancerStrategyProperties\n    \n    \n        The properties of D2 LoadBalancer.\n    \n\n\n    \n        transportClientProperties\n    \n    \n        A map of all properties related on the creation transport client \n    \n\n\n    \n        degraderProperties\n    \n    \n        Properties of D2 Degrader.\n          Basically it's a map of all properties related to how D2 perceives a single\n            server's health so D2 can redirect traffic to healthier server. Contrast this\n        to LoadBalancer properties which is used to determine the health of the entire cluster.\n        The difference is, if the health of cluster deteriorate, d2 will start dropping\n        requests instead of redirecting traffic.\n    \n\n\n    \n        banned\n    \n    \n        A list of all the servers that shouldn't be used. \n    \n\n\n\n\ntransportClient Level Properties\n\nProperties used to create a client to talk to a server.\n\n\n    \n    \n        \n            Property Name\n        \n        \n            Description\n        \n    \n\n    \n        http.queryPostThreshold\n    \n    \n        The max length of a URL before we convert GET into POST because the server buffer header size maybe limited. Default is Integer.MAX_VALUE (a.k.a not enabled).\n    \n\n\n    \n        http.poolSize\n    \n    \n        Maximum size of the underlying HTTP connection pool. Default is 200.\n    \n\n\n    \n        http.requestTimeout\n    \n    \n        Timeout, in ms, to get a connection from the pool or create one, send the request, and receive a response (if applicable). Default is 10000.\n    \n\n\n    \n        http.idleTimeout\n    \n    \n        Interval, in ms, after which idle connections will be automatically closed. Default is 25000.\n    \n\n\n    \n        http.shutdownTimeout\n    \n    \n        Timeout, in ms, the client should wait after shutdown is initiated before terminating outstanding requests. Default is 10000.\n    \n\n\n    \n        http.maxResponseSize\n    \n    \n        Maximum response size, in bytes, that the client can process. Default is 2 MB. \n    \n\n\n\n\ndegraderProperties Level Properties\n\nNote that each degrader is used to represent a server among many servers in a cluster.\n\n\n\n\n    \n        Property Name\n    \n    \n        Description\n    \n\n\n    \n        degrader.name\n    \n    \n        Name that will show up in the logs (make debugging easier)\n    \n\n\n    \n        degrader.logEnabled\n    \n    \n        Whether or not logging is enabled in degrader\n    \n\n\n    \n        degrader.latencyToUse\n    \n    \n        What kind of latency to use for our calculation. We support AVERAGE (default), PCT50, PCT90, PCT95, PCT99\n    \n\n\n    \n        degrader.overrideDropDate\n    \n    \n        What fraction of the call should be dropped. A value larger than 0 means this client will permanenty drop that fraction of the calls. Default is -1.0.\n    \n\n\n    \n        degrader.maxDropRate\n    \n    \n        The maximum fraction of calls that can be dropped. A value of greater or equal than 0 and less than 1\n            means we cannot degrade the client to drop all calls if necessary. Default is 1.0.\n    \n\n\n    \n        degrader.maxDropDuration\n    \n    \n        The maximum duration, in ms, that is allowed when all requests are dropped. For example if maxDropDuration is 1 min and the last request that should not\n        be dropped is older than 1 min, then the next request should not be dropped. Default is 60000.\n    \n\n\n    \n        degrader.upStep\n    \n    \n        The drop rate incremental step every time a degrader crosses the high water mark. Default is 0.2.\n    \n\n\n    \n        degrader.downStep\n    \n    \n        The drop rate decremental step every time a degrader recover below the low water mark. Default is 0.2.\n    \n\n\n    \n        degrader.minCallCount\n    \n    \n        The minimum number of calls needed before we use the tracker statistics to determine whether a client is healthy or not. Default is 5.\n    \n\n\n    \n        degrader.highLatency\n    \n    \n        If the latency of the client exceeds this value then we'll increment the computed drop rate. The higher the computed drop rate,\n            the less the traffic that will go to this server. Default is 3000.\n    \n\n\n    \n        degrader.lowLatency\n    \n    \n        If the latency of the client is less than this value then we'll decrement the computed drop rate. The lower the computed drop rate,\n        the more the traffic will go to this server. Default is 500\n    \n\n\n    \n        degrader.highErrorRate\n    \n    \n        If the error rate is higher than this value then we'll increment the computed drop rate which cause less traffic to this server. \n    \n\n\n    \n        degrader.lowErrorRate\n    \n    \n        If the error rate is lower that this value then we'll decrement the computed drop rate which in turn will cause\n        more traffic to this server.\n    \n\n\n    \n        degrader.highOutstanding\n    \n    \n        If the number of outstanding call is higher than this value then we'll increment the computed drop rate\n        which causes less traffic to this server. Default is 10000.\n    \n\n\n    \n        degrader.lowOutstanding\n    \n    \n        If the number of outstanding call is lower than this value then we'll decrement the computed drop rate\n            which causes more traffic to this server. Default is 500.\n    \n\n\n    \n        degrader.minOutstandingCount\n    \n    \n        The number of outstanding calls sohuld be greater or equal than this value for the degrader to use\n            the average outstanding latency to determine if high and low watermark condition has been met.\n            High and low water mark conditions are any of these: errorRate, latency and outstandingCount. Default is 5.\n    \n\n\n    \n        degrader.overrideMinCallCount\n    \n    \n        If overriden, we will use this value as the minimum number of calls needed before we compute drop rate. Default is -1.\n    \n\n\n\n\nloadBalancerStrategy Level Properties\n\nProperties for load balancers. This affects all servers in a cluster.\n\n\n\n    \n        \n            Property Name\n        \n        \n            Description\n        \n    \n    \n\n    \n        http.loadBalancer.hashMethod\n    \n    \n        What kind of hash method we should use (this is relevant to stickiness). Valid values are none or uriRegex \n    \n\n\n    \n        http.loadBalancer.hashConfig\n    \n    \n        If you declare this, you need to define the regexes list that we need to use to parse the URL\n    \n\n\n    \n        http.loadBalancer.updateIntervalMs\n    \n    \n        Time interval that the load balancer will update the state (meaning should load balancer, rebalance the traffic, should it increase the drop rate, etc). Default value is 5000.\n    \n\n\n    \n        http.loadBalancer.pointsPerWeight\n    \n    \n        The max number of points a client get in a hashring per 1.0 of weight. Default is 100. Increasing this number will increase the computation needed to create a hashring but lead to more even-ness in the hashring.\n    \n\n\n    \n        http.loadBalancer.lowWaterMark\n    \n    \n        If the cluster average latency, in ms, is lower than this, we'll reduce the entire cluster drop rate. (This will affect all the clients in the same cluster regardless whether they are healthy or not). Default value is 500. \n    \n\n\n    \n        http.loadBalancer.highWaterMark\n    \n    \n        If the cluster average latency is higher than this, in ms, we'll increase the cluster drop rate.(This will affect all the clients in the same cluster regardless whether they are healthy or not). Default value is 3000.\n    \n\n\n    \n        http.loadBalancer.initialRecoveryLevel\n    \n    \n        Once a cluster gets totally degraded, this is the baseline that the cluster use to start recovering. Let's say a healthy client has 100 points in a hashring. At a complete degraded state, it has 0 point. Let's say the initial recovery level is 0.005, that means the client get 0.5 point not enough to be reintroduced (because a client need at least 1 point). Default value is 0.01.\n    \n\n\n    \n        http.loadBalancer.ringRampFactor\n    \n    \n        Once a cluster is in the recovery mode, this is the multiplication factor that we use to increase the number of point for a client in the ring. For example: a healthy client has 100 points in a hashring. It's completely degraded now with 0 points. The initialRecoveryLevel is set to 0.005 and ringRampFactor is set to 2. So during the #1 turn of recovery we get 0.5 point. Not enough to be reintroduced into the ring. But at #2 turn, because ringRampFactor is 2, then we get 1 point. Turn #3 we get 2 points, etc. Default value is 1.\n    \n\n\n    \n        http.loadBalancer.globalStepUp\n    \n    \n        The size of step function when incrementing drop rate in the cluster. Default value is 0.2. Example if globalStepUp = 0.2  drop rate is 0.0 then becomes 0.2 then becomes 0.4 etc as the cluster gets more degraded\n    \n\n\n    \n        http.loadBalancer.globalStepDown\n    \n    \n        Same as http.loadBalancer.globalStepUp except this is for decrementing drop rate\n    \n\n\n\n\n",
      tags: null,
      id: 4
    });
    
    

  
    index.add({
      title: "Rest.li FAQ",
      category: null,
      content: "Rest.li FAQ\n\nHow do I validate a record with default values filled in?\n\nFoo foo = new Foo();  \nValidationResult result = ValidateDataAgainstSchema.validate(foo.data(),\nfoo.schema(), new\nValidationOptions(RequiredMode.FIXUP_ABSENT_WITH_DEFAULT));  \nassert(result.isValid());  \n\n\nThis will fail if the underlying data is read-only and default values\ncannot be set for absent fields.\n\nThis will also work for partially filled in records. It will only add\ndefault values to fields that are absent.\n\nHow do I convert a Pegasus data schema to an Avro data schema?\n\nIt requires the data-avro module, or data-avro-*.jar.\n\nAt runtime, use:\n\nFoo foo = new Foo();  \nDataSchema pegasusSchema = foo.schema();  \norg.apache.avro.Schema avroSchema =\nSchemaTranslator.dataToAvroSchema(pegasusSchema);  \nString avsoSchemaInJson = avroSchema.toString();  \n\n\nFrom the command line, to create text files with Avro schema from PDSC\nfiles (version 0.17.1 or higher):\n\n# syntax:\n# java [-Dgenerator.resolver.path=&lt;path&gt;] com.linkedin.data.avro.AvroSchemaGenerator &lt;targetDir&gt; [&lt;pdscFile|schemaFullName&gt;]...\njava -Dgenerator.resolver.path=src/main/pegasus com.linkedin.data.avro.generator.AvroSchemaGenerator ../build/main/codegen/avro src/main/pegasus/com/linkedin/foo/*.pdsc\n# or\njava -Dgenerator.resolver.path=src/main/pegasus com.linkedin.data.avro.generator.AvroSchemaGenerator ../build/main/codegen/avro com.linkedin.foo.Foo\n\n\nClasspath must be setup to include data-avro.jar and its dependencies.\n\nHow do I use a data type embedded inside a data schema file?\n\nYou may experience errors, such as the\n    following:\n\nType cannot be resolved: 1,1: \"a.b.D\" cannot be resolved.\n\nwhen a.b.D’s definition is embedded in another type, for\nexample, a.b.C.\n\nEmbedded data types do not have their own data schema file. As a result, such\ndata types can only be referenced within the containing data schema file; it can\nnot be referenced externally. To change this behavior, pull out the\ndefinition of the internal type to a separate data schema file.\n\nInternally, this behavior is due to reason that the schema parser\nreferences the data types by filenames. The schema file for a.b.C\nis expected to be found at pegasus/a/b/C.pdl. Since Rest.li\ndoes not prefix the containing data type’s name to the embedded type’s\nname, when type a.b.D is embedded inside C.pdl, the\nschema parser will not be able to find pegasus/a/b/D.pdl.\n\nWhy is my Java 8 build generating all sorts of Javadoc warnings/errors due to doclint?\n\nWhen using Java 8, you may experience build failures due to the\nfollowing:\n\n[error] /home/myuser/data-template/src/main/codegen/com/linkedin/PegasusFormSchema.java:192: error: malformed HTML\n* return type: java.util.Set&lt;java.lang.Integer&gt;\n[error] /home/myuser/data-template/src/main/codegen/com/linkedin/PegasusFormSchema.java:192: error: bad use of '&gt;'\n* return type: java.util.Set&lt;java.lang.Integer&gt;\n\n\nThe root cause is incorrectly formatted string values for anything\nwithin the doc or symbolDocs (in .pdsc files) attributes within your data schemas. Due to\nthe addition of doclint in JDK8, anything under the doc or symbolDocs\nattribute must be W3C HTML 4.01 compliant. This is because the contents\nof this string will appear as Javadocs in the generated Java ‘data\ntemplate’ classes later. Please take this into consideration when\nwriting your documentation.\n\nThe alternative is to disable doclint altogether:\n\nif (JavaVersion.current().isJava8Compatible()) {\n  allprojects {\n    tasks.withType(Javadoc) {\n      options.addStringOption('Xdoclint:none', '-quiet')\n    }\n  }\n}\n\n\nMore details on doclint can be found here: Javadoc has become very\nstrict\n\nMore details on doc and symbolDocs are at Data Schemas.\n\nHow does Rest.li full update method work with schema evolution?\n\nThere is a potential pitfall to be aware of. Consider the following\nscenario:\n\n\n  Suppose a schema begins on version 0 with a single optional field A.\nWe write a client that sends an update request with the field A\npopulated.\n  At some point, we add an optional field B in version 1.\n  Our old client still sends a request with A populated, but it is not\nclear how to interpret its request. From the server’s perspective,\nwe cannot distinguish a new client that wants to null out field B\n(e.g. schema version 1) vs. a client that is not aware of field B\n(e.g. version 0).\n\n",
      tags: null,
      id: 5
    });
    
    

  
    index.add({
      title: "Dynamic Discovery (D2)",
      category: null,
      content: "Dynamic Discovery (D2)\n\nContents\n\n\n  What is D2\n  Terminology\n  D2 and Zookeeper\n  D2 Load Balancer\n  Stores\n  Operations\n\n\nWhat is D2\n\n\n    Dynamic Discovery (D2) is a layer of indirection similar to DNS for the rest.li framework.\n    Functionally speaking, D2 translates a URI like d2://&lt;my d2 service&gt; to another address like\n    http://myD2service.something.com:9520/someContextPath.\n\n\n    Rest.li uses D2 to help decouple a REST resource from the real address of the resource.\n    D2 also acts as a client side load balancer. \n\n Note that D2 is an optional layer in the rest.li framework.\n    In fact, rest.li client can communicate directly to rest.li servers without D2.\n    But D2 provides nice benefits like partitioning, load balancing, and many others.\n    Vice versa, D2 can also be used outside of the rest.li framework.\n    Because in essence D2 is just a name service and a load balancer.\n\n\n    If you just need a quick working tutorial for D2 please refer to\n    Dynamic Discovery Quickstart\n\n\nTerminology\n\n\nService: Represents a REST resource, an endpoint, a thing that you want to interact with. The name of a service must be unique. \nCluster: Represents a set of Services. A service must belong to exactly one cluster only.\nURI: Represent a machine that belongs to a cluster. A physical representation of a cluster.\n\n\n\n    As we said above, D2 works like DNS. It translates a D2 URI to a real address.\n    D2 works by keeping the state in Zookeeper. We chose Zookeeper because it's distributed\n    and fault tolerant.\n\n\n    When a client is about to send a request, the D2 client library extract the service name from the d2 URI.\n    Then the d2 client library queries zookeeper for the cluster that owns that service.\n    Once d2 client know the cluster, it will then queries zookeeper for the available URIs for that cluster.\n\n    Given a list of URIs, D2 client can select which URI to send the request to.\n    The D2 client will listen for any updates related to the cluster and service it previously contacted. So if there's\n    any changes happening either because the server membership changes, or there are new services in a cluster,\n    the d2 client can pick up the changes immediately\n\n\n    Sometimes D2 client's connection to zookeeper might be interrupted. When this happens, D2 will not know\n    what is the latest state so it will assume the state is the same as before. D2 keep backup of the state\n    in the filesystem. If the zookeeper connection interruption happened for a long period of time (configurable), D2\n    will discard the state and will fail to work.\n\n\nD2 and Zookeeper\n\n\n    Running D2 requires a Zookeeper ensemble running somewhere. Please download Zookeeper\n    if you don't have one yet.\n\n\n\n\n\nD2 Load Balancer\n\nAs we said above, all the load balancing happened in the client side. D2 client keep tracks of the health of the cluster.\n\nThere are 2 types of mode that we can use to load balance traffic.\n\n     Shifting traffic away from one server to the other a.k.a LOAD_BALANCING \n     Dropping request on the floor a.k.a CALL_DROPPING. \n\nWe aim to alternate between these 2 modes but it’s not always guaranteed.\n\nSo how do we choose between CALL_DROPPING and LOAD_BALANCING?\n\nWe measure 2 different things for health. One is the cluster health and the other one is the client health.\n\nFor cluster health, we only measure the average cluster latency. If the average cluster latency is higher than LoadBalancer’s high water mark, we’ll increment the drop rate by 20%. Drop rate means all traffic to this cluster will be dropped 20% of the time. So obviously cluster health is relevant only to CALL_DROPPING mode. If the cluster latency exceeds high water mark 5 times in a row, we’ll reach 100% drop rate. We have some measure of “recovery mode” to prevent the cluster from getting stuck in perpetual “drop everything” mode. During this mode, we’ll still allow traffic to pass by to calibrate our cluster latency once in a while.\n\nOn the other hand, client health is tracked per client. We tracked many things per client e.g. error rate, number of calls, latency of calls, etc. We use this measurement to compute the “computed drop rate” of the client. Healthy client is a client whose latency is lower than client’s high water mark (NOTE that there’s client’s high water mark and there’s also load balancer’s high water mark). For healthy client the computed drop rate should be 0. The computed drop is inversely proportional to the number of virtual points the client gets in a hash ring.\n\nThe points are used to distribute traffic amongst many clients. For example there are 4 clients for service “widget”. In perfect condition, each client would have 100 points (this is configurable in service properties). So total points in the hash ring would be 400. If one client’s latency becomes higher than water mark, the computed drop rate will change then the number of points of that client maybe reduced to 80. So that client will receive less traffic and the other servers will get the remaining traffic.\n\nWe try to alternate between CALL_DROPPING and LOAD_BALANCING mode. The logic for doing this alternation happens in Load Balancer Strategy.\n\n Implementation of the client load balancer \n\nHere are the moving components that you should know about load balancer:\n\nProperties (ServiceProperties, ClusterProperties and URIProperties). For load balancing purposes we only care about ServiceProperties. Because that's where all the important parameters, like percent error rate, minimum call count, average latency that we can configure to tell whether a service is in \"bad state\", are located.\n Clients: rest.li client wraps a client (which we know is a d2 client). The implementation of that client is called DynamicClient. DynamicClient has a LoadBalancer and a wrapper over many simpler R2 clients to send requests.\n LoadBalancer: LoadBalancer figures out the following question: \"given a resource name, tell me who can serve the resource, then get all the clients that can send bytes to the servers, then choose one according to the load balancer strategy logic\".\n LoadBalancerStrategies: The strategy is used by LoadBalancer to pick one client among many potential clients. The interface takes a list of TrackerClients and choose one to return.\n\nIn the following section we’ll elaborate more of each component:\n\n Properties \n\nD2 uses a hierarchy of properties to model the system:\n\n ServiceProperties \n ClusterProperties \n UriProperties \n\n\n ServiceProperties\n\nLike its name, ServiceProperties defines anything related to a service. The most important one is the load balancer strategy properties. For example: we can set the highWaterMark and lowWaterMark for this service. If the average latency of all the servers that serves this resource is higher than highWaterMark we’ll start dropping calls because we know the servers are in a “degraded” state.\n\n ClusterProperties\n\nClusterProperties define’s a cluster’s name, partioning, preferred schemes, banned nodes, and connection properties.\n\n The schemes are defined in their priority. That is, if a cluster supports both HTTP and Spring RPC, for instance, the order of the schemes defines in which order the load balancer will try to find a client. \n The banned nodes are a list of nodes (URIs) that belong to the cluster, but should not be called. \n The connection properties describe things like what is the maximum size of a response. What is the time out value of an http connection and so on. \n(Note that we are migrating connection properties to ServiceProperties. This allows users to have a more fine-grained configuration. We estimate the code change will be released in version 1.8.13 and above)\n\n\n UriProperties \n\nUriProperties define a cluster name and asset of URIs associated with the cluster. Each URI is also given a weight, which will be passed to the load balancer strategy.\n\n D2 Client: what are D2 Client anyway? \n\nD2 Client is a wrapper over other simpler clients. The real implementation of D2 Client is DynamicClient.java. But underneath we use R2 client to shove bits from client to server. So DynamicClient wraps r2 clients with three classes: TrackerClient, RewriteClient, and LazyClient. The underlying R2 clients are: HttpNettyClient, FilterChainClient and FactoryClient.\n\n TrackerClient \n\nThe TrackerClient attaches a CallTracker and Degrader to a URI. When a call is made to this client, it will use call tracker to track it, and then forward all calls to the r2 client. CallTracker keeps track of call statistics like call count, error count, latency, etc.\n\n RewriteClient \n\nThe RewriteClient simply rewrites URIs from the URN style to a URL style. For example, it will rewrite “urn:MyService:/getWidget” to “http://hostname:port/my-service/widgets/getWidget”.\n\n LazyClient \n\nThe LazyClient is just a wrapper that does not actually create an r2 client until the first rest/rpc request is made.\n\n Client Wrapper Diagram \n\n\n\n LoadBalancer \n\nThere is currently one “true” implementation of a LoadBalancer in com.linkedin.d2.balancer. This implementation is called SimpleLoadBalancer. There are other implementations of LoadBalancer that will wrap this SimpleLoadBalancer for example: ZKFSLoadBalancer. In any case, the simple load balancer contains one important method: getClient. The getClient method is called with a URN such as “urn:MyService:/getWidget”. The responsibility of the load balancer is to return a client that can handle the request, if one is available, or to throw a ServiceUnavailableException, if no client is available.\n\nWhen getClient is called on the simple load balancer, it:\n        &lt;ul&gt;\n            &lt;li&gt;First tries to extract the service name from the URI that was provided. &lt;/li&gt;\n            &lt;li&gt; It then makes sure that it’s listening to that service in the LoadBalancerState. &lt;/li&gt;\n            &lt;li&gt; It then makes sure that it’s listening to the service’s cluster in the LoadBalancerState &lt;/li&gt;\n            &lt;li&gt; If either the service or cluster is unknown, it will throw a ServiceUnavailableException. &lt;/li&gt;\n            &lt;li&gt; It will then iterate through the prioritized schemes (prpc, http, etc) for the cluster.   &lt;/li&gt;\n            &lt;li&gt; For each scheme, it will get all URIs in the service’s cluster for that scheme, and ask the service’s load balancer strategy to load balance them.&lt;/li&gt;\n            &lt;li&gt; If the load balancer strategy returns a client, it will be returned, otherwise the next scheme will be tried.&lt;/li&gt;\n            &lt;li&gt;If all schemes are exhausted, and no client was found, a ServiceUnavailableException will be thrown. &lt;/li&gt;\n        &lt;/ul&gt;\n\n Strategies \n\nLoad balancer strategies have one responsibility. Given a list of TrackerClients for a cluster, return one that can be used to make a service call. There are currently two implementations of load balancer strategies: random and degrader.\n\n Random \n\nThe random load balancer strategy simply chooses a random tracker client from the list that it is given. If the list is empty, it returns null. This is the default behavior for dev environment. Because in development environments, one may wish to use the same machine for every service. so with this strategy, we will always return the “dev” tracker client to route the request (and prevent confusion).\n\n Degrader \n\nThe load balancer strategy that attempts to do degradation is the DegraderLoadBalancerStrategy. Here are some facts about the degrader strategy:\n        &lt;ul&gt;\n Each node in a cluster (TrackerClient) has an associated CallTracker and Degrader.\n The CallTracker tracks things like latency, number of exceptions, number of calls, etc for a given URI endpoint in the cluster.\n The Degrader uses the CallTracker to try and figure out whether to drop traffic, how much traffic to drop, the health of the node, etc. This is boiled down to a \"drop rate\" score between 0 and 1. \n If the cluster's average latency per node is less than the max cluster latency, all calls will go through. The probability of selecting a node in the cluster will depend on its computed drop rate (nodes with lower drop rates will be weighted higher), but no messages will be dropped.\n If the cluster's average latency per node is greater than the max cluster latency, the balancer will begin allowing the nodes to drop traffic (using the degrader's checkDrop method).\n\n Partitioning \n\nD2 currently support range-based and hash-based partitioning.\n\n Load Balancer Flow \n\n\n\nHere is an example of the code flow when a request comes in. For the sake of this example, we’ll a fictional widget service. Let’s also say that in order to get the data for a widget resource, we need to contact 3 different services: WidgetX, WidgetY, and WidgetZ backend.\n\nOn the server side:\n\n When a machine joins a *cluster*, let's say we add a new machine to Widget Server Cluster. Let's say that is machine number #24. Then discovery server code in machine #24 will \"announce\" to D2 zookeeper that there is another machine joining the widget server cluster. \n It will tell zookeeper about the machine #24 *URI*. \n All the \"listeners\" for \"widget server\" *service* will be notified (these are all the clients for example widget front-end) and since the load balancer client side has the load balancing strategy, the client will determine which machine gets the new request. \n\nOn the client side:\n\n A request comes to @http://example.com/widget/1@  \n The HTTP load balancer knows that /widget/ is redirected to widget service (this is not the D2 load balancer) \n One of the machines in widget front-end gets the request and processes it. \n Since there's D2 client code in every war and the D2 client code is connected to D2 zookeeper, the client code knows how to load balance the request and choose the machine for each service needed to construct the returned data. \n In this case we assume that widget front-end needs a resource from WidgetX, WidgetY and WidgetZ backend. So the D2 client code in widget front-end is listening to these 3 services in zookeeper.  \n In the example, the D2 client code in widget front-end chooses machine #14 for WidgetX backend, machine #5 for WidgetY and machine #33 for WidgetZ backend. \n Then the requests get dispersed to each corresponding machine. \n\nStores\n\nIn D2, a store is a way to get/put/delete properties.\n\n Store Type \n\n ZooKeeper \n\nD2 contains two ZooKeeper implementations of DynamicDiscovery. The first is the ZooKeeperPermanentStore. This store operates by attaching listeners to a file in ZooKeeper. Every time the file is updated, the listeners are notified of the property change. The second is the ZooKeeperEphemeralStore. This store operates by attaching listeners to a ZooKeeper directory, and putting sequential ephemeral nodes inside of the directory. The ZooKeeperEphemeralStore is provided with a “merger” that merges all ephemeral nodes into a single property. Whenever a node is added or removed to the directory, the ZooKeeperEphemeralStore re-merges all nodes in the directory, and sends them to all listeners.\n\nIn the software load balancing system, the permanent store is used for cluster and service properties, while the ephemeral store is used for URI properties.\n\n File System\n\nThe file system implementation simply uses a directory on the local filesystem to manage property updates. When a property is updated, a file is written to disk with the property’s key. For instance, putting a property with name “foo” would create /file/system/path/foo, and store the serialized property data in it. The File System will then alert all listeners of the update.\n\n In-Memory \n\nThe in-memory implementation of Dynamic Discovery just uses a HashMap to store properties by key. Whenever a store is put/delete occurs, the HashMap is updated, and the listeners are notified.\n\n Toggling \n\nThe toggling store that wraps another PropertyStore. The purpose of the toggling store is to allow a store to be “toggled off”. By toggling a store off, all future put/get/removes will be ignored. The reason that this class is useful is because LinkedIn wants to toggle the ZooKeeper stores off if connectivity is lost with the ZooKeeperCluster (until a human being can verify the state of the cluster, and re-enable connectivity).\n\n Registries \n\nIn D2, a registry is a way to listen for properties. Registries allow you to register/unregister on a given channel. Most stores also implement the registry interface. Thus, if you’re interested in updates for a given channel, you would register with the store, and every time a put/delete is made, the store will update the listeners for that channel.\n\n Messengers  \n\nBy default, none of the stores in Dynamic Discovery are thread safe. To make the stores thread safe, a PropertyStoreMessenger can be used. The messenger is basically a wrapper around a store that forces all writes to go through a single thread. Reads still happen synchronously.\n\n\n\nOperations\n\nPopulating Zookeeper\n\nDeleting zookeeper nodes\nJMX\n\nWe have beans in the com.linkedin.d2 JMX name space.\n\n",
      tags: null,
      id: 6
    });
    
    

  
    index.add({
      title: "Guice dependency injection with Rest.li",
      category: null,
      content: "Guice Dependency Injection With Rest.li\n\nGuice dependency injection may be used with Rest.li by using the restli-guice-bridge module.\n\nAn example project using guice dependency injection is available in the Rest.li codebase:\n\nhttps://github.com/linkedin/rest.li/tree/master/examples/guice-server\n\nThis example shows how Rest.li’s GuiceRestliServlet may be used to run Rest.li servers with full guice dependency injection.\n\nTo use it, first add a dependency to the restli-guice-bridge module, for example:\n\ndependencies {\n  compile \"com.linkedin.pegasus:restli-guice-bridge:1.9.23\"\n}\n\n\nNext, set up a guice servlet, defining a guice GuiceServletContextListener for your application.  For an example, see /examples/guice-server/server/src/main/java/com/example/fortune/inject/FortunesGuiceServletConfig.java\n\nLastly, configure your web.xml. For an example, see /examples/guice-server/server/src/main/webapp/WEB-INF/web.xml\n",
      tags: null,
      id: 7
    });
    
    

  
    index.add({
      title: "Rest.li Projections in Java",
      category: null,
      content: "How to Use Projections in Java\n\nContents\n\n\n  What Are Projections\n  What Can Be Projected\n  Getting the PathSpec of a Field\n  How To Make a REST Request with Projections using the Java\nClient\n  Turning Off the Rest.li Framework’s AUTOMATIC\nProjection\n  FAQ\n\n\nWhat Are Projections\n\nProjections are a way for a client to request only specific fields from an object instead of the entire object. Using projections when the client needs a few fields from an object is a good way to self-document the code, reduce payload of responses, and even allow the server to relax an otherwise time consuming computation or IO operation. You can read more about projections at Projections.\n\nWhat Can Be Projected\n\nClients can project the entity object(s) in a response. For example, one\ncan project:\n\n\n  The RecordTemplate value provided in a GetResult from a GET\n  Each of the RecordTemplate objects in the values returned in a map\nby a BATCH_GET\n  Each of the RecordTemplate objects in a list returned by a FINDER\n  Each of the RecordTemplate objects in each CollectionResult returned by a BATCH_FINDER\n\n\nFor resource methods returning CollectionResult, the Metadata and Paging\nthat is sent back to the client may also be projected.\n\nGetting the PathSpec of a Field\n\nUsing projections in Java code relies heavily on PathSpec objects,\nwhich represent specific fields of an object. To get a PathSpec of a\nfield bar of a RecordTemplate object Foo, you would write the\nfollowing:\n\nPathSpec pathSpec = Foo.fields().bar();\n\n\nFor Paging projection, here is an example on how to get the PathSpec\nof the total field:\n\nPathSpec pathSpec = CollectionMetadata.fields().total();\n\n\nIt is not possible to set projections for non-RecordTemplate objects.\n\nHow To Make a REST Request with Projections using the Java Client\n\nProjections are set by the request builder. To set a request projection\nfor entity objects in the response, create your builder as you normally\nwould and then add your projection to it:\n\nbuilder.fields(pathSpec);\n\n\nthe fields() method can take as arguments any number of PathSpecs, or\nan array of them.\n\nbuilder.fields(pathSpec1, pathSpec2, pathSpec3);\n\nbuilder.fields(pathSpecArray);\n\n\nThis will create a positive projection for your given fields. The\nrequest will only return fields that you have specified with\n.fields(...).\n\nSimilarly, you can do the same for custom Metadata projection and Paging\nprojection for resource methods that return CollectionResult, for\nexample:\n\nbuilder.metadataFields(pathSpec1, pathSpec2);\nbuilder.pagingFields(CollectionMetadata.fields().total());\n\n\nTurning Off the Rest.li Framework’s AUTOMATIC Projection\n\nIf you choose to examine and apply projections manually, or if you\nsimply would like to disable them for performance optimization, you can\nturn off the framework’s AUTOMATIC projection processing.\n\nThis can be done by setting the “projection mode” to MANUAL on the\nResourceContext:\n\n//For entity objects in the response\ngetContext().setProjectionMode(ProjectionMode.MANUAL);\n\n//For custom Metadata projection (CollectionResult only)\ngetContext().setMetadataProjectionMode(ProjectionMode.MANUAL);\n\n\nFor example:\n\npublic Greeting get(Long key)\n{\n  MaskTree mask = context.getProjectionMask();\n  if (mask != null)\n  {\n    // client has requested a projection of the entity\n    getContext().setProjectionMode(ProjectionMode.MANUAL); // since we’re manually applying the projection\n    // manually examine the projection and apply it entity before returning\n    // here we can take advantage of the information the projection provides to only load the data the\n    // client requested\n  }\n  else\n  {\n    // client is requesting the full entity\n    // construct and return the full entity\n  }\n}\n\n\nand in the case of CollectionResult, you could do the following as well:\n\n@Finder(\"myFinder\")\npublic CollectionResult&lt;SomeEntity, SomeCustomEntity&gt; myFinderResourceMethod(\n  final @PagingContextParam PagingContext ctx,\n  final @ProjectionParam MaskTree entityObjectProjection,\n  final @MetadataProjectionParam MaskTree metadataProjection,\n  final @PagingProjectionParam MaskTree pagingProjection)\n{\n\n  final List&lt;SomeEntity&gt; responseList = new ArrayList&lt;&gt;();\n  if (entityObjectProjection != null)\n  {\n    // client has requested a projection of the entity\n    getContext().setProjectionMode(ProjectionMode.MANUAL); // since we’re manually applying the projection\n\n    // manually examine the projection and apply it entity before returning\n    // here we can take advantage of the information the projection provides to only load the data the\n    // client requested\n    responseList.addAll(fetchFiltereredEntities());\n  }\n  else\n  {\n    // client is requesting the full entities\n    // construct the full entities\n    responseList.addAll(fetchEntities());\n  }\n\n  final SomeCustomEntity customEntity;\n  if (metadataProjection != null)\n  {\n    // client has requested a projection of the custom metadata\n    getContext().setMetadataProjectionMode(ProjectionMode.MANUAL); // since we’re manually applying the meta data projection\n\n    // manually examine the projection and apply it entity before returning\n    // here we can take advantage of the information the projection provides to only load the data the\n    // client requested\n    customEntity = fetchSomeFilteredCustomEntity();\n  }\n  else\n  {\n    // client is requesting the full metadata entity\n    // construct and return the full metadata\n    customEntity = fetchSomeCustomEntity();\n  }\n\n  final Integer total;\n  if (pagingProjection != null)\n  {\n    // client has requested a projection of the paging information\n    // since the rest.li framework will always automatically project paging,\n    // we can still selectively calculate the total based on the path spec\n    if(pagingProjections.getOperations.get(CollectionMetadata.fields().total()) == MaskOperation.POSITIVE_MASK_OP)\n    {\n      total = calculateTimeConsumingTotal();\n    }\n  }\n  else\n  {\n    total = null;\n  }\n\n  return new CollectionResult(responseList, total, customEntity);\n}\n\n\nNote that Paging projection is always automatically applied by the\nRest.li framework if there is a request by the client to do so. This is\nbecause it is the Rest.li framework who is responsible for constructing\nthe pagination (CollectionMetadata) which includes items such as the\nnext/prev links . The MaskTree provided for Paging is simply provided\nas a reference to the resource method with the most common use case\nbeing whether or not to pass null for the total in the construction of\nCollectionResult.\n\nFAQ\n\nIs it possible to create a negative projection if I want all but a few fields?\n\nNo. If you want a large number of fields, you will need to include them\nall in the .fields(...) method\ncall.\n\nIf a field’s type is itself a RecordTemplate, can I create a projection on it?\n\nYes, the simplest way to is to use the RecordTemplate.fields() method\nto help construct the appropriate pathspec to pass to the builder’s\n.fields(...) method call. For example:\n\nnew ExampleBuilders(options).get()\n  .id(id)\n  .fields(RootRecord.fields().message().id())\n  .build()\n\n\nApplies projection on a GET request to a resource where the message\nfield of RootRecord.pdl is a record type called Message.pdl, and\nonly the id fields of the message is being projected. The same logic\ncan be applied to RecordTemplates within custom Metadata and Paging\nprojection.\n\nCan I examine a request’s projections on the server side?\n\nIn general, examining a request’s projections on the server side will\nnot be necessary. When the server returns an object to the client, the\nREST framework will take care of stripping all unrequested fields. It is\nnot necessary for the server to examine the projection and strip fields\nitself.\n\nHowever, it is possible for the server to examine a request’s\nprojection.\n\nMaskTree entityProjection = getContext().getProjectionMask();\nMaskTree metadataProjection = getContext().getMetadataProjectionMask();\nMaskTree pagingProjection = getContext().getPagingProjectionMask();\n\n\nOr, if you are using free-form resources, you can get the same\nMaskTree by having it injected in, for example:\n\n@RestMethod.Get\npublic Greeting get(Long key, @ProjectionParam MaskTree projection)\n{\n  // …\n}\n\n\nThis will get you all possible projections of a request. If there were\nno projections available, the respective MaskTrees would be null.\nNote that the use of these annotations is mandatory if you specify\nMaskTrees in your method signatures.\n\nIf there were projections, you can check the status of each field.\n\nMaskOperation mask = projections.getOperations.get(pathSpec);\n\nif (mask == MaskOperation.POSITIVE_MASK_OP)\n{\n  // field is requested.\n}\nelse\n{\n  // field is not requested\n}\n\nMaskOperation totalMask = pagingProjections.getOperations.get(CollectionMetadata.fields().total());\n\nif (totalMask == MaskOperation.POSITIVE_MASK_OP)\n{\n  // the total field in pagination is requested.\n}\nelse\n{\n  // total is not requested\n}\n\n\nYou can use this information in whatever way you wish to. For example,\nresource methods may choose to exclude the calculation of ‘total’\n(thereby passing null for total into CollectionResult) if the client\ndecided they didn’t need it.\n",
      tags: null,
      id: 8
    });
    
    

  
    index.add({
      title: "Rest.li Projections",
      category: null,
      content: "Rest.li Projections\n\nContents\n\n\n  Motivation\n  Goals\n  Proposed Solution\n  REST API\n  Client API\n\n\nFor details on how to use Projections from Java, see How to use\nprojections in Java.\n\nMotivation\n\nMany real life use cases require filtering of objects solely based on\ntheir structure. Two main use cases are:\n\n\n  Selecting only a subset of object contents for performance reasons.\nFor example, select only names from User profile objects.\n  Removing certain fields based on security policy. For example,\nremove private email and phone number fields from User profile\nobjects.\n\n\nWe call this type of filtering - structural filtering or projections.\nThe following section describes design and implementation of structural\nfiltering in Pegasus.\n\nGoals\n\nTwo main goals of structural filtering design are:\n\n\n  Ease of use\n  Efficiency\n\n\nIn order to satisfy those goals, it is desirable that:\n\n\n  Order in which filters are applied is not important. This simplifies\nlife on the client side.\n  Filters are composable. This adds flexibility on the server side and\nallow efficient implementation.\n\n\nMore formally, filters are functions on objects; if f, g and h are\nfilters and x is an object, we would like the have:\n\n\n  Commutativity: f(g(x)) = g(f(x))\n  Semantically valid composition function: having f, g define (fg)\n  If possible, associativity: (fg)h(x) = f(gh)(x)\n\n\nProposed Solution\n\nBefore going into details, one note on terminology. There is a big\noverlap between Map and Object types (in general). In many languages,\nthey are treated interchangeably. You can think of an object as a map,\nwhere map key is field name and map value is a field value. Moreover,\nJSON does not distinguish between the two. Thus, this document describes\nonly how to define filters for Map data type. All definitions related to\nMap type also apply to Object type.\n\nThere are two types of structural filters users might want to express:\n\n\n  Positive mask: select only specified fields from the object\n  Negative mask: remove specified fields from the object\n\n\nNegative Projection Support\n\nIt is important to note that even though the Rest.li framework can\nprocess negative masks for negative projections, the generated client\nbuilders do NOT allow such negative masks to be created. Only\npositive masks can be created by the generated request builders and\nhence all projection use cases within LinkedIn use positive projections.\n\nThe documentation listed below here for positive and negative masks is\nprovided simply for reference purposes. At some point in the future, it\nmay be possible that the Rest.li framework supports negative projection.\n\nSyntax\n\nStructural filtering can be expressed as a JSON or any equivalent in\nmemory representation. Ability of expressing filters as JSON objects is\nbeneficial because it can be easily processed by clients and is language\nagnostic.\n\nPositive Mask Syntax\n\nPositive mask is a JSON object, with number 1 assigned to selected\nfields, for example:\n\n{\n  \"person\": {\n    \"phone\": 1,\n    \"firstname\": 1,\n    \"lastname\": 1,\n    \"current_position\": {\n      \"job_title\": 1\n    }\n  }\n}\n\n\nis a mask, which selects only phone, first name, last name and current\nposition’s job title from User Profile.\n\nNegative Mask Syntax\n\nNegative mask is a JSON object, with number 0 assigned to selected\nfields, for example:\n\n{\n  \"profile\": {\n    \"phone\": 0\n  }\n}\n\n\nis a mask, which hides phone number in the User Profile.\n\nComplex data structures\n\nArray\n\nSyntax for array is the following:\n\n\"array_field\": {\n  \"$start\": 10,\n  \"$count\": 15,\n  \"$*\": {\n    (...)\n  }\n}\n\n\nwhere $* is a wildcard mask, applied to every element in Array. $start\nand $count are optional fields which specify range of array that will be\nreturned. First element of an array has index 0. Semantic of $start and\n$count is intuitive:\n\n\n  $start specify first element of an array, which will be returned\n  $count specifies how many elements, starting from $start will be\nreturned\n\n\nSpecifying range in an array is treated as a positive mask e.g. it is\nequivalent to selecting elements of array using positive mask.\n\nIf mask contains only $start, then $count is implicitly evaluated to\nInteger.MAX_INT.\n\nIf mask contains only $count, then $start is implicitly evaluated to 0.\n\nIf entire array needs to be masked, then syntax is simply:\n\n\"array_field\": 1\n\n\nfor positive mask or\n\n\"array_field\": 0\n\n\nfor negative mask.\n\nMap\n\nSyntax for the map is the following:\n\n\"map_field\": {\n    \"$*\": {\n      /* mask for every values in the map */\n    },\n    \"key1\": {\n      /* mask for value of key1 */\n    },\n    \"key2\": {\n      /* mask for value of key2 */\n    }\n  }\n}\n\n\nwhere $* is a wildcard mask, applied to every value in Map.\n\nIf mask for map contains both wildcard $* and values for specific key\ne.g. key1, then mask, which will be applied to the value of key1 is a\ncomposition of wildcard mask and mask for key1. See next chapters for\nsemantics of mask composition.\n\nIf entire value for a specific key needs to be masked, syntax is:\n\n\"map_field\": {\n  \"key1\": 1,\n  \"key2\": 1\n}\n\n\nfor positive mask and:\n\n\"map_field\": {\n  \"key1\": 0,\n  \"key2\": 0\n}\n\n\nfor negative mask.\n\nFinally, if entire map needs to be masked, syntax is:\n\n\"map_field\": 1\n\n\nfor positive mask and:\n\n\"map_field\": 0\n\n\nfor negative mask.\n\nEscaping\n\nMask syntax defines meta-fields: $, $start and $count. They do not\nrepresent mask for fields with names: $, $start, $count and they have\nspecial semantics. In order to be able to represent mask for fields,\nwhich names start with ‘$’ character, ‘$’ character is escaped in all\nfield names e.g.:\n\n{\n  \"$$field\": 1\n}\n\n\nis a mask, which selects only field with name “$field”.\n\nMasks semantics\n\nPositive mask is used in situations where user wants to limit the\nresults only to specified fields (most likely for performance reasons).\n\nIf positive mask is applied to an object, all fields not specified in\nthe mask will be permanently removed from the object, leaving only\nfields specified in the mask.\n\nNegative mask is used in situations where user wants to restrict access\nto some fields. One use case might be applying security policies on the\nserver side, where certain confidential fields are removed from objects.\nOther use case is that client might want to fetch entire object\nexcluding some specified fields e.g. for performance reasons - see REST\nAPI section for use case example.\n\nIf negative mask is applied to an object, all fields specified in the\nmask will be permanently removed from the object, leaving all other\nfields untouched in the object.\n\nIn case all fields of an object get removed by the negative mask, then\nthe result is an empty object. Similarly, if object does not have any of\nfields specified in positive mask, the result is an empty object.\n\nMasks Composition\n\nMotivation\n\nTwo main use cases for mask composition are:\n\n\n  Server side service implementation wants to apply various security\npolicies on top of positive mask obtained from request; for\nperformance reasons it is better to compose masks and apply them in\none go, especially if object is big\n  Assembly-like engine wants to compose multiple positive masks, send\nsingle request to fetch data and return result to requesters; with\nproposed mask composition semantics this would be possible\n\n\nMasks of the Same Kind\n\nIn terms of JSON representation, composition of 2 or more masks of same\nkind is just union of objects that JSON represents.\nComposition of two positive masks is mask equivalent to sum of fields\nrequested in separate masks.\n\nThe following table summarizes the semantics of composition of two\npositive masks (1 means that field exists in positive mask, - means that\nit does not exist in mask, v in result means that field exist in an\nobject after applying mask, - means that field is missing):\n\n\n  \n    \n      positive mask\n      positive mask\n      result\n    \n  \n  \n    \n      -\n      -\n      -\n    \n    \n      -\n      1\n      v\n    \n    \n      1\n      -\n      v\n    \n    \n      1\n      1\n      v\n    \n  \n\n\nThe following table summarizes the semantics of composition of two\nnegative masks (0 means that field exists in negative mask, - means that\nit does not exist in mask, v in result means that field exist in an\nobject after applying mask, - means that field is missing):\n\n\n  \n    \n      negative mask\n      negative mask\n      result\n    \n  \n  \n    \n      -\n      -\n      v\n    \n    \n      -\n      0\n      -\n    \n    \n      0\n      -\n      -\n    \n    \n      0\n      0\n      -\n    \n  \n\n\nFor example, composition of two positive masks:\n\nmask1:\n\n{\n  \"a\": 1,\n  \"c\": 1\n}\n\n\nmask2:\n\n{\n  \"b\": 1,\n  \"d\": 1\n}\n\n\ncomposition of mask1 and mask2:\n\n{\n  \"a\": 1,\n  \"b\": 1,\n  \"c\": 1,\n  \"d\": 1\n}\n\n\nCase of array’s $start and $count meta-fields requires extra\nexplanation. Since $start and $count are treated as a positive mask, for\nexample selection of sub-range of array, composition of two ranges is\nsmallest range that contains both ranges:\n\nmask1:\n\n\"array_field\": {\n  \"$start\": 15,\n  \"$count\": 20,\n  \"$*\": {\n    (...)\n  }\n}\n\n\nmask2:\n\n\"array_field\": {\n  \"$start\": 20,\n  \"$count\": 30,\n  \"$*\": {\n    (...)\n  }\n}\n\n\ncomposition of mask1 and mask2:\n\n\"array_field\": {\n  \"$start\": 15,\n  \"$count\": 35,\n  \"$*\": {\n    /* composition of masks from mask1 and mask2 */\n    (...)\n  }\n}\n\n\nanother example, when ranges are disjoint:\n\nmask1:\n\n\"array_field\": {\n  \"$start\": 10\n  \"$count\": 5,\n  \"$*\": {\n    (...)\n  }\n}\n\n\nmask2:\n\n\"array_field\": {\n  \"$start\": 20,\n  \"$count\": 5,\n  \"$*\": {\n    (...)\n  }\n}\n\n\ncomposition of mask1 and mask2:\n\n\"array_field\": {\n  \"$start\": 10,\n  \"$count\": 15,\n  \"$*\": {\n    /* composition of masks from mask1 and mask2 */\n    (...)\n  }\n}\n\n\nMasks of different kinds\n\nIntuitively the output of composition of positive and negative masks is\nas if at first, positive mask was applied and after that, negative mask\nwas applied.\n\nThe following table summarizes the semantics of composition of positive\nand negative masks (1 means that field exists in positive mask, 0 means\nthat field exists in negative mask, v in result means that field exist\nin an object after applying mask, - means that field is missing):\n\n\n  \n    \n      positive mask\n      negative mask\n      result\n    \n  \n  \n    \n      -\n      -\n      -\n    \n    \n      -\n      0\n      -\n    \n    \n      1\n      -\n      v\n    \n    \n      1\n      0\n      -\n    \n  \n\n\nHere’s a few examples.\n\nmask1 (positive):\n\n{\n  \"a\": 1,\n  \"b\": 1\n}\n\n\nmask2 (negative):\n\n{\n  \"b\": 0,\n  \"c\": 0\n}\n\n\ninput object:\n\n{\n  \"a\": \"value1\",\n  \"b\": \"value2\",\n  \"c\": \"value3\",\n  \"d\": \"value4\"\n}\n\n\nresult after applying composition of mask1 and mask2:\n\n{\n  \"a\": \"value1\"\n}\n\n\nComposition of simple mask with complex mask\n\nIt is possible that simple mask (0 or 1) is composed with a complex\nmask, represented as an object.\n\nComposition of negative mask 0 with complex mask\n\nSince negative mask (0) has the higher priority than positive mask, then\nresult of composition of mask equal to 0 with any other mask is 0 e.g.\ncomposition of mask:\n\n{\n  \"a\": 0\n}\n\n\nwith\n\n{\n  \"a\": {\n    \"$*\": 1\n    \"b\": 0,\n  }\n}\n\n\nis equal to:\n\n{\n  \"a\": 0\n}\n\n\nComposition of positive mask 1 with complex mask\n\nThe semantics of positive mask is: select this field and all it’s\nchildren. Hence, the following two masks are semantically equivalent:\n\n{\n  \"a\": 1\n}\n\n\n{\n  \"a\": {\n    \"$*\": 1\n  }\n}\n\n\nIn other words, positive mask is recursive. If positive mask was not\nrecursive, then it would not be possible to express the following\nfilter: “select field ”a” and all it’s children” without prior knowledge\nof all available fields in field “a”.\n\nComposition of positive mask with a complex mask propagates positive\nmask recursively e.g. composition of:\n\n{\n  \"a\": 1\n}\n\n\nwith:\n\n{\n  \"a\": {\n    \"b\": 0\n  }\n}\n\n\nyields result:\n\n{\n  \"a\": {\n    \"$*\": 1,\n    \"b\": 0\n  }\n}\n\n\nIf complex mask already contains wildcard mask, then $*=1 is\nrecursively pushed down to the wildcard of the wildcard e.g. composition\nof positive mask:\n\n{\n  \"profile\": 1\n}\n\n\nwith negative mask:\n\n{\n  \"profile\": {\n    \"$*\": {\n      \"password\": 0\n    }\n  }\n}\n\n\nyields result:\n\n{\n  \"profile\": {\n    \"$*\": {\n      \"$*\": 1,\n      \"password\": 0\n    }\n  }\n}\n\n\nThe reason why pushing down the $*=1 mask preserves correct semantics\nis because simple mask 1 is equivalent to the complex mask: { “$*”: 1\n}.\n\nIt means that the following masks are equivalent:\n\n{\n  \"a\": 1\n}\n\n\nand\n\n{\n  \"a\": {\n    \"$*\": 1\n  }\n}\n\n\nMask composition properties\n\nMask composition is commutative. It means that it doesn’t matter in\nwhich order masks are composed. However, application of masks, which are\nor were built using positive masks, on the object is not associative\noperation. Consider the following example:\n\nmask1:\n\n{\n  \"a\": 1\n}\n\n\nmask2:\n\n{\n  \"b\": 1\n}\n\n\ninput object:\n\n{\n  \"a\": \"value1\",\n  \"b\": \"value2\"\n}\n\n\nIf we first compose masks and apply mask obtained from composition on\ninput object the result will contain both fields “a” and “b”. However,\nif we first apply mask1 on input object and then apply mask2 on the\nresult, then final result will be empty object.\n\nREST API\n\nREST API uses syntax similar to LinkedIn public REST API extended with\nconcept of negative masks. The need for concept of negative mask is\nrequired to accommodate the following scenario.\n\nLet’s assume that User Profile contains a field “emails” which contains\nmailbox contents in it. Client might want to request User Profile data\n(without knowing exactly which fields are available) but definitely\nwithout “emails” field, which might be huge. The notion of not needing\nto know which fields are available allows backward compatible evolution\nof data model without modification of all clients. This use case might\nappear in mid-tier, where service retrieves objects, do\ncalculations/modify them and return to requesters. It would be\nbeneficial if mid-tier services were oblivious (to some extent) to data\nmodel evolution.\n\nThe reasons for choosing LinkedIn public API syntax in REST API, instead\nof encoding JSON are the following:\n\n\n  It is concise, so filters can be written by hand when needed (for\nexample, during development and testing).\n  It is familiar to LinkedIn public API users.\n  It is expressive.\n\n\nInternally, filter expressions and JSON filter expressions will be\nparsed into the same data structure. This will allow composition of\nfilters (for example, security policies). REST API will not be fully\ncompatible with original LinkeIn public API syntax.\n\nProposed syntax will be used only in REST interface. Composition of\nsimple masks can generate complicated masks. Pegasus will automatically\ngenerate correct expression and will be able to parse it. The only time,\nwhen developer will deal with projections syntax is during\nexperimenting, debugging and so on.\n\nFor example, the following JSON positive mask:\n\n{\n  \"person\": {\n    \"firstname\": 1,\n    \"lastname\": 1\n  }\n}\n\n\nwould translate to the following expression:\n\n:(person:(firstname,lastname))\n\n\nProjections are passed as a value of ‘fields’ request parameter in URL\ne.g.\n\nhttp://host:port/context/resource/id?fields=person:(firstname,lastname)\n\n\nPlease note that, as mentioned above, negative projections should\nnot be used.\n\nArray\n\nThe syntax for array representation is the following:\n\n\"array_field\": {\n\"$start\": 10,\n\"$count\": 15,\n\"$*\": {\n  \"field1\": 1,\n  \"field2\": 1\n  }\n}\n\n\nwill be represented as:\n\narray_field:($*:(field1,field2),$start:10,$count:15)\n\n\nMap\n\nThe syntax for map representation follows similarity between Object and\nMap data structure. The syntax is the following, for mask on map:\n\n \"map_field\": {\n    \"$*\": {\n      \"field1\": 1\n    },\n    \"key1\": {\n      \"field2\": 1\n    },\n    \"key2\": {\n      \"field3\": 1\n    }\n  }\n}\n\n\nwill be represented as:\n\nmap_field:($*:(field1),key1:(field2),key2:(field3))\n\n\nClient API\n\nJava client should use helper classes in:\n\ncom.linkedin.data.transform.filter.request\n\n\nto build and manipulate projections.\n",
      tags: null,
      id: 9
    });
    
    

  
    index.add({
      title: "Rest.li Request Response API (R2)",
      category: null,
      content: "Rest.li Request Response API (R2)\n\nContents\n\n\n  Introduction\n  Layers\n  Requests and Responses\n\n\nIntroduction\n\nR2 is the request / response API underlying Rest.li.  It includes abstractions for REST requests and responses, filter chains for customized processing, and transport abstraction. It is designed so that it can be easily customized for non-open source use cases.\n\nR2 can be used independently, with D2 (our Dynamic Discovery system), or with both D2 and Rest.li.\n\nLayers\nThe following diagram shows the layers involved in the R2 system. Each layer will be described in detail below.\n\n\n\nRequests and Responses\n\nIn this section, we describe messages in the R2 system. The message hierarchy has been designed to make it possible to add and work with broader message abstractions when appropriate. Originally, REST was the basic type of message in R2, but with the addition of R2 Streaming, STREAM became the most basic type of messages in R2, with REST built on top of STREAM. Because REST is the most common use case, we will stick to describing the REST model here, and leave STREAM as a separate doc.\n\nMessages have a few properties that are worth describing here:\n\n  They are immutable. It is not possible to change a message after it has been created. It is, however, possible to copy a message and make changes using builders, which will be described later.\n  They are thread-safe due to immutability.\n  New messages are created using builders.\n  Existing messages can be copied and modified using builders.\n\n\nMessages\n\n“RestMessage” is the root of the message hierarchy. All messages in R2 contain an “entity” (which may be empty), and corresponds to the request or response data. For REST, the R2 entity is equivalent to a REST entity. For Streaming, because of the nature of streaming, the R2 entity is replaced by an EntityStream. R2 Streaming will be discussed more fully in a separate doc.\n\nAll REST messages add headers in addition to the base message properties. Headers must conform to the definition in RFC 2616 (described in section 4.2 and associated sections). The RestMessage interface is (Note: all code snippets have been simplified here for documentation purposes, and may not reflect the full complexity of the class hierarchy):\n\npublic interface RestMessage extend MessageHeaders\n{\n  /**\n   * Returns the entity for this message.\n   *\n   * @return the entity for this message\n   */\n  ByteString getEntity();\n \n  /**\n   * Returns a {@link RestMessageBuilder}, which provides a means of constructing a new message using\n   * this message as a starting point. Changes made with the builder are not reflected by this\n   * message instance.\n   *\n   * @return a builder for this message\n   */\n  RestMessageBuilder&lt;? extends RestMessageBuilder&lt;?&gt;&gt; builder();\n}\n\n\nIn addition to an entity, all messages provide a builder that can be used to copy the message and modify its copy. In the case of the Message above, the builder is a RestMessageBuilder.\nRestMessages are subdivided into RestRequests and RestResponses. The interfaces for these are described below.\n\nRestRequest\nA request has a URI. This provides information to the client about how to direct the request - for example, which protocol to use, which server to connect to, what service to invoke, etc. R2 can be used with D2 (Dynamic Discovery), so in those cases URNs will be used for the URI. These URNs will be resolved internally by the Dynamic Discovery system.\nRestRequests add a method property, which matches the semantics for a REST message (i.e. the method is one of GET, PUT, POST, DELETE):\n\npublic interface RestRequest extends RestMessage, Request\n{\n  /**\n   * Returns the URI for this request.\n   *\n   * @return the URI for this request\n   */\n  URI getURI();\n\n  /**\n   * Returns the REST method for this request.\n   *\n   * @return the REST method for this request\n   * @see com.linkedin.r2.message.rest.RestMethod\n   */\n  String getMethod();\n \n  /**\n   * Returns a {@link RestRequestBuilder}, which provides a means of constructing a new request using\n   * this request as a starting point. Changes made with the builder are not reflected by this\n   * request instance.\n   *\n   * @return a builder for this request\n   */\n  RestRequestBuilder builder();\n}\n\n\nRestResponse\n\nRestResponses add a status property, which matches the semantics of a REST status code (e.g. 200 - OK, see RFC 2616 for details about HTTP status codes):\n\npublic interface RestResponse extends RestMessage, Response\n{\n  /**\n   * Returns the status for this response.\n   *\n   * @return the status for this response\n   * @see com.linkedin.r2.message.rest.RestStatus\n   */\n  int getStatus();\n \n  /**\n   * Returns a {@link RestResponseBuilder}, which provides a means of constructing a new response using\n   * this response as a starting point. Changes made with the builder are not reflected by this\n   * response instance.\n   *\n   * @return a builder for this response\n   */\n  @Override\n  RestResponseBuilder builder();\n}\n\n\nByteStrings\n\nEntities are stored as ByteStrings in R2. ByteStrings provide a mechanism to ensure that the byte data is immutable and not copied unless absolutely necessary. The ByteString interface looks like the following:\n\npublic final class ByteString\n{\n  /**\n   * Returns an empty {@link ByteString}.\n   *\n   * @return an empty {@link ByteString}\n   */\n  public static ByteString empty();\n \n  /**\n   * Returns a new {@link ByteString} that wraps a copy of the supplied bytes. Changes to the supplied bytes\n   * will not be reflected in the returned {@link ByteString}.\n   *\n   * @param bytes the bytes to copy\n   * @return a {@link ByteString} that wraps a copy of the supplied bytes\n   * @throws NullPointerException if {@code bytes} is {@code null}.\n   */\n  public static ByteString copy(byte[] bytes);\n \n  /**\n   * Returns a new {@link ByteString} that wraps the bytes generated from the supplied string with the\n   * given charset.\n   *\n   * @param str the string to copy\n   * @param charset the charset used to encode the bytes\n   * @return a {@link ByteString} that wraps a copy of the supplied bytes\n   */\n  public static ByteString copyString(String str, Charset charset);\n \n  /**\n   * Returns a new {@link ByteString} with bytes read from an {@link InputStream}.\n   *\n   * If size is zero, then this method will always return the {@link ByteString#empty()},\n   * and no bytes will be read from the {@link InputStream}.\n   * If size is less than zero, then {@link NegativeArraySizeException} will be thrown\n   * when this method attempt to create an array of negative size.\n   *\n   * @param inputStream that will provide the bytes.\n   * @param size provides the number of bytes to read.\n   * @return a ByteString that contains the read bytes.\n   * @throws IOException from InputStream if requested number of bytes\n   *                     cannot be read.\n   */\n  public static ByteString read(InputStream inputStream, int size) throws IOException;\n \n  /**\n   * Returns a copy of the bytes in this {@link ByteString}. Changes to the returned byte[] will not be\n   * reflected in this {@link ByteString}.\n   *\n   * Where possible prefer other methods for accessing the underlying bytes, such as\n   * {@link #asByteBuffer()}, {@link #write(java.io.OutputStream)}, or {@link #asString(Charset)}.\n   * The first two make no copy of the byte array, while the last minimizes the amount of copying\n   * (constructing a String from a byte[] always involves copying).\n   *\n   * @return a copy of the bytes in this {@link ByteString}\n   */\n  public byte[] copyBytes();\n \n  /**\n   * Copy the bytes in this {@link ByteString} to the provided byte[] starting at the specified offset.\n   *\n   * Where possible prefer other methods for accessing the underlying bytes, such as\n   * {@link #asByteBuffer()}, {@link #write(java.io.OutputStream)}, or {@link #asString(Charset)}.\n   * The first two make no copy of the byte array, while the last minimizes the amount of copying\n   * (constructing a String from a byte[] always involves copying).\n   *\n   * @param dest is the destination to copy the bytes in this {@link ByteString} to.\n   * @param offset is the starting offset in the destination to receive the copy.\n   */\n  public void copyBytes(byte[] dest, int offset);\n \n  /**\n   * Returns a read only {@link ByteBuffer} view of this {@link ByteString}. This method makes no copy.\n   *\n   * @return read only {@link ByteBuffer} view of this {@link ByteString}.\n   */\n  public ByteBuffer asByteBuffer();\n \n  /**\n   * Return a String representation of the bytes in this {@link ByteString}, decoded using the supplied\n   * charset.\n   *\n   * @param charset the charset to use to decode the bytes\n   * @return the String representation of this {@link ByteString}\n   */\n  public String asString(Charset charset);\n \n  /**\n   * Return an {@link InputStream} view of the bytes in this {@link ByteString}.\n   *\n   * @return an {@link InputStream} view of the bytes in this {@link ByteString}\n   */\n  public InputStream asInputStream();\n \n  /**\n   * Writes this {@link ByteString} to a stream without copying the underlying byte[].\n   *\n   * @param out the stream to write the bytes to\n   *\n   * @throws IOException if an error occurs while writing to the stream\n   */\n  public void write(OutputStream out) throws IOException;\n\n\nBuilders\n\nAs mentioned previously, builders provide the following basic functionality:\n\n\n  Create a new message\n  Copy a message, modify it, and create a new immutable copy\n\n\nTo create a new message, use RestRequestBuilder / RestResponseBuilder as appropriate. Builder methods are designed to be chained. Here is an example of chaining:\n\nfinal RestResponse res = new RestResponseBuilder()\n  .setEntity(new byte[] {1,2,3,4})\n  .setHeader(\"k1\", \"v1\")\n  .setStatus(300)\n  .build()\n\n\nTo copy a message, it is sufficient to ask the message for its builder. Typically this can be done with the builder method, but in some cases a special builder method must be used (when working with abstract messages).\nHere is an example of copying and modifying a message:\n\nfinal RestRequest req = ...;\nfinal RestRequest newReq = req.builder()\n                             .setEntity(new byte[] {5,6,7,8})\n                             .setURI(URI.create(\"anotherURI\"))\n                             .build();\n\n\nHere is an example of copying and modifying an abstract request:\n\nfinal Request req = ...;\nfinal Request newReq = req.requestBuilder()\n                          .setEntity(new byte[] {5,6,7,8})\n                          .setURI(URI.create(\"anotherURI\"))\n                          .build();\n\n\nCallbacks\n\nR2 is, by design, asynchronous in nature. As will be shown below, R2 provides two mechanisms to wait for an asynchronous operation to complete: callbacks and Futures. Futures should be familiar to most Java developers, so we will not discuss them further in this document. Callbacks are less common in Java and warrant some quick discussion.\nIn R2, the Callback interface looks like:\n\npublic interface Callback\n{\n  /**\n   * Called if the asynchronous operation completed with a successful result.\n   *\n   * @param t the result of the asynchronous operation\n   */\n  void onSuccess(T t);\n \n  /**\n   * Called if the asynchronous operation failed with an error.\n   *\n   * @param e the error\n   */\n  void onError(Exception e);\n}\n\n\nIn some cases it is only possible to invoke an asynchronous operation with a callback (and not a Future). In those cases, which are not common for external users, it is possible to use a FutureCallback as shown in this example:\n\nfinal FutureCallback future = new FutureCallback();\nasyncOp(..., future);\nreturn future.get();\n\n\nIn some cases, code does not need to wait for completion of an event. In the case of the future, simply do not call get(). In the case of callbacks, use Callbacks.empty(), as shown in this example:\n\nasyncOp(..., Callbacks.empty());\n\n\nKeep in mind that it will not be possible to know when the operation completed - or even if it completed successfully.\nSometimes code will want to know when an operation has completed, but is not concerned with the result. In this case, a SimpleCallback can be used or adapted to a Callback with Callbacks.adaptSimple(…).\n\nClient API\n\nThe R2 client API provides the mechanism for sending request and responses to a remote service or resource handler. The diagram below shows where the client sits in the R2 stack.\n\nThe main interface in this layer is the Client interface, shown here:\n\npublic interface Client\n{\n  /**\n   * Asynchronously issues the given request and returns a {@link Future} that can be used to wait\n   * for the response.\n   *\n   * @param request the request to issue\n   * @return a future to wait for the response\n   */\n  Future restRequest(RestRequest request);\n \n  /**\n   * Asynchronously issues the given request. The given callback is invoked when the response is\n   * received. This event driven approach is typically more complicated to use and is appropriate\n   * for building other abstractions, such as a DAG based resolver.\n   *\n   * @param request the request to issue\n   * @param callback the callback to invoke with the response\n   */\n  void restRequest(RestRequest request, Callback callback);\n \n  /**\n   * Starts asynchronous shutdown of the client. This method should block minimally, if at all.\n   *\n   * @param callback a callback to invoke when the shutdown is complete\n   */\n  void shutdown(Callback callback);\n}\n\n\nRequests are made asynchronously using either Futures or Callbacks (see Callback section for details).\n\nRequest Handler API\n\nThe Request Handler API is the server-side counterpart to the client, as shown in this diagram:\n\nRequest Handlers are used for two purposes:\nDetermining the rules for dispatching a request to a service / resource manager or another dispatcher\nHandling a request (as a service or a resource manager)\n\nThe REST Request Handler interface looks like:\n\npublic interface RestRequestHandler\n{\n  void handleRequest(RestRequest request, Callback callback);\n}\n\n\nFilter Chains\n\nThe filter chain provides a mechanism for doing special processing for each request and response in the system. For example, logging, statistics collections, etc., are appropriate for this layer.\n\nThe FilterChain provides methods for adding new filters and for processing requests, responses, and errors.\nRequests pass through the filter chain starting from the beginning and move towards the end. Responses and errors start from the end of the filter chain and move towards the beginning.\n\nIf an error occurs while a request moves through the filter chain (either due to a thrown Exception or due to an onError(…) call), then the error is first sent to the filter that raised the error and then it moves back towards the beginning of the filter chain. Any filters that show up after the filter that threw the exception will not get a chance to process the request.\n\nFilters\n\nR2 provides a set of interfaces that can be implemented to intercept different types of messages. They are:\n\n\n  Message filters\n  MessageFilter - intercepts both requests and responses\n  RequestFilter\n  ResponseFilter\n  REST filters\n  RestFilter - intercepts both REST requests and REST responses\n  RestRequestFilter\n  RestResponseFilter\n\n\nMessages filters can be used to handle messages in an abstract way (as Requests and Responses). REST filters can be used to handle messages of the specific type (REST or possibly another type, like STREAM). Different types of filters should not be used together, as they override the hooks provided by the Message filters.\n\nClientQueryTunnelFilter / ServerQueryTunnelFilter\n\nOne notable set of filters is the ClientQueryTunnelFilter and the ServerQueryTunnelFilter. These filters allow long queries to be transformed by moving the query parameters into the body, and reformulating the request as a POST. The original method is specified by the X-HTTP-Method-Override header. See QueryTunnelUtil.java for more details.\n\nWire Attributes\n\nWire attributes provide a mechanism to send “side-band” data to a remote endpoint along with a request or response. They are exposed at the filter chain layer and can be queried or modified by filters. They are not made available at the request / response layer because the entity (and headers, for REST) should supply all of the data necessary to process a request or response.\nWire attributes are sent as headers with the R2 HTTP transport.\n\nLocal Attributes\n\nLocal attributes are used by filters during response processing to get data stored during the request. Filters use this mechanism because responses are not guaranteed to be processed on the same thread as their requests.\n\nTransports\n\nThe transport bridges convert our abstract requests, response, and wire attributes into transport-specific requests. We have support for an asynchronous HTTP transport and possibly other transports at this layer.\n\nHTTP Transport\n\nIn the HTTP transport there is a standard transformation of our REST messages to equivalent HTTP messages.\nWire attributes are transported as headers, using the attribute name.\n\nTransport Protocol\nUnder the hood, the request will be encoded based on the Rest.li protocol and sent over the wire to the server.\n",
      tags: null,
      id: 10
    });
    
    

  
    index.add({
      title: "Rest.li Request Response Framework",
      category: null,
      content: "Rest.li Request Response Framework\n\nPegasus’s request/response framework, often called R2,  includes abstractions for REST and RPC requests and responses, filter chains for customized processing, and transport abstraction.\n\nR2 can be used in conjunction with the Dynamic Discovery system (also known as D2). The combined stack can be referred to as “R2D2”. R2 can be used independently as well.\n\nFilters\nThe R2 framework in Rest.li contains a filter chain layer. This allows developers to process and modify the content, the associated wire attributes, and the local attributes for each request/response.\n\nTo implement a filter, simply implement the relevant Filter interface (for REST, this is RestFilter; for RPC, use RpcFilter).\n\nTo use a filter, instantiate a Rest.li server/client with a FilterChain that contains the filters you want to use in the order that you would like to use them in. Note that the order of processing is as follows:\n\n  Requests are processed starting from the beginning of the chain and move towards the end of the filter chain.\n  Responses are processed from the end of the filter chain and move back towards the beginning of the filter chain.\n\n\nFilter Example\n\nConsider the example given in the Rest.li client tutorial:\n\nfinal HttpClientFactory http = new HttpClientFactory();\nfinal Client r2Client = new TransportClientAdapter(\n                                      http.getClient(Collections.&lt;String, String&gt;emptyMap()));\n\n\nSuppose your filter was implemented in some class MyClientFilter. To add this filter, you might do something like this:\n\nFilterChain fc = FilterChains.empty().addFilter(new MyClientFilter());\nfinal HttpClientFactory http = new HttpClientFactory(fc);\nfinal Client r2Client = new TransportClientAdapter(\n                                      http.getClient(Collections.&lt;String, String&gt;emptyMap()));\n\n\nSo how would one go about writing a filter?\nAs an example, suppose we wanted to use filters to compress the responses we receive from the server.\n\nA client filter could do this:\n\n  Add an HTTP Accept-Encoding header to its outbound requests.\n  On inbound responses, the filter would read the Content-Encoding header and decompress the payload accordingly.\n\n\nA corresponding server might do this:\n\n  The server’s compression filter can read the inbound request’s Accept-Encoding header and store it as a local attribute.\n  When the server response is ready to go, the filter can intercept the outbound response, compress it, and send the compressed payload off with the corresponding HTTP Content-Encoding header.\n\n\nSee an implementation example here: https://github.com/linkedin/rest.li/tree/master/r2/src/main/java/com/linkedin/r2/filter/compression\n\nFor a full list of filters, see: List-of-R2-filters\n",
      tags: null,
      id: 11
    });
    
    

  
    index.add({
      title: "Rest.li 2.0 response API",
      category: null,
      content: "Rest.li 2.0 response API\n\nPegasus 1.17.2 features a new set of response APIs for various batch operations. The motivation of this change is to simplify the old response APIs. This change focuses on two sets of operations: BatchGet and BatchUpdate/BatchPartialUpdate/BatchDelete.\n\nBatchGet\nThe following table summarizes the Rest.li response types:\n\n\n  \n    \n      Response name\n      Returned by\n      Strong-typed key?\n      Value type\n      New in 1.17.2?\n    \n  \n  \n    \n      BatchResponse\n      BatchGetRequestBuilder.build()\n      No\n      T\n      No\n    \n    \n      BatchKVResponse\n      BatchGetRequestBuilder.buildKV()\n      Yes\n      T\n      No\n    \n    \n      BatchEntityResponse\n      BatchGetEntityRequestBuilder.build()\n      Yes\n      EntityResponse\n      Yes\n    \n  \n\n\nEntityResponse is a new RecordTemplate class, which contains three fields:\n\n  entity provides an entity record if the server resource finds a corresponding value for the key.\n  status provides an optional status code.\n  error provides the error detail from the server resource (generally entity and error are mutually exclusive as null, but it is ultimately up to the server resource).\n\n\nNote that since EntityResponse contains error field, the Map&lt;K, V&gt; returned by BatchEntityResponse.getResults() contains both successful as well as failed entries. BatchEntityResponse.getErrors() will only return failed entries.\n\nBatchUpdate/BatchPartialUpdate/BatchDelete\nThe response type of the BatchUpdate series methods are not changed. However, similar to EntityResponse, we added a new error field to UpdateStatus (the value type of the BatchUpdate series methods). Furthermore, BatchKVResponse&lt;K, UpdateStatus&gt;.getResults() will returns both successful as well as failed entries. getErrors() will only return failed entries.\n",
      tags: null,
      id: 12
    });
    
    

  
    index.add({
      title: "Rest.li-2.x-upgrade-instructions",
      category: null,
      content: "Contents\n\n\n  Introduction\n  Source Code Changes\n  Protocol Changes\n  Upgrade and Deployment Ordering\n\n\nIntroduction\n\nRest.li 2.0 introduces a new backwards incompatible URI format, as well as removes several APIs that have been marked as deprecated for quite some time. You can find details about these changes on our user guide as well as our protocol documentation. This page documents the steps needed to upgrade from a 1.x release to a 2.x release.\n\nHaving issues with any of the steps listed below? Please create an issue and someone from the Rest.li team will help you.\n\nSource Code Changes\n\nTake these steps to make your source code compatible with Rest.li 2.x:\n\n\n  Upgrade your Rest.li dependency to the latest version of Rest.li 1.x.\n  Compile your code with deprecation warnings turned on. This is typically done by setting the -Xlint:deprecation flag when using javac.\n  Fix all the deprecation warnings related to Rest.li APIs. Almost all APIs marked deprecated in Rest.li 1.x are removed in Rest.li 2.x. To do this, look at the Javadoc for the deprecated API to find out which non-deprecated method to use instead. For example - one of the APIs that we removing in Rest.li 2.x is com.linkedin.restli.client.BatchRequest#getIdObjects. If you look at the Javadoc for this method we have instructions on what to use instead.\n  Recompile your code with deprecation warnings turned on to make sure you’ve removed all deprecated Rest.li APIs.\n  Upgrade your Rest.li dependency to the latest version of Rest.li 2.x.\n  Compile your code. If you removed all the deprecated Rest.li APIs in step 3, there should be no issues.\n\n\nProtocol Changes\n\nFor the most part the protocol changes are taken care off under the hood by Rest.li. As an application developer, you don’t have to worry about the Rest.li 2.0 protocol changes.\n\nHowever, if you are hard coding URLs, HTTP request/response bodies, or HTTP request/response headers in your code you will have to update these to the Rest.li 2.0 protocol format. Details of the protocol can be found on this page.\n\nUpgrade and Deployment Ordering\n\nThe basic rules for upgrading and deploying your clients and servers is as follows:\n\n\n  Upgrade your server to Rest.li 2.x using the instructions described above.\n  Deploy your server. Since Rest.li servers running Rest.li 2.x can understand the 1.x protocol this is safe to do.\n  Upgrade your client to Rest.li 2.x using the instructions described above.\n  Deploy your client. It should now start sending protocol 2 traffic.\n\n\nIf you have a complicated Rest.li call graph involving multiple Rest.li services, start the upgrade and deployment process at the leaf nodes of the graph (i.e. the Rest.li services that don’t call other Rest.li services) and work your way backwards.\n",
      tags: null,
      id: 13
    });
    
    

  
    index.add({
      title: "Rest.li Filters",
      category: null,
      content: "Rest.li Filters\n\nContents\n\n\n  Introduction\n  How Filters Work\n  Using Filters\n  Filter Chaining\n  Transferring State Between Filters\n  Exception Handing and Filter Chains\n  Making Asynchronous Blocking Calls from Filters\n\n\nIntroduction\n\nOn the server side, Rest.li provides a mechanism to intercept incoming requests and outgoing responses via filters. Each\nRest.li filter contains methods that handle both requests and responses.\n\nOn the request side, filters can be used for a wide range of use cases, including request validation, admission control,\nand throttling.\n\nSimilarly on the response side, filters can be used for a wide range of use cases, including augmentation of response\nbody and encrypting sensitive information in the response payload.\n\nHow Filters Work\n\nWhen using a filter, you have the option of implementing the interface’s onRequest, onResponse, and onError\nmethods - here is where you specify what the filter will do. onRequest is invoked on the request before the actual\nresource method is invoked. onResponse is invoked on the response after the resource method is invoked but before\nbeing passed to the R2 stack. onError is invoked when an exception occurs in one of the filter’s methods or if it\nreceives a response error from the previous filter’s onResponse method. onError of the first filter in the response\nfilter chain will also be invoked if the REST resource method returns an error.\n\nIf you do not implement these methods, the default behavior for each method is to do nothing. For example, you can\nchoose to only implement the onRequest method. This way, on responses or errors, the filter will simply pass the\nresponse/error to the next filter.\n\nWhen a request arrives, the filters intercept it. If onRequest executes successfully, it will pass it to the next\nfilter. If an exception occurs, all subsequent filters are skipped, the filter’s onError will be invoked, and an error\nresponse is passed through the filter chain in reverse and sent back to the client.\n\nWhen a response is returned from the REST resource method, it is passed into the filter’s onResponse method. If the REST\nresource method returns an error response, it will be passed into the onError method instead. If a filter’s onResponse\nexecutes successfully, it will pass the response to the next filter. If an exception occurs, the filter will pass it to\nthe next filter’s onError method.\n\nWhen onError is invoked, by default it will pass the error response to the next filter’s onError method. You can\nspecify additional handling (e.g. logging the error) before passing the response on. You can specify logic to fix the\nerror, whereupon the next filter’s onResponse method will be invoked.\n\nWhen a Rest.li server is configured to use filters, the filters will be invoked for all incoming requests and outgoing\nresponses of all resources hosted by that server. Therefore, when implementing filters, please keep in mind that filters\nare cross-cutting and should be applicable to all resources that are hosted by the given Rest.li server.\n\nUsing Filters\n\nCreating a concrete filter is simple. All you need to do is implement the com.linkedin.restli.server.filter.Filter\ninterface. Rest.li guarantees that for a given request-response pair, the same instance of FilterRequestContext is\nmade available to both the request filter and response filter.\n\nFilter Return Type\n\nEach filter method returns a CompletableFuture&lt;Void&gt;. A CompletableFuture represents the status result of filter\nexecution and has 3 states - completed, completed with exception, and incomplete. The next filter will not be invoked\nuntil the previous filter has completed (either successfully or exceptionally).\n\nIf the filter does not call any asynchronous methods, you can simply return CompletableFuture.completedFuture(null) -\nthis returns an already completed future, and it will cause the filter chain to invoke the next filter.\n\nIf there is an error, you can either throw an Exception or return a future that has already called\nfuture.completeExceptionally(exception) - both will do the same thing.\n\nIf the filter calls an asynchronous method, you can instantiate an incomplete CompletableFuture and return it from the\nfilter method. This future should be passed into your asynchronous method - when the method finishes, you can call\nfuture.complete(null). This will trigger the filter chain to invoke the next filter. If there is an error, you can\ncall future.completeExceptionally(exception). There are more details on this below.\n\nNot completing a future, whether successfully or exceptionally, will cause the filter chain processing to hang\nindefinitely.\n\nFilter Requests\n\nThe implementation of the onRequest method is free to modify the incoming request. Additionally, it can also reject\nthe incoming request by throwing an exception or completing the future exceptionally - in this case, a response error is\nautomatically passed into the filter’s onError method.\n\nThe onRequest method has access to the FilterRequestContext. FilterRequestContext is an interface that abstracts\ninformation regarding the incoming request, including the request URI, projection mask, request query parameters, and\nrequest headers. Please see documentation of FilterRequestContext for more info.\n\nAfter all the filters’ onRequest method have been successfully invoked, the filter chain passes the request to the\nRest.li resource.\n\nFilter Responses\n\nThe implementation of the onResponse method can inspect and modify the outgoing response body, HTTP status, and\nheaders. Throwing an exception causes the response to be converted into an error response and passed into the next\nfilter’s onError method.\n\nThe onResponse method has access to the FilterRequestContext and FilterResponseContext. The\nFilterResponseContext is an interface that abstracts information regarding the outgoing response, including the\nresponse HTTP status, response body, and response headers. Please see documentation of FilterResponseContext for more\ninfo.\n\nAfter the last filter’s onResponse method has been invoked successfully, the filter chain passes the outgoing response\nto the underlying R2 stack. If the last filter’s onResponse method’s future completes exceptionally, the response is\nconverted into an error response and is passed into the R2 stack.\n\nFilter Errors\n\nThe implementation of the onError method handles errors, and has the capability to alter the response body, HTTP\nstatus, and headers. The onError method has access to the exception that caused the error, FilterRequestContext, and\nFilterResponseContext.\n\nThe CompletableFuture that is returned by this method should be completed exceptionally unless this filter fixes the\nerror, whereupon the future should be completed successfully. The paradigm is that if an error exists in the response at\nthe end of the filter, the future should be completed exceptionally.\n\nIf an exception occurs within the onError method itself, the next filter’s onError will be invoked. However, the\nmost recently occurring exception will be passed in as the exception argument.\n\nAfter the last filter’s onError method has been invoked, the filter chain passes the outgoing response error to the\nunderlying R2 stack. If the last filter’s onError method’s future completes successfully (i.e. the error was fixed), the\nerror response is converted into a success response and passed to the R2 stack.\n\nExample Filter\n\nimport com.linkedin.restli.server.filter.FilterRequestContext;\nimport com.linkedin.restli.server.filter.FilterResponseContext;\nimport com.linkedin.restli.server.filter.Filter;\n\npublic class RestliExampleFilter implements Filter\n{\n  @Override\n  public CompletableFuture&lt;Void&gt; onRequest(final FilterRequestContext requestContext)\n  {\n    log.debug(String.format(\"Received %s request for %s resource.\", requestContext.getMethodType(), requestContext.getFilterResourceModel().getResourceName()));\n    return CompletableFuture.completedFuture(null);\n  }\n\n  @Override\n  public CompletableFuture&lt;Void&gt; onResponse(final FilterRequestContext requestContext, final FilterResponseContext responseContext)\n  {\n    System.out.println(String.format(\"Responding to %s request for %s resource with status code %d.\", requestContext.getMethodType(),\n                                     requestContext.getFilterResourceModel().getResourceName(), responseContext.getResponseData().getStatus().getCode()));\n    return CompletableFuture.completedFuture(null);\n  }\n\n  @Override\n  public CompletableFuture&lt;Void&gt; onError(Throwable t, final FilterRequestContext requestContext, final FilterResponseContext responseContext)\n  {\n    log.debug(t.toString());\n    CompletableFuture&lt;Void&gt; future = new CompletableFuture&lt;Void&gt;();\n    if (isErrorFixable(t))\n    {\n      fixError();\n      future.complete(null); // success\n    }\n    else\n    {\n       future.completeExceptionally(t); // could not fix error, so this filter did not execute successfully\n    }\n    return future;\n  }\n}\n\n\nWhen a request arrives, this filter prints the request type and resource name for every incoming request.\n\nWhen a response is sent, this filter prints the HTTP response code along with request type and resource name for every\noutgoing response.\n\nWhen there is an error, this filter logs the exception that caused it. Notice how the filter has the ability to either\nfix the error or propagate the error (complete normally vs. complete exceptionally).\n\nResponse Data API\n\nThe FilterResponseContext has access to a RestLiResponseData object. This object contains the response data as a\nRestLiResponseEnvelope, which also includes the HTTP status (if success) and the error exception (if error). Besides,\nit contains headers and cookies, as well as indicators for response type and the resource method.\n\nResponse Envelope API\n\nThe RestLiResponseEnvelope contains the actual data from the response, as well as HTTP status (if success) and the\nerror exception (if error). For example, a GET response would store the retrieved resource data in the envelope.\n\nIf there is an error, the exception will never be null but the data stored inside of RestLiResponseEnvelope will\nalways be null (the envelope itself will not be null, only the data inside of it). The opposite is true if there is no\nerror.\n\nThe type of response envelope is based on the Rest.li resource method. For example, a GET response would have data\nstored in the GetResponseEnvelope.\n\n\n  \n    \n      Resource Method\n      Response Envelope\n    \n  \n  \n    \n      GET\n      GetResponseEnvelope\n    \n    \n      CREATE\n      CreateResponseEnvelope\n    \n    \n      ACTION\n      ActionResponseEnvelope\n    \n    \n      BATCH_GET\n      BatchGetResponseEnvelope\n    \n    \n      BATCH_PARTIAL_UPDATE\n      BatchPartialUpdateResponseEnvelope\n    \n    \n      BATCH_UPDATE\n      BatchUpdateResponseEnvelope\n    \n    \n      BATCH_DELETE\n      BatchDeleteResponseEnvelope\n    \n    \n      BATCH_CREATE\n      BatchCreateResponseEnvelope\n    \n    \n      BATCH_FINDER\n      BatchFinderResponseEnvelope\n    \n    \n      GET_ALL\n      GetAllResponseEnvelope\n    \n    \n      FINDER\n      FinderResponseEnvelope\n    \n    \n      UPDATE\n      UpdateResponseEnvelope\n    \n    \n      PARTIAL_UPDATE\n      PartialUpdateResponseEnvelope\n    \n    \n      OPTIONS\n      OptionsResponseEnvelope\n    \n    \n      DELETE\n      DeleteResponseEnvelope\n    \n  \n\n\nResponse envelopes are grouped together based on ResponseTypes. Each response type shares the same data format, and\nthus use the same getters and setters. A parent response envelope is subclassed by the envelopes in the same\nResponseType group.\n\nFor example, GetResponseEnvelope, ActionResponseEnvelope, and CreateResponseEnvelope all store a RecordTemplate\nand all use getRecord and setRecord as their data access methods. As such, RecordResponseEnvelope is the parent\nenvelope for all three. Grouping them together this way reduces code duplication because you can write code for all\nenvelopes that share the same interface.\n\n\n  \n    \n      Response Type\n      Parent Response Envelope\n      Child Response Envelopes\n    \n  \n  \n    \n      SINGLE_ENTITY\n      RecordResponseEnvelope\n      GetResponseEnvelope, CreateResponseEnvelope, ActionResponseEnvelope\n    \n    \n      CREATE_COLLECTION\n      N/A - only one envelope falls under this response type,  so no need for parent\n      BatchCreateResponseEnvelope\n    \n    \n      GET_COLLECTION\n      CollectionResponseEnvelope\n      GetAllResponseEnvelope, FinderResponseEnvelope\n    \n    \n      BATCH_COLLECTION\n      N/A - only one envelope falls under this response type,  so no need for parent\n      BatchFinderResponseEnvelope\n    \n    \n      BATCH_ENTITIES\n      BatchResponseEnvelope\n      BatchGetResponseEnvelope, BatchUpdateResponseEnvelope, BatchPartialUpdateResponseEnvelope, BatchDeleteResponseEnvelope\n    \n    \n      STATUS_ONLY\n      EmptyResponseEnvelope\n      PartialUpdateResponseEnvelope, UpdateResponseEnvelope, DeleteResponseEnvelope, OptionsResponseEnvelope\n    \n  \n\n\nA typical use case is as follows - notice there are 2 ways to handle different response data types, the first using the resource method and the second using the response type:\n\npublic class RestliExampleFilter implements Filter\n{\n  @Override\n  public CompletableFuture&lt;Void&gt; onResponse(FilterRequestContext requestContext, FilterResponseContext responseContext)\n  {\n    RestLiResponseData&lt;?&gt; responseData = responseContext.getResponseData();\n    switch (responseData.getResourceMethod()) \n    {\n      // Example showing determining code path based on resource method (CREATE, GET, etc.)\n      case CREATE:        // Handle CREATE response\n        CreateResponseEnvelope envelope = (CreateResponseEnvelope) responseData.getResponseEnvelope();\n        someMethod(envelope.getStatus());\n        anotherMethod(envelope.getRecord());\n        envelope.setRecord(new EmptyRecord()); //Modify the response\n        break;\n      case GET        // Handles GET responses\n        break;\n      default:\n      // Other types available as well.\n    }\n\n    // Another example, this time showing determining code path based on response type (SINGLE_ENTITY, GET_COLLECTION, etc.)\n    switch (responseData.getResponseType()) \n    {\n      case SINGLE_ENTITY:        // Handle GET, ACTION, and CREATE responses - note how you can apply the same logic to all 3 because they share the same data access interface\n        RecordResponseEnvelope envelope = (RecordResponseEnvelope) responseData.getResponseEnvelope();\n        someMethod(envelope.getRecord());\n        envelope.setRecord(new EmptyRecord()); //Modify the response\n        break;\n      case GET_COLLECTION        // Handles GET_ALL and FINDER responses\n        break;\n      default:\n      // Other types available as well.\n    }\n    return CompletableFuture.completedFuture(null);\n  }\n}\n\n\nFilter Chaining\n\nRest.li supports chaining of filters. When a Rest.li server is configured to use multiple filters, the filters are\nordered in the same order specified in the RestLiConfig. On requests, filters that are declared closer to the\nbeginning are invoked first. On responses, filters that are declared closer to the end are invoked first. See diagram at\ntop of document for visualization.\n\nApproach 1 to chain three filters.\n\nfinal RestLiConfig config = new RestLiConfig();\nconfig.addFilter(new FilterOne(), new FilterTwo(), new FilterThree());\n\n\nApproach 2 to chain three filters.\n\nfinal RestLiConfig config = new RestLiConfig();\nconfig.addFilter(new FilterOne());\nconfig.addFilter(new FilterTwo());\nconfig.addFilter(new FilterThree());\n\n\nApproach 3 to chain three filters.\n\nfinal RestLiConfig config = new RestLiConfig();\nconfig.addFilter(Arrays.asList(new FilterOne(), new FilterTwo(), new FilterThree()));\n\n\nApproach 4 to chain three filters\n\n&lt;bean class=\"com.linkedin.restli.server.RestLiConfig\"&gt;\n    &lt;property name=“filters&gt;\n        &lt;list&gt;\n            &lt;bean class=“FilterOne”/&gt;\n            &lt;bean class=“FilterTwo”/&gt;\n            &lt;bean class=\"FilterThree”/&gt;\n        &lt;/list&gt;\n    &lt;/property&gt;\n&lt;/bean&gt;\n\n\nTransferring State Between Filters\n\nIt is recommended that Rest.li filters be stateless. To facilitate transfer of state between filters, Rest.li provides a\nscratch pad in the form of a Java Map. This scratch pad can be accessed via the getFilterScratchpad method on the\nFilterRequestContext. See below for an example Rest.li filter that computes the request processing time and print it\nto standard out.\n\nException Handling and Filter Chains\n\nThe manner in which exceptions are handled in the filter’s request vs. response methods are different.\n\nThere are 2 ways a filter can invoke an exception:\n\n\n  The filter throws an exception from within one of its methods\n  The filter completes its future exceptionally - i.e. future.completeExceptionally(throwable)\n\n\nRequests\n\nIf an exception is thrown while processing a request or if the future is completed exceptionally, further processing of\nthe request is terminated and the filter’s onError method is invoked. In other words, in order for the incoming request\nto reach the resource implementation, invocation of all filters’ onRequest methods needs to be successful.\n\nResponses\n\nException/error handling in the context of response filters is a little more involved than in the case of request\nfilters. Response filters are applied to both successful responses as well as all types of errors.\n\nSuch errors can include:\n\n\n  Exceptions thrown by the resource method, including runtime exceptions such as NullPointerException or\n  RestLiServiceException.\n  Exceptions generated by Rest.li due to bugs in resource methods. These could include bugs such as nulls returned\n  directly from the resource methods, or indirectly such as null values inside of returned objects (e.g a null element\n  list inside of a CollectionResult).\n\n\nSubsequently, response filters can transform a successful response from the resource to an error response and vice\nversa. In addition, a successful response from a filter earlier in the filter chain can be transformed into an error\nresponse and vice versa by filters that are subsequent in the filter chain.\n\nThe exception/error handling behavior of response filters is summarized as follows:\n\n\n  If the last filter in the filter chain throws an exception or completes its future exceptionally, an error response\n  is returned to the client corresponding to this exception.\n  If an exception is thrown or the result future is completed exceptionally by any filter except the last filter in the\n  filter chain. The subsequent filter’s onError method is invoked. You can specify error handling in the onError method\n  (i.e. fix the error or propagate it to the next filter).\n  The response that is generated as a result of executing the filter chain is the response that is forwarded to the\n  client. Note that the filter chain can transform a successful/error response from the resource to a error/successful\n  response that’s sent to the client.\n\n\nWhen an exception occurs, the HTTP status code will be automatically set according to this rule:\n\n\n  If the exception is a RestLiServiceException, the status will be taken from the exception.\n  If not, the status will be set to 500 (Internal Server Error).\n\n\nIt is recommended that filters throw a RestLiServiceException.\n\nNote that response headers will be maintained if an exception is thrown, however some new headers signifying an error may be added.\n\nMaking Asynchronous Blocking Calls from Filters\n\nSituations may arise where you may need to make external calls within your filter code. Say for example, there’s an\nexternal Auth service that your service integrates with. Every call that comes to your service should be first routed to\nthe Auth service for approval, and only if the Auth service give you a green light, can your resource process the\nrequest. Let’s say you have a RestLi filter that abstracts away the invocation of the Auth service. One way to implement\nthis Auth filter is as follows:\n\nimport com.linkedin.restli.server.filter.FilterRequestContext;\nimport com.linkedin.restli.server.filter.Filter;\n\npublic class AuthFilter implements Filter\n{\n  @Override\n  public CompletableFuture&lt;Void&gt; onRequest(FilterRequestContext requestContext)\n  {\n    String resourceName = requestContext.getResourceModel().getResourceName();\n    // Now invoke the auth service.\n    Request&lt;Permission&gt; getRequest = builders.get().resourceName(resourceName).build();\n    Permission permission = getClient().sendRequest(getRequest).getResponse().getEntity();\n    log.debug(String.format(\"Received permission %s from auth service for request for %s resource.\",\n                             requestContext.getMethodType(), resourceName));\n    if (permission.isGranted()) \n    {\n       // Since we have permissions, pass the request along.\n       return CompletableFuture.completedFuture(null);\n    } \n    else \n    {\n      throw new RestLiServiceException(HttpStatus.S_401_UNAUTHORIZED, \"Permission denied\");\n    }\n  }\n}\n\n\nThe above implementation makes a synchronous call to an external auth service to authenticate the incoming request.\nAlthough the above implementation is functionally correct, it is not very efficient. Upon close investigation, you’ll\nobserve that the request processing thread of your service is now blocked on an outgoing call to the auth service. If\nthe auth service is slow to respond to requests, very soon it’s possible that all threads of your service is blocked\nwaiting for response from the auth service.\n\nThe Rest.li filters provide a CompletableFuture interface that handles the asynchronous callbacks for you. The\nimplementation is shown below:\n\nimport com.linkedin.restli.server.filter.FilterRequestContext;\nimport com.linkedin.restli.server.filter.Filter;\n\npublic class AuthFilter implements Filter\n{\n  @Override\n  public CompletableFuture&lt;Void&gt; onRequest(FilterRequestContext requestContext)\n  {\n    CompletableFuture&lt;Void&gt; future = new CompletableFuture&lt;Void&gt;();\n    String resourceName = requestContext.getResourceModel().getResourceName();\n    // Now invoke the auth service.\n    Request&lt;Permission&gt; getRequest = builders.get().resourceName(resourceName).build();\n    Callback&lt;Response&lt;Permission&gt;&gt; cb = new Callback&lt;Response&lt;Permission&gt;&gt;()\n      {\n        @Override\n        public void onSuccess(Response&lt;Permission&gt; response)\n        {\n          Permission permission = response.getEntity();\n          log.debug(String.format(\"Received permission %s from auth service for request for %s resource.\",\n                                  requestContext.getMethodType(), resourceName));\n          if (permission.isGranted()) \n          {\n            // Since we have permissions, pass the request along.\n           future.complete(null);\n          } \n          else\n          {\n            future.completeExceptionally(new RestLiServiceException(HttpStatus.S_401_UNAUTHORIZED, \"Permission denied\"));\n          }\n        }\n        @Override\n        public void onError(Throwable e)\n        {\n          future.completeExceptionally(new RestLiServiceException(HttpStatus.S_500_INTERNAL_SERVER_ERROR, e));\n        }\n      }\n    // Invoke the auth service asynchronously. \n    getClient().sendRequest(getRequest, requestContext, cb);\n    return future;\n  }\n}\n\n\nThe above implementation makes an asynchronous blocking call to the external auth service to authenticate the incoming\nrequest. In this implementation, the request processing thread of your service is NOT blocked on an outgoing call to the\nauth service and is free to process more incoming requests for your service. By using CompletableFuture, you can make\noutgoing asynchronous calls from within RestLi filters.\n",
      tags: null,
      id: 14
    });
    
    

  
    index.add({
      title: "Rest.li with Netty",
      category: null,
      content: "Rest.li with Netty\n\nContents\n\n\n  Basics\n  Programmatically calling the Netty Launcher\n  Calling the Netty Launcher from the Command Line\n  Calling the Netty Launcher from a Gradle Task\n\n\nBasics\n\nRest.li can be run in a variety of HTTP frameworks. Out of the box, Rest.li supports both Netty and Servlet containers, such as Jetty.\n\nRest.li includes a restli-netty-standalone artifact containing a single class: com.linkedin.restli.server.NettyStandaloneLauncher.  This launcher class configures a Netty server to dispatch requests to all Rest.li resources that are both in the current classpath and in the list of package names the launcher is provided when it is created.\n\nProgrammatically calling the Netty Launcher\n\n    import com.linkedin.restli.server.NettyStandaloneLauncher;\n    ...\n    NettyStandaloneLauncher launcher = new NettyStandaloneLauncher(\n      8080 /*port*/,\n      \"com.example.fortunes\" /* resource package(s) */\n    );\n    launcher.start();\n    // ... server is running.  launcher.stop() can be called to stop it.\n\n\nThread pool sizes may also optionally be configured using the overloaded constructor.\n\nCalling the Netty Launcher from the Command Line\n\njava -cp &lt;classpath&gt; com.linkedin.restli.server.NettyStandaloneLauncher 8080 com.example.fortunes\n\n\nCalling the Netty Launcher from a Gradle Task\n\n    task startFortunesServer(type: JavaExec) {\n      main = 'com.linkedin.restli.server.NettyStandaloneLauncher'\n      args = ['-port', '8080', '-packages', 'com.example.fortune.impl']\n      classpath = sourceSets.main.runtimeClasspath\n      standardInput = System.in\n    }\n\n\nThread pool sizes may also optionally be configured by providing a number to the ‘-thread’ and ‘-parseqthreads’ args.\n",
      tags: null,
      id: 15
    });
    
    

  
    index.add({
      title: "Rest.li with servlet containers",
      category: null,
      content: "Rest.li With Servlet Containers\n\nContents\n\n\n  Introduction\n  Step 1. Creating a web.xml File\n  Step 2. Building a War\n  Jetty\n\n\nIntroduction\n\nRest.li may be run in a variety of http frameworks. Out of the box,\nRest.li supports both Netty and Servlet\ncontainers, such as Jetty.\n\nThis describes how to run Rest.li on a servlet container by building a\nwar containing a rest.li servlet. It also covers how to run Rest.li with\nJetty.\n\nStep 1. Creating a web.xml File\n\nRest.li provides a RestliServlet class to integrate rest.li into any\nJava servlet container.\n\nTo use RestliServlet, you will add a web.xml file.\n\nFor example, below is\nexample-standalone-app/server/src/main/webapp/WEB-INF/web.xml, an\nextremely simple web.xml example that creates a RestliServlet and\nconfigures it to load all rest.li resources in the\ncom.example.fortune.impl package.\n\nNotice that RestliServlet automatically scans all resource classes in\nthe specified package and initializes the REST endpoints/routes without\nany hard-coded connection. Adding additional resources or operations can\nbe done simply by expanding your data schema and providing additional\nfunctionality in your Resource\nclass(es).\n\nfile: example-standalone-app/server/src/main/webapp/WEB-INF/web.xml\n\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n\\&lt;\\!DOCTYPE web-app PUBLIC ‘-//Sun Microsystems, Inc.//DTD Web\nApplication 2.3//EN’ ‘http://java.sun.com/dtd/web-app_2_3.dtd’\\&gt;\n\n&lt;web-app&gt;  \n&lt;display-name&gt;Fortunes App&lt;/display-name&gt;  \n&lt;description&gt;Tells\nfortunes&lt;/description&gt;\n\n&lt;!-- servlet definitions --&gt;\n\n&lt;servlet&gt;  \n&lt;servlet-name&gt;FortunesServlet&lt;/servlet-name&gt;  \n&lt;servlet-class&gt;com.linkedin.restli.server.RestliServlet&lt;/servlet-class&gt;  \n&lt;init-param&gt;  \n&lt;param-name&gt;resourcePackages&lt;/param-name&gt;  \n&lt;param-value&gt;com.example.fortune.impl&lt;/param-value&gt;  \n&lt;/init-param&gt;  \n&lt;load-on-startup&gt;1&lt;/load-on-startup&gt;  \n&lt;/servlet&gt;\n\n&lt;!-- servlet mappings --&gt;\n\n&lt;servlet-mapping&gt;  \n&lt;servlet-name&gt;FortunesServlet&lt;/servlet-name&gt;  \n&lt;url-pattern&gt;/\\*&lt;/url-pattern&gt;  \n&lt;/servlet-mapping&gt;\n\n&lt;/web-app&gt;  \n\n\nThe parseq thread count (use to make outbound requests) may optionally\nbe configured by setting the parSeqThreadPoolSize servlet init param,\nfor example:\n\n&lt;init-param&gt;  \n&lt;param-name&gt;parseqThreadPoolSize&lt;/param-name&gt;  \n&lt;param-value&gt;10&lt;/param-value&gt;  \n&lt;/init-param&gt;  \n\n\nTo configure jetty’s inbound request thread pool size, see Jetty’s\ndocumentation.\n\nStep 2. Building a War\n\nTo build a war, we need a build.gradle file in the server directory,\nwhich should look like this:\n\nfile: example-standalone-app/server/build.gradle\n\napply plugin: ‘war’  \napply plugin: ‘pegasus’\n\next.apiProject = project(‘:api’)\n\ndependencies {  \ncompile project(path: ‘:api’, configuration: ‘dataTemplate’)  \ncompile spec.product.pegasus.restliServer  \n}  \n\n\nFor on the gradle ‘war’ plugin, see: Gradle War\nPlugin\n\nJetty\n\nTo run Rest.li on Jetty, use the war plugin like you would for any other\nservlet (above). For convenience, you can optionally setup a Gradle task\nto run your application in Jetty. Here’s an example:\n\nfile: example-standalone-app/server/build.gradle\n\napply plugin: ‘war’  \napply plugin: ‘pegasus’\n\next.apiProject = project(‘:api’)\n\ndependencies {  \ncompile project(path: ‘:api’, configuration: ‘dataTemplate’)  \ncompile spec.product.pegasus.restliServer  \n}\n\nconfigurations {  \njetty8  \n}\n\ndependencies {  \njetty8 “org.mortbay.jetty:jetty-runner:8.1.15.v20140411” // set to\nwhatever version of jetty you want to test with  \n}\n\ntask JettyRunWar(type: JavaExec) {  \nmain = “org.mortbay.jetty.runner.Runner”  \nargs = \\[war.archivePath\\]  \nclasspath configurations.jetty8  \n}  \n\n\nTo start Rest.li on Jetty run:\n\ngradle JettyRunWar  \n\n\nThe server will start on port 8080 under the /server context path, for\nexample:\n\ncurli http://localhost:8080/fortunes/1  \n\n",
      tags: null,
      id: 16
    });
    
    

  
    index.add({
      title: "Rest.li to Avro conversions",
      category: null,
      content: "Rest.li to Avro Conversions\n\nContents\n\n  Converting Schemas\n  Converting Data\n  FAQ\n\n\nSometimes it is necessary to convert between Avro and Rest.li formats. That is, either converting schemas (Rest.li DataSchemas to Avro Schemas and vice versa) or converting data (Rest.li DataMaps to Avro GenericRecords and vice versa). Rest.li provides ways to do this using the data-avro module.\n\nConverting Schemas\nThe key class for converting schemas is the SchemaTranslator class.\nConverting Avro to Rest.li\nTo convert from Avro to Rest.li, you will use the avroToDataSchema methods in SchemaTranslator.\nThe default method takes in only the Avro schema you wish to convert as input:\ncom.linkedin.data.avro.SchemaTranslator.avroToDataSchema(avroSchema);\n\nThis schema can either be a stringified version of the Schema or an org.apache.avro.Schema.\n\nThere is also a similar method that also accepts an AvroToDataSchemaTranslationMode.  Generally, this method doesn’t need to be used.  However, if you have embedded your rest.li schema within your Avro schema, and you can use this with the AvroToDataSchemaTranslationMode to speed up the translation process. This is normally done when translating from Rest.li format to Avro format.  See the section for converting from Rest.li to Avro to learn more about this.\n\nConverting Rest.li to Avro\nTo convert from Rest.li to Avro, you will use the dataToAvroSchema methods in SchemaTranslator. Like the avroToDataSchema method, it can take in either a stringified restli schema, or a  DataSchema, and, optionally, a DataToAvroSchemaTranslationOptions\ncom.linkedin.data.avro.SchemaTranslator.dataToAvroSchema(dataSchema);\ncom.linkedin.data.avro.SchemaTranslator.dataToAvroSchema(dataSchema, translationOptions);\n\n\nDataToAvroSchemaTranslationOptions has four parts:\n\n  The translation mode OptionalDefaultMode\n  The JSON style JsonBuilder.Pretty\n  The schema embedding mode EmbedSchemaMode\n  The namespace override flag overrideNamespace\n\n\nOptionalDefaultMode determines how defaults are translated into Rest.li format.  Since Avro requires that a union’s default value always be of the same type as the first member type of the union, if a type is not consistently initialized with a single default type, translations may encounter problems.  By default this value is set to TRANSLATE_DEFAULT, but if your translations are encountering issues around default values, you may wish to set this to TRANSLATE_TO_NULL, which will cause all optional fields with a default value to have their default value set to null in the Avro translation.\n\nJsonBuilder.Pretty simply sets the format of the output JSON.  By default, this is set to COMPACT.\n\nEmbedSchemaMode determines whether or not to embed the original Rest.li schema into the resulting Avro schema.  This can speed translation back (or make a translation back more accurate) to Rest.li format with the correct settings passed to the avroToDataSchema method. By default, this is set to NONE.\n\noverrideNamespace is a boolean flag indicating whether the namespaces of the translated Avro schemas should be overridden. If this flag is set to true, then the namespace of each translated Avro schema will be prepended with a special prefix,\n\"avro.\" (e.g. com.x.y becomes avro.com.x.y). This is helpful in cases where pegasus schemas and their Avro counterparts are included in the same project, potentially causing namespace/package conflicts.\n\nConverting Data\nThe key class for converting data is the DataTranslator class.\nConverting Avro to Rest.li\nTo convert from Avro to Rest.li, you will use the genericRecordToDataMap method in DataTranslator. You’ll need the Avro GenericRecord you are converting, the Avro Schema the GenericRecord conforms to, and the Rest.li RecordDataSchema of the Rest.li type you are converting to:\ncom.linkedin.data.avro.DataTranslator.genericRecordToDataMap(genericRecord, recordDataSchema, avroSchema);\n\nThere are no versions of this method that accept any special options.\n\nConverting Rest.li to Avro\nTo convert from Rest.li to Avro, you will use the dataMapToGenericRecord methods in DataTranslator.  You will need the Rest.li DataMap you are converting, the RecordDataTemplate your DataMap conforms to, and, optionally, the Avro Schema you are converting your data to. If you do not pass in an Avro Schema, then the schema translator will be used to convert your passed in RecordDataSchema to an Avro Schema, using default settings.\ncom.linkedin.data.avro.DataTranslator(dataMap, dataSchema);\ncom.linkedin.data.avro.DataTranslator(dataMap, dataSchema, avroSchema);\n\n\nAutomatically generating avro schemas as part of a build\n\nRest.li will generate avro schemas for all your pegasus schemas (.pdl files) automatically if the build is configured to enable this.\n\nSee Gradle generateAvroSchema Task for details on how to enable.\n\nFAQ\nHow do I get the RecordDataSchema of a particular Record type?\nThe RecordDataSchema field of generated Record classes are private, so you cannot get them directly.  However, there is a helper method in com.linkedin.data.template.DataTemplateUtil called getSchema that can help you get the Schema.  Simply pass in the class of the Record and it will return a basic DataSchema.  If you know this Schema is a RecordDataSchema, you can safely cast the result to RecordDataSchema.\n(RecordDataSchema)com.linkedin.data.template.DataTemplateUtil.getSchema(MyRecord.class);\n\n",
      tags: null,
      id: 17
    });
    
    

  
    index.add({
      title: "Scala-Integration",
      category: null,
      content: "Scala integration\n\nContents\n\n\n  SBT Plugin\n  Writing Resources in Scala\n  Scaladoc\n\n\nSBT Plugin\n\nRest.li is fully integrated with the SBT build system through an SBT plugin.  See Rest.li SBT Plugin.\n\nWriting Resources in Scala\n\nRest.li resource classes may be written in Scala, for example:\n\n**\n * A sample scala resource.\n */\n@RestLiCollection(name=\"sampleScala\", namespace = \"com.example.restli\")\nclass SampleScalaResource extends CollectionResourceTemplate[java.lang.Long, Sample] with PlayRequest {\n\n  /**\n   * A sample scala get.\n   */\n  override def get(key: java.lang.Long): Sample = {\n    val message = \"hello world\"\n\n    new Sample()\n      .setMessage(s\"You got this from a Scala Resource: ${message}!\")\n      .setId(key)\n  }\n}\n\n\nScaladoc\n\nScaladoc is supported using a plugin.  To enable the plugin in Gradle, modify your build.gradle files, adding a dependency on restli-tools-scala and depending on it the module that contains your Rest.li resource Scala classes. Find the latest version of restli-tools-scala on Maven Central.\n\nproject.ext.externalDependency = [\n  // ...\n  'scalaLibrary_2_12': 'org.scala-lang:scala-library:2.12.7',\n  'restliToolsScala_2_12' : 'com.linkedin.sbt-restli:restli-tools-scala_2.12:0.3.9'\n],\n\n\napply plugin: 'pegasus'\napply plugin: 'scala'\n\next.apiProject = ...\ndependencies {\n  // ...\n  compile externalDependency.scalaLibrary_2_12\n  compile externalDependency.restliToolsScala_2_12\n}\n\n",
      tags: null,
      id: 18
    });
    
    

  
    index.add({
      title: "Send Rest.li request query in request Body",
      category: null,
      content: "Send Rest.li request query in request Body\n\nContents\n\n\n  Introduction\n  ClientQueryTunnelFilter and\nServerQueryTunnelFilter\n  Request Without Body\n  Request With Body\n\n\nIntroduction\n\nThe Rest.li protocol specifies what HTTP method will be used for each type of Rest.li request. However, sometimes due to security constraint  (i.e. not wanting to send some sensitive information in URI) or jetty buffer limitation (i.e. there may pose a threshold on the longest query that can go through), it may be required to customize the HTTP method used to send a particular Rest.li request to a Rest.li server.\n\nClientQueryTunnelFilter and ServerQueryTunnelFilter\n\nRest.li has provided\nClientQueryTunnelFilter\n(R2 client filter) and\nServerQueryTunnelFilter\n(R2 server filter) to support such HTTP method customization. These two\nfilters behaves as follows:\n\n\n  On sending a rest request from client, ClientQueryTunnelFilter will\ninvoke\nQueryTunnelUtil\nencoding function to encode a standard non-POST rest.li request by\nmoving the query param line into the body, and reformulating the\nrequest as a POST. The original method is specified by the\nX-HTTP-Method-Override header. This header is important to\nindicate that on the server side we should invoke QueryTunnelUtil\ndecoding function to get back original request so that this\nconversion looks completely transparent to users. User can indicate\nwhether they want to perform such encoding in two ways:\n\n\n\n\n\n  Either by specifying a queryPostThreshold in initializing\nClientQueryTunnelFilter. That means if the rest request raw query\nlength is greater than this specified queryPostTreshold,\nClientQueryTunnelFilter will automatically convert this request to\nPOST.\n  Or by forcing request conversion to specify in request context\n    \n       requestContext.putLocalAttr(R2Constants.FORCE_QUERY_TUNNEL, true);\n   \n\n\n\n\n\n  On receiving a rest request at server, ServerQueryTunnelFilter will\ninvoke QueryTunnelUtil.decode to convert request back to its\noriginal form based on a special header set by\nClientQueryTunnelFilter, that is, X-HTTP-Method-Override.\n\n\nRequest Without Body\n\nFor a rest.li request without body, for example, a BATCH_GET request\nlike this http://localhost?ids=List(1,2,3), the transformed POST\nrequest is x-www-form-urlencoded with query params stored in the body,\nas follows:\n\ncurl -X POST http://localhost \\\n  -H \"X-HTTP-Method-Override: GET\" \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  --data $'ids=1,2,3' \n\n\nBasically,\n\n\n  change GET to POST (-X POST)\n  add X-HTTP-Method-Override header for original HTTP method (-H\n\"X-HTTP-Method-Override: GET\")\n  add Content-Type header (-H \"Content-Type:\napplication/x-www-form-urlencoded\")\n  move all the query string to body (@—data $’ids=List(1,2,3)’)\nNote that if QueryTunnelUtil need to do actual encoding or decoding,\nthe request would be fully buffered first,\neven if it’s streaming.\nWe believe QueryTunnelUtil is almost exclusively for GET requests,\npractically no use case would require\nexcessively long query for POST &amp; PUT, and we’d be surprised if\nanyone is using QueryTunnelUtil for PUT &amp; GET.\nHence, fully buffering request that has to be encoded/decoded is\npractically not a problem and gives up the best\nreturn for the investment of our efforts.\n\n\nRequest With Body\n\nFor a rest.li request with body, for example, a BATCH_UPDATE request\nlike this http://localhost?ids=List(1,2,3), the transformed POST\nrequest will be of Content-Type of multipart/mixed with 2 sections:\n\n\n  The first section should be of type x-www-form-urlencoded and\ncontain the query params\n  The second should contain what would have been the original body,\nalong with it’s associated content-type.\n\n\nIt will look as follows:\n\ncurl -X POST http://localhost \\\n  -H \"X-HTTP-Method-Override: PUT\" \\\n  -H \"Content-Type: multipart/mixed; boundary=xyz\" \\\n  --data $'--xyz\\r\\nContent-Type: application/x-www-form-urlencoded\\r\\n\\r\\nids=List(1,2,3)\\r\\n--xyz\\r\\n \n         Content-Type: application/json\\r\\n\\r\\n{\"foo\":\"bar\"}\\r\\n--xyz--' \n\n\nHere,\n\n\n  change GET to POST (-X POST)\n  add X-HTTP-Method-Override header for original HTTP method (-H\n\"X-HTTP-Method-Override: GET\")\n  add Content-Type header (-H \"Content-Type: multipart/mixed;\nboundary=xyz\"). Note that here we need to specify a boundary\ndelimiter (here we use xyz for illustration) for multipart body,\nthis delimiter needs to be unique and not appearing in your request\ncontent body or url.\n  move all the query string and original request body to body as two\nsections explained above.\n\n",
      tags: null,
      id: 19
    });
    
    

  
    index.add({
      title: "Spring dependency injection with Rest.li",
      category: null,
      content: "Spring Dependency Injection With Rest.li\n\nSpring dependency injection may be used with Rest.li by using the\nrestli-spring-bridge\nmodule.\n\nAn example project using spring dependency injection is available in the\nRest.li codebase:\n\nhttps://github.com/linkedin/rest.li/tree/master/examples/spring-server\n\nTo use it, first add a dependency to the restli-spring-bridge module,\nfor example:\n\ndependencies {  \ncompile “com.linkedin.pegasus:restli-spring-bridge:1.9.23”  \n}  \n\n\nThen, wire in a Rest.li server and a RestliHttpRequestHandler. For an\nexample, see\n/examples/spring-server/server/src/main/webapp/WEB-INF/beans.xml\n\nLastly, use Spring’s HttpRequestHandlerServlet to run the\nRestliHttpRequestHandler as a servlet. For an example, see\nexamples/spring-server/server/src/main/webapp/WEB-INF/web.xml\n",
      tags: null,
      id: 20
    });
    
    

  
    index.add({
      title: "Validation in Rest.li",
      category: null,
      content: "Validation In Rest.li\n\nContents\n\n\n  Specifying Validation Rules\n  Custom Validation Rules\n  Rest.li Validation Annotations\n  Using the Rest.li Data Validator For Servers\n  Using the Rest.li Data Validator For Clients\n  Validating Data Without Rest.li Context\n  Backwards Compatibility\n\n\nThere are many situations in which incoming or outgoing data should be\nvalidated. For example, when creating a record which stores user\ninformation, you may want to check that the username field doesn’t\ncontain special characters, or you may want to ensure that all required\n(non-optional) fields are present before processing the incoming record.\nWhen a required field is not present (and doesn’t have a default value\neither), calling getXXX() will throw a\nRequiredFieldNotPresentException. So validating the entity beforehand\ncan simplify the error handling code because you wouldn’t need to call\nhasXXX() for every single field before calling getXXX(). Also, you may\nwant to check that clients are not trying to modify certain fields, like\nserver-generated ids or timestamps. Rest.li validation provides a\nmechanism to perform such validation.\n\nTo use Rest.li’s validation feature, you need to specify validation\nrules and choose how to enable validation.\n\nSpecifying Validation Rules\n\nRest.li validates using three types of rules:\n\n\n  Schema validation rules (checking the type of field values and the\npresence of required fields)\n  Custom validation rules (checking whether a number is in some range,\nmatching a string to a regular expression, etc)\n  Rest.li validation annotations (@ReadOnly and @CreateOnly)\n\n\nThe first two rules are specified in the data schema (.pdl file), and\nthe third one is specified in the resource implementation. This is\nbecause the first two rules only deal with information in the data\nitself, but the third rule also needs additional Rest.li context such as\nthe method type. For example, a read only field shouldn’t be included in\na create request, but should be included in a get response.\n\nFor partial update requests (patches), the goal is to ensure that if the\npatch is applied to a valid entity, the modified entity will also be\nvalid. For example, a patch that deletes a required field is invalid\nbecause the modified entity will be missing that field. If there is a\ncustom validation rule on the username field that it must be at least 3\ncharacters, a patch setting the username to “AA” is invalid.\n\nYou can read more about required and optional fields in Data Schemas - PDL Syntax - Record Type.\n\nCustom Validation Rules\n\nRest.li includes some customizable\nvalidators,\nsuch as “strlen” and “regex”, that can be added to schemas. Developers\ncan write additional validators for any specific need.\n\nFor example, to use “strlen” to validate a string between 1 and 20 chars\nlong, we add it to the “validate” map of the field in the schema, for\nexample:\n\nnamespace com.example\n\nrecord Fortune {\n  @validate.strlen = {\"min\": 1, \"max\": 20}\n  message: string\n}\n\n\nValidator names are case sensitive and must have a matching validator\nJava class in the current classpath. Rest.li finds the validator class\nby uppercasing the first letter of the validator name and appending the\n“Validator” suffix. For example, “strlen” maps to\nStrlenValidator.\nDevelopers writing additional validators only need to write a class\nextending AbstractValidator and include it in the classpath to use it.\n\nAdditional details are described in the javadoc for\nDataSchemaAnnotationValidator.\n\nRest.li Validation Annotations\n\nFields in a pegasus data schema can be either required or optional.\nHowever, there are certain cases where this distinction is not\nexpressive enough. For example, a client shouldn’t send a\nserver-generated id in a create request (i.e. id is optional), but the\nserver must send the id when responding to a get request (i.e. id is\nrequired). To cover cases like this, we introduce two new Rest.li\nvalidation annotations: @ReadOnly and @CreateOnly.\n\nA @ReadOnly field cannot be set or changed by the client, ex.\nserver-generated ID field. A @CreateOnly field can be set the client\nwhen creating the entity, but cannot be changed by the client\nafterwards. This annotation implies that the field is immutable, ex. a\npurchase price. The client will send the price to the CREATE method\nwhile creating a purchase entry.\n\nAs a best practice, Rest.li validation annotation should not conflict\nwith schema validation rules and custom validation rules specified in\nthe schema. For example, @ReadOnly should only be used to enforce that\nan optional field is not present. It should not be specified for a\nrequired field, making missing required field value valid.\n\nThe Rest.li data\nvalidator\nwill enforce the following rules for fields in request data based on the\nannotation:\n\nNote that validation is not turned on by default, and servers have to manually call the validator or use the validation filter.\n\n\n  \n    \n       \n      Create\n      Partial Update\n    \n  \n  \n    \n      @ReadOnly\n      Must not be present (See notes below)\n      Must not be present\n    \n    \n      @CreateOnly\n      N/A\n      Must not be present\n    \n  \n\n\nBatch create, batch update, and batch partial update are treated the\nsame as create, update, and partial update respectively.\nNotes on @ReadOnly Validation Rules for Create Request\n\nIf @ReadOnly is specified to a field that is required in schema, the\nfield is treated as optional and the validation rule enforces that the\nfield is not present in the Create request data.\n\nValidation for Update Requests\n\nUpdate methods can be used in two scenarios:\n\n  As a PUT method, that updates the whole entity. The client should’ve\nfetched the original entity, updated it and then called UPDATE with the\nfull entity. In this case, both @ReadOnly and @CreateOnly marked fields\nshould be present.\n  As CREATE when UPDATE is used to as UPSERT method. In this scenario, UPDATE\nis used both to update or create the entity (if it is not already present). When\nthe entity is being created, the @ReadOnly fields may be absent and the @CreateOnly\nfields may be present. Similarly, when the entity is be updated, both sets of fields\nwill be present (similar to scenario 1).\n\n\nSo to support both these scenarios, the validation for Update requests is relaxed\nto allow @ReadOnly and @CreateOnly fields to be present and for @ReadOnly\nfields to be optional (when the field is marked as required in schema).\n\nIn the update request, when the @ReadOnly or @CreateOnly fields are present, and if \nthe request is updating an existing entity, they are expected to have the same value as\nthe original entity (if the field was missing from the original entity,\nit should be missing in the update request too). However, this is not\nchecked by the Rest.li framework and should be checked manually in the\nresource implementation.\n\nSpecifying Rest.li Validation Annotations\n\nRest.li validation annotations are specified on top of the resource. For\nexample,\n@RestLiCollection(name = \"photos\", namespace = \"com.linkedin.restli.example.photos\")\n@ReadOnly(\"urn\")  \n@CreateOnly({\"/id\", \"/EXIF\"})\npublic class PhotoResource extends CollectionResourceTemplate&lt;Long, Photo&gt;\n{\n  ...\n}\n\nEvery path should correspond to a field in a record, and not an enum\nconstant, a member of a union, etc. Paths should be specified in the\nformat used by\n“PathSpec”:https://github.com/linkedin/rest.li/blob/master/data/src/main/java/com/linkedin/data/schema/PathSpec.java.\nNote that the first / character can be either specified or omitted. You\ncan check the correct path for a field by getting its PathSpec and\ncalling toString(). For example, if the ValidationDemo record contains\nan array field like this:\n\nrecord ValidationDemo {\n  ArrayWithInlineRecord: optional array[record myItem {\n    bar1: string,\n    bar2: string\n  }]\n  // ...\n}\n\n\nValidationDemo.fields().ArrayWithInlineRecord().items().bar1().toString()\nwill return /ArrayWithInlineRecord/*/bar1.\n\nYou can also refer to these rules:\n\n  For a non-nested field, put the field name. e.g. “stringA”\n  For a nested field, put the full path separated by / characters. e.g.\n“location/latitude”\n  For a field of an array item, specify the array name followed by the wildcard and the field name. e.g. “ArrayWithInlineRecord/*/bar1”\n  Similarly, for a field of a map value, specify the map name followed by the wildcard and the field name.\n  For a field of a record inside a union, specify the union name, followed by the fully qualified record schema name, and then the field name. e.g. “UnionFieldWithInlineRecord/com.linkedin.restli.examples.greetings.api.myRecord/foo2”\n\n\nBecause full paths are listed, different rules can be specified for\nrecords that have the same schema. For example, if the schema contains\ntwo Photos, you can make the id of photo1 ReadOnly and id of photo2\nnon-ReadOnly. This is different from the optional/required distinction\nspecified in data schemas, where if the id of photo1 is required, the\nid of photo2 will also be required.\n\nUsing the Rest.li Data Validator For Servers\n\nThe Rest.li data validator can be called directly or indirectly using\nthe validation filters. When the resource calls it directly, it gets\nspecific error messages about where and why the validation failed, and\ncan decide how to handle the error. When a filter is used instead, the\nresource does not get to examine why validation failed. Instead, the\nclient gets an error response with a message describing why the\nvalidation failed. The validation filters are convenient if you want to\nsimply discard invalid requests or responses. On the other hand, if you\nneed to log the error or fail requests for only certain types of errors,\nyou need to call the validator directly.\n\nRequest validation before resource handling\n\nThe RestLiValidationFilter\nrejects all invalid requests automatically. It sends a 422\n(Unprocessable Entity) error response back to the client if the data is\ninvalid. A sample error message is:\nERROR :: /stringA :: ReadOnly field present in a create request\nERROR :: /stringB :: field is required but not found and has no default value\n\n\nResponse validation after resource handling\n\nThe RestLiValidationFilter also discards all invalid responses. The\nfilter sends a 500 error response back to the client if the response is\ninvalid. A sample error message is:\nERROR :: /stringA :: length of “Lorem ipsum dolor sit amet” is out of range 1...10\nERROR :: /stringB :: field is required but not found and has no default value\n\n\nRest.li Filters explains how to install the filter.\n\nRequest validation during resource handling\n\nTo use the Rest.li data validator explicitly, it needs to be declared as\na method parameter using the @ValidatorParam annotation.\nFor example, to validate the input of a create request,\n@RestMethod.Create  \npublic CreateResponse create(final Fortune entity, @ValidatorParam RestLiDataValidator validator)\n{\n  ValidationResult result = validator.validateInput(entity);  \n  if (!result.isValid())  \n  {  \n    throw new RestLiServiceException(HttpStatus.S_422_UNPROCESSABLE_ENTITY, result.getMessages().toString());  \n  }  \n  ...\n}\n\n\nBatch requests have to be validated one by one:\n@RestMethod.BatchPartialUpdate\npublic BatchUpdateResult&lt;Integer, Fortune&gt; batchUpdate(BatchPatchRequest&lt;Integer, Fortune&gt; updates, @ValidatorParam\nRestLiDataValidator validator)\n{\n  for (Map.Entry&lt;Integer, PatchRequest&lt;Fortune&gt;&gt; entry : entityUpdates.getData().entrySet())\n  {\n    Integer key = entry.getKey();\n    PatchRequest&lt;Fortune&gt; patch = entry.getValue();\n    ValidationResult result = validator.validateInput(patch);\n    if (result.isValid())\n    {\n      // update entity\n    }\n    else\n    {\n      errors.put(key, new RestLiServiceException(HttpStatus.S_422_UNPROCESSABLE_ENTITY, result.getMessages().toString()));\n    }\n  }\n  ...\n}\n\n\nThe ValidationDemoResource\nclass shows how to use the validator for each resource method type.\n\nResponse validation during resource handling\n\nSimilar to request validation, the Rest.li data validator needs to be\ndeclared as a method parameter.\nFor example:\n\n@RestMethod.BatchGet\npublic Map&lt;Integer, Fortune&gt; batchGet(Set&lt;Integer&gt; ids, @ValidatorParam RestLiDataValidator validator)\n{\n  Map&lt;Integer, Fortune&gt; resultMap = new HashMap&lt;Integer, Fortune&gt;();\n  ...\n  for (Fortune entity : resultMap.values())\n  {\n    ValidationResult result = validator.validateOutput(entity);\n    if (!result.isValid())\n    {\n      // fix the entity\n    }\n  }  \n  ...\n}\n\n\nUsing the Rest.li Data Validator For Clients\n\nRequest validation\n\nClients can validate requests before sending it to the server, to ensure\nthat the request wouldn’t be rejected by the server. Request builders\nfor create, update, partial update and their respective batch operations\nhave validateInput() methods.\n\nPhoto newPhoto = new Photo().setTitle(“New Photo”).setFormat(PhotoFormats.PNG).setExif(newExif);\nValidationResult validationResult = PhotosCreateRequestBuilder.validateInput(newPhoto);\nif (validationResult.isValid())\n{\n  // send request\n}\nelse\n{\n  // fix photo\n}\n\n\nInput data for batch requests have to be validated one by one:\n\nfor (PatchRequest&lt;Photo&gt; patch : patches)\n{\n  ValidationResult validationResult = PhotosPartialUpdateRequestBuilder.validateInput(patch);\n  if (!validationResult.isValid())\n  {\n    // fix patch\n  }\n  ...\n}\n\n\nResponse validation\n\nWhen validating data returned by the server, clients have to use the ValidateDataAgainstSchema class as explained below.\n\nValidating Data Without Rest.li Context\n\nWhen ReadOnly or CreateOnly annotations are used, Rest.li context\n(method type, request vs response) is necessary to validate the data.\nOtherwise the data and the schema information is enough. Data to Schema\nValidation\nexplains how to validate data using the ValidateDataAgainstSchema\nclass. If it is used with DataSchemaAnnotationValidator, it will\nconsider the first two types of rules out of three listed in\nSpecifying Validation Rules.\n\nFor example:\n\n// Send the request to the server and get the response\nfinal Photo photo = restClient.sendRequest(request).getResponse().getEntity();\nDataSchemaAnnotationValidator validator = new DataSchemaAnnotationValidator(photo.schema());\nValidationResult result = ValidateDataAgainstSchema.validate(photo.data(), photo.schema(), new ValidationOptions(), validator);\nif (!result.isValid())\n{\n  // handle the error\n} \n\n\nBackwards Compatibility\n\nAdding or removing a @ReadOnly or @CreateOnly annotation for a field is considered backwards incompatible. When an annotation is added, old client’s create or partial update requests may fail validation because they still contain the field in the request. When an annotation is removed, clients may have to send fields that they didn’t have to before.\n",
      tags: null,
      id: 21
    });
    
    

  
    index.add({
      title: "Writing-unit-tests-for-Rest.li-clients-and-servers",
      category: null,
      content: "Writing Unit Tests for Rest.li\n\nContents\n\n\n  Introduction\n  Writing Unit Tests for Rest.li Clients\n  Writing Unit Tests for Rest.li Servers\n  Writing Unit Tests for Rest.li Data (RecordTemplates and DataMaps)\n\n\nIntroduction\n\nThe Rest.li team added classes to Rest.li (starting with Rest.li 1.14.7) to make writing unit tests for Rest.li clients and servers easier. These classes are spread across three modules: restli-client-testutils, restli-common-testutils, and restli-server-testutils.\n\nWriting Unit Tests for Rest.li Clients\n\nThe classes that help writing unit tests for Rest.li clients are present in restli-client-testutils. These classes are mainly builders and factories that help in creating different types of com.linkedin.restli.client.Response objects. The expected use case for these classes is where you want to test that your code is able to process a Response object that it receives from making a Rest.li request.\n\nSuppose you want to test a method that makes a GET request which returns a Greeting entity and then this method returns the property message from this entity. Your code that you want to test might look something like this:\n\npublic class MyApplication\n{\n  ...\n  \n  public String getMessage(long id) throws Exception  \n  {\n    GetRequest&lt;Greeting&gt; getRequest = GREETINGS_BUILDERS.get().id(id).build();\n    return _restClient.sendRequest(getRequest).getResponseEntity().getMessage();\n  }\n  \n  ...\n}\n\n\nTo test this method, your code might look like this:\n\nGetRequest&lt;Greeting&gt; expectedRequest = GREETINGS_BUILDERS.get().id(id).build();\n\n// use the test utilities to construct a response that is processed by the method\n// we are testing\nGreeting mockGreeting = new Greeting().setId(1L).setMessage(\"test message\");\nMockSuccessfulResponseFutureBuilder&lt;Greeting&gt; responseFutureBuilder = \n  new MockSuccessfulResponseFutureBuilder&lt;Greeting&gt;().setEntity(mockGreeting);\nResponseFuture&lt;Greeting&gt; mockResponse = responseFutureBuilder.build();\n\n// assume myApplication is an instance of MyApplication\n// assume mockClient is a mock RestClient that has been created using EasyMock\nEasyMock.expect(mockClient.sendRequest(expectedRequest)).andReturn(mockResponse);\nAssert.assertEquals(myClass.getMessage(1L), \"test message\");\n\n\nMockResponseBuilder\n\nMockResponseBuilder is a builder that can be used to easily construct Response objects. You can use it to set the headers, entity, HTTP response code, and so on, of the Response object. The use case that we envision for this class is when you are testing code that uses a version of sendRequest in RestClient that takes in a CallBack&lt;Response&lt;T&gt;&gt; as one of it’s parameters. The CallBack argument can be filled in using this builder.\n\nMockSuccessfulResponseFutureBuilder\n\nThe MockSuccessfulResponseFutureBuilder class is used to construct ResponseFutures that represent a successful Rest.li call, for example, no exceptions are thrown and the HTTP status code is 2xx. This builder can be used to set the entity, HTTP response code, and so on, of the response.\n\nMockFailedResponseFutureBuilder\n\nThe MockFailedResponseFutureBuilder class is used to construct ResponseFutures that represent either a failed Rest.li call. This failed call can be of two types:\n\n\n  The call failed completely and the server did not return any result. For example, you make a GET request but there is no entity for that ID, resulting in a 404.\n  The call failed but the server still sent a response. For example you make a GET request and the server returns a partially constructed entity with some fields not filled out because of an internal server problem. In this case, there is an entity in the response as well as an error.\n\n\nPlease see the JavaDoc on this class for a detailed explanation on how each of the scenarios above can be modeled. This builder also lets us define how server exceptions should be treated using the setErrorHandlingBehavior(ErrorHandlingBehavior errorHandlingBehavior) method. Again, please see the JavaDoc for more details.\n\nMockBatchCreateIdResponseFactory, MockBatchKVResponseFactory, MockBatchEntityResponseFactory, MockBatchResponseFactory, and MockCollectionResponseFactory\n\nThe other classes in this module, namely MockBatchCreateIdResponseFactory, MockBatchKVResponseFactory, MockBatchEntityResponseFactory, MockBatchResponseFactory, and MockCollectionResponseFactory are used to construct specific types of RecordTemplate objects. The use case for these classes is that they will be used to construct a specific type of entity which can then be used in the MockResponseBuilder, MockSuccessfulResponseFutureBuilder, or MockFailedResponseFutureBuilder as the entity object. For example, say you are making a request to a Rest.li finder on a /greetings resource. You can model the fact that this request returns a CollectionResponse&lt;Greeting&gt; as follows:\n\n// assume \"data\" is what we want to return\nCollectionResponse&lt;Greeting&gt; mockResponse = MockCollectionResponseFactory.create(Greetings.class, data);\nMockSuccessfulResponseFutureBuilder&lt;CollectionResponse&lt;Greeting&gt;&gt; responseFutureBuilder = \n  new MockSuccessfulResponseFutureBuilder&lt;CollectionResponse&lt;Greeting&gt;&gt;().setEntity(expectedGreeting);\n// use the ResponseFuture that you will get from the builder in tests\n\n\nWriting Unit Tests for Rest.li Servers\n\nMockHttpServerFactory\n\nThe MockHttpServerFactory class is used to create a stand-alone Rest.li server easily that you can then write tests against. The primary use case for this class would be to bring up a Rest.li server at the start of the test, and then each test can test a different endpoint that this server supports. This factory also allows us to pass in beans that would have been injected into your Rest.li server by the Rest.li framework. For more information on bean injection in a Rest.li server, see the documentation on Dependency Injection.\n\nSuppose we want to test that we have implemented the GET method correctly for our resource GreetingsResource. Let’s assume that GreetingsResource needs one bean named “db” of type DataBase to be injected. Here is what our test might look like (assuming we are using TestNG):\n\n\npublic class TestGreetingsResource\n{\n  private HttpServer _testServer;\n  private Client _restClient;\n  \n  private static final int PORT = 7777;\n  private static final String HOST = \"http://localhost:\" + PORT;\n  private static final GreetingsBuilders GREETINGS_BUILDERS = new GreetingsBuilders();\n  \n  @BeforeTest\n  public void init()\n  {\n    DataBase testDataBase = new TestDataBase();\n    Map&lt;String, Object&gt; beans = new HashMap&lt;String, Object&gt;();\n    beans.put(\"db\", testDataBase);\n    Set&lt;Class&gt; resourceClasses = new HashSet&lt;Class&gt;();\n    resourceClasses.put(GreetingsResource.class);\n    \n    // use the factory to create a test server\n    _testServer = MockHttpServerFactory.create(PORT, resourceClasses, beans, true);\n\n    // start the server\n    _testServer.start();\n    \n    // initialize other members\n  }\n  \n  @Test\n  public void testGet()\n  {\n    GetRequest&lt;Greeting&gt; getRequest = GREETINGS_BUILDERS.get().id(1L).build();\n    Greeting expectedGreeting = new Greeting().setId(1L).setMessage(\"test greeting\");\n    // the request sent by the _restClient goes to our _testServer. \n    Greeting actualGreeting = _restClient.sendRequest(getRequest).getResponseEntity();\n    Assert.assertEquals(actualGreeting, expectedGreeting);\n  }\n  \n  // other tests\n  \n  @AfterTest\n  public void cleanup()\n  {\n    _testServer.stop();\n  }\n}\n\n\n\nWriting Unit Tests for Rest.li Data (RecordTemplates and DataMaps)\n\nThe restli-common-testutils module contains DataAssert, a class which allows you to compare DataMaps and RecordTemplates.\n\nDataAssert\n\nThe DataAssert class contains methods that allow you to compare two DataMaps, RecordTemplates or Collections of RecordTemplates. These methods print out an easy to read output in case of an inequality, with the error message containing only the field(s) which makes the passed in objects not equal to each other. This is better than using a regular JUnit or TestNG Assert.assertEquals method as it might be hard to figure which field(s) caused the inequality, since the vanilla Assert.assertEquals print out the toString() of both objects which can be hard to read if there are many fields. Internally, this class uses TestNG to run assertions.\n\nYou can also specify ValidationOptions that will be applied to the passed in RecordTemplates before any comparison takes place. Please see the Javadoc for more details. Similarly, when comparing two DataMaps you can also specify that null is the same as an empty DataList or DataMap. Again, please see the Javadoc for more details.\n\nHere is an example on how to use this class:\n\n// we make a request to our server for which we know the expected result.\n// we then compare the actual result with the expected result.\nGreeting actualGreeting = _restClient.sendRequest(request).getResponseEntity();\nGreeting expectedGreeting = new Greeting().setId(1L).setMessage(\"Greeting 1\");\n// we do not want to perform fix-up or coercion, hence we pass in null as the last argument.\nDataAssert.assertRecordTemplateDataEqual(actualGreeting, expectedGreeting, null);\n\n\n",
      tags: null,
      id: 22
    });
    
    

  
    index.add({
      title: "Introduction of Annotation Processor",
      category: null,
      content: "Introduction of Annotation Processor\n\n\n  Introduction of Annotation Processor\n    \n      Annotating Pegasus Schemas\n      Inherit and override schema annotations\n        \n          A common application of overriding annotation using PathSpec\n        \n      \n      Use the schema annotation processor\n    \n  \n\n\nThis page introduces a generic schema annotation processing tool in Rest.li. Before reading this, it is recommended that readers be familiar with the concept of PathSpec in Rest.li.\n\nAnnotating Pegasus Schemas\nIn Rest.li, schema authors are able to add annotations using ‘@’ sytnax to schema fields or the schema itself.\n\nExample: annotate on the schema fields\nrecord UserPersonallyIdentifiableInformation {\n  @data_classification = \"MEDIUM\"\n  firstName: string,\n  @data_classification = \"MEDIUM\"\n  lastName: string,\n  @data_classification = \"LOW\"\n  userId: long,\n  @data_classification = \"HIGH\"\n  socialSecurityNumber: string,\n}\n\n\nExample: annotate directly on the schema\n@data_classification = \"HIGH\"\nrecord UserPersonallyIdentifiableInformation {\n  firstName: string,\n  lastName: string,\n  userId: long,\n  socialSecurityNumber: string,\n}\n\nNote that the “data_classifcation” annotation is specified at record level.\n\nThese annotations in above examples are stored as an attribute inside DataSchema’s class as “property”. Just as the example shows, both the field and the DataSchema can have this “property”. Rest.li framework did not provide specification on how these annotations should be interpreted and it was left to the user to add logic for interpreting them.\n\nInherit and override schema annotations\nRest.li users found it useful to process annotations during schema processing. One use case is to introduce “inheritance” and “overrides” to annotations so those annotations can be dynamically processed in the way user defines when the schemas were reused. This gives annotation extensibility and adds to schema reusability.\n\nHere are examples: \nExample case 1 : Users might want the annotation of a field to be inherited. The fields can inherit the annotations from the upper level.\n@persistencePolicyInDays = 30\nrecord UserVisitRecord {\n  visitedUrls: array[URL]\n  visitedUserProfiles: array[UserRecord]\n}\n\nReading from the schema, we might find both visitedUrls and visitedUserProfiles have persistencePolicyInDays annotated as 30.\n\nExample case 2: Users might want the annotation of a field to be overriden. Override is where the annotation on a field or a schema might get updated when other annotations assign it another value.\n@persistencePolicyInDays = 30\nrecord  UserVisitRecord {\n  @persistencePolicyInDays = 10\n  visitedUrls: array[URL]\n\n  visitedUserProfiles: array[UserRecord]\n}\n\nReading from the schema, a user might find the field visitedUserProfiless persistencePolicyInDays annotation is 30 but visitedUrls’s persistencePolicyInDays annotation has value 10, which overrides the original value inherited (i.e. the value 30).\n\nExample case 3:  Override might also happen when the annotation needs to be updated by another value assigned from other annotation locations\nrecord UserVisitRecord {\n  //...\n  @persistencePolicyInDays = 365\n  recycledChatHistories: array[chat]\n\n  @persistencePolicyInDays = 10\n  visitedUrls: array[URL]\n  //...\n}\n\nrecord EnterpriseUserRecord {\n  userName: UserName\n  //...\n  @persistencePolicyInDays = {\"/recycledChatHistories\" : 3650}\n  visitRecord: UserVisitRecord\n  //...\n}\n\nIn this example, the schema EnterpriseUserRecord reused UserVisitRecord and its annotation in the field visitRecord, and overrides the annotation value for the field recycledChatHistories.\n\nAll above examples shows that inheritance and overrides give more extensibility to schema annotations. Users should be free to define their own rules regarding how those annotations are read. There should be an unambiguous annotation value for the fields after the user’s rules are applied. The processing step to figure out the eventual value of a field or schema’s annotation is called “resolution” and such value is called “resolved” value.\n\nWhat is more, users could have their own customized logic to process the annotations and define the custom behavior when annotations are overridden or inherited, and even have the flexibility to set a customized resolved value by calling another local function or remote procedure. The need for such extensibility gives the motivation for Annotation Processor. We aim to build a tool that can process annotations during schema resolution, process all interested annotations by plugging in the user’s own logic and saving the “resolved” value back to the corresponding DataSchema.\n\nA common application of overriding annotation using PathSpec\nIn the above EnterpriseUserRecord example, @persistencePolicyInDays = {\"/recycledChatHistories\" : 3650} were used to denote the field that are being overriden is recycledChatHistories. This is basically using PathSpec as a reference to the field or child schema that needs to be overriden. PathSpec can be used as a path to define relationships between fields among nested schemas. Users can use PathSpec to unambiguously specify all the paths to child fields that they want to override.\n\nThe above usage is very common at LinkedIn so an implementation of annotation processing based on this behavior is provided as part of annotation processor.\n\nIt assumes \n(1) All overrides to the fields are specified using PathSpecs to the fields. \n(2) All overrides are pointing to fields, and not to the record.\n\nFor more examples regarding the syntax, one can read the java doc from PathSpecBasedSchemaAnnotationVisitor, schema processor tests and schema test examples using PathSpec based overriding.\n\nUsers can seek to extend this case to adapt to their own use cases. Please see next section regarding what class to extend to fit the best use cases.\n\nUse the schema annotation processor\nWe have created a paradigm of processing Rest.li schema annotations and wrapped them into com.linkedin.data.schema.annotation package.\nFirst thing to understand is how DataSchema object is internally stored. We added resolvedProperties attribute, in order to store the final resolved annotation.\n\nFor an example schema:\n\nrecord Employee {\n  id: long\n  supervisor: Employee\n}\n\n\nIts memory presentation is as followed:\n\nDataSchema Example\n\n\nThe SchemaAnnotationProcessor will process DataSchema using DataSchemaRichContextTraverser and resolve the annotation for fields in the schemas.\n\n\nSequence Diagram\n\n\nThe DataSchemaRichContextTraverser traverse the data schema and in turn calls an implementation of SchemaVisitor. It is the SchemaVisitor that resolves the annotations for fields in the schemas, based on the context provided by the DataSchemaRichContextTraverser. This process would eventually produces a copy of the origianl schema to return, because the original schema should not be modified during traversal.\n\n\nClass Diagram\n\n\nIf the overriding of annotations is specified using the PathSpec syntax, the PathSpecBasedSchemaAnnotationVisitor and SchemaAnnotationHandler class are the ones to use for such use case. If a user wants to customize this, one should either look for re-implementing the SchemaVisitor, or extending PathSpecBasedSchemaAnnotationVisitor, or simply implement SchemaAnnotationHandler\n\n\n  \n    \n      Situation\n      Recommendation\n    \n  \n  \n    \n      Annotation based on PathSpec based overriding\n      Create a SchemaAnnotationHandler of your own implementation\n    \n    \n      Annotation based on PathSpec with custom traversal context handling\n      Override PathSpecBasedSchemaAnnotationVisitor and optionally create a SchemaAnnotationHandler of your own implementation\n    \n    \n      Annotation not using PathSpec based overriding\n      Implement your own SchemaVisitor\n    \n    \n      Annotation needs custom way of traversal\n      Override DataSchemaRichContextTraverser\n    \n  \n\n",
      tags: null,
      id: 23
    });
    
    

  
    index.add({
      title: "API reference",
      category: null,
      content: "TODO\n",
      tags: null,
      id: 24
    });
    
    

  
    index.add({
      title: "Avro Translation",
      category: null,
      content: "Avro Translation\n\nContents\n\n\n  Translating Schemas to and from Avro\n  Translating Data to and from Avro\n\n\nTranslating Schemas to and from Avro\n\nThe schema and data translators inspect your classpath to determine\nwhich version of avro you are using and require you have the matching\npegasus `data-avro-\\` adapter module in your classpath.\n\nFor example, if you are using avro 1.6, you must add a dependency on the\npegasus `data-avro-1_6` module:\n\ncom.linkedin.pegasus:data-avro-1_6:&lt;current-version&gt;  \n\n\nIf you are using avro 1.4, it’s adaptor module is included by default so\nyou don’t need to depend on it explicitly.\n\nSchema translation is implemented by the\n`com.linkedin.data.avro.SchemaTranslator` class.\n\nFor example, to convert from a avro schema, do:\n\nDataSchema pegasusDataSchema = SchemaTranslator.avroToDataSchema(avroSchema, options);  \n\n\nAnd to convert to an avro schema, do:\n\nSchema avroSchema = SchemaTranslator.dataToAvroSchema(pegasusDataSchema, options);  \n\n\nTranslating Data to and from Avro\n\nData translation is implemented by the\n`com.linkedin.data.avro.DataTranslator` class. Translating data\nrequires that one has schemas for both formats (.avsc and .pdsc). Please\nsee above section section about translating schemas for details. Once\nboth schemas are available, data can be converted.\n\nFor example, to convert avro data, do:\n\nDataTranslator.dataMapToGenericRecord(data, pegasusDataSchema,\navroSchema); // for dataMaps  \n// OR  \nGenericRecord avroRecord = DataTranslator.dataMapToGenericRecord(recordTemplate.data(), recordTemplate(), avroSchema); // for record templates  \n\n\nAnd to convert from avro data, do:\n\nDataMap pegasusData = genericRecordToDataMap(avroRecord, pegasusDataSchema, avroSchema);  \n\n",
      tags: null,
      id: 25
    });
    
    

  
    index.add({
      title: "Batch Finder Resource Method",
      category: null,
      content: "Batch Finder Resource Method\n\nContents\n\n\n  Overview\n  Protocol\n    \n      Request\n      Response\n    \n  \n  Client\n    \n      Java Request Builders\n    \n  \n  Resource API\n    \n      Criteria Filter\n      Method Annotation and Parameters\n      BatchFinderResult\n      Error Handling\n    \n  \n  FAQ\n\n\nOverview\nThe BATCH_FINDER resource method accepts a list of filters set. \nInstead of callings multiple finders with different filter values, we call 1 BATCH_FINDER method with a list of filters.\n\nFor example, a client might want to call the same FINDER with different search criteria.\nCombining multiple individual requests into a single batch request can save the application significant network latency.\nAlso, the server can execute searches more efficiently if they are combined as a single query.\n\nBATCH FINDER should not have any visible side effects.\nFor example, it should be safe to call whenever the client wishes.\nHowever, this is not something enforced by the framework, and it is up to the application developer that there are no side effects.\n\nIt is important to note that:\n\n  the operations may execute on the server out of order\n  the response objects are expected to be returned in the same order and position as the respective input search criteria.\n  The BATCH_FINDER will require implementing a resource method to handle a BATCH_FINDER requests. It won’t behave like a multiplexer that will call automatically existing finders\n\n\nProtocol\n\nRequest\n\nSee more details here BatchFinderUri.\n\nResponse\n\nSee more details here BatchCollectionResponse.\n\nClient\n\nJava Request Builders\n\nThe client framework includes a code-generation tool that reads the IDL (see Restspec IDL for details) and generates type-safe Java binding for each resource and its supported methods.\nThe bindings are represented as RequestBuilder classes.\nFor each resource described in an IDL file, a corresponding builder factory will be generated.\nThe factory contains a factory method for each resource method supported by the resource. \nThe factory method returns a request builder object with type-safe bindings for the given method.\nMore details in Resource Builder Factory.\n\nFor example:\n\n// Request builders factory class\n// which provides all the specific request builders to corresponding resource method in defined resource class\npublic class GreetingsRequestBuilders extends BuilderBase {\n    public GreetingsRequestBuilders()\n    public GreetingsRequestBuilders(String primaryResourceName)\n    public GreetingsCreateRequestBuilder create()\n    public GreetingsGetRequestBuilder get()\n    ...\n    public GreetingsFindBySearchRequestBuilder findBySearch()\n    // the BATCH_FINDER resource method named \"SearchGreetings\"\n    public GreetingsBatchFindBySearchGreetingsRequestBuilder batchFindBySearchGreetings() \n}\n\n\nThe request builders factory class provides a method to generate the corresponding BATCH_FINDER request builder.\nMore details about generated class and method declaration in\nBATCH FINDER Request Builder.\n\n@Generated(value = \"com.linkedin.pegasus.generator.JavaCodeUtil\", comments = \"Rest.li Request Builder\")\npublic class BatchfindersBatchFindBySearchGreetingsBuilder\n    extends BatchFindRequestBuilderBase&lt;Long, Greeting, BatchfindersBatchFindBySearchGreetingsBuilder&gt;\n{\n\n    public BatchfindersBatchFindBySearchGreetingsBuilder(java.lang.String baseUriTemplate, ResourceSpec resourceSpec, RestliRequestOptions requestOptions) {\n        super(baseUriTemplate, Greeting.class, resourceSpec, requestOptions);\n        super.name(\"searchGreetings\");\n    }\n    \n    // the 1st way to set the batch query parameter\n    public BatchfindersBatchFindBySearchGreetingsBuilder criteriaParam(Iterable&lt;GreetingCriteria&gt; value) {\n        super.setReqParam(\"criteria\", value, GreetingCriteria.class);\n        return this;\n    }\n\n    // the 2nd way to set the batch query parameter\n    public BatchfindersBatchFindBySearchGreetingsBuilder addCriteriaParam(GreetingCriteria value) {\n        super.addReqParam(\"criteria\", value, GreetingCriteria.class);\n        return this;\n    }\n\n    // set common query parameter\n    public BatchfindersBatchFindBySearchGreetingsBuilder messageParam(java.lang.String value) {\n        super.setReqParam(\"message\", value, java.lang.String.class);\n        return this;\n    }\n}\n\n\nHere is an example to show how to use request builder to build BATCH_FINDER request.\n\n  @Test\n  public void testUsingResourceBuilder() throws RemoteInvocationException {\n    // define batch search criteria\n    GreetingCriteria c1 = new GreetingCriteria().setId(1L).setTone(Tone.SINCERE);\n    GreetingCriteria c2 = new GreetingCriteria().setId(2L).setTone(Tone.FRIENDLY);\n    GreetingCriteria c3 = new GreetingCriteria().setId(100);\n    \n    //set batch query parameter and common query parameter\n    Request&lt;BatchCollectionResponse&lt;Greeting&gt;&gt; req = new GreetingsRequestBuilders().batchFindBySearchGreetings()\n        .criteriaParam(Arrays.asList(c1, c2, c3)).messageParam(\"hello world\").build();\n    Response&lt;BatchCollectionResponse&lt;Greeting&gt;&gt; resp = REST_CLIENT.sendRequest(req).getResponse();\n    BatchCollectionResponse&lt;Greeting&gt; response = resp.getEntity();\n  \n    List&lt;BatchFinderCriteriaResult&lt;Greeting&gt;&gt; batchResult = response.getResults();\n  \n    Assert.assertEquals(batchResult.size(), 3);\n  \n    // on success\n    List&lt;Greeting&gt; greetings1 = batchResult.get(0).getElements();\n    Assert.assertTrue(greetings1.get(0).hasTone());\n    Assert.assertTrue(greetings1.get(0).getTone().equals(Tone.SINCERE));\n  \n    // on error\n    ErrorResponse error = batchResult.get(2).getError();\n    Assert.assertTrue(batchResult.get(2).isError());\n    Assert.assertEquals(error.getMessage(), \"Fail to find Greeting!\");\n  }\n\n\nResource API\nBATCH_FINDER is supported on Collection and Association Resources only(See more details about\nCollection Resource and\nAssociation Resource).\nResources may provide zero or more BATCH_FINDER resource methods. Each BATCH_FINDER method must\nbe annotated with the @BatchFinder annotation.\n\nPagination default to start=0 and count=10. Clients may set both of these parameters to any desired value.\n\nCriteria Filter\nTo implement a batch finder, the resource owner has to define a RecordTemplate to define a criteria filter parameter.\nThe batch finder method will have to accept a array of this criteria filter.\n\nExample:\n\nThe resource owner need to define their own search criteria GreetingCriteria.pdl file.\n\nnamespace com.linkedin.restli.examples.greetings.api\n\n/**\n * A search criteria to filter greetings.\n */\nrecord GreetingCriteria {\n\n  /**\n   * Greeting ID to filter on\n   */\n  id: long\n\n  /**\n   * Greeting tone to filter on\n   */\n  tone: Tone\n}\n\n\nThe GreetingCriteria class represents a set of criteria (id and tone) by which to filter.\nThis Java class is auto-generated from the schema.\n\npublic class GreetingCriteria extends RecordTemplate\n{\n\n    private final static GreetingCriteria.Fields _fields = new GreetingCriteria.Fields();\n    private final static RecordDataSchema SCHEMA = ((RecordDataSchema) DataTemplateUtil.parseSchema(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"GreetingCriteria\\\",\\\"namespace\\\":\\\"com.linkedin.restli.examples.greetings.api\\\",\\\"doc\\\":\\\"A search criteria to filter greetings.\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"doc\\\":\\\"Greeting ID to filter on\\\"},{\\\"name\\\":\\\"tone\\\",\\\"type\\\":{\\\"type\\\":\\\"enum\\\",\\\"name\\\":\\\"Tone\\\",\\\"symbols\\\":[\\\"FRIENDLY\\\",\\\"SINCERE\\\",\\\"INSULTING\\\"]},\\\"doc\\\":\\\"Greeting tone to filter on\\\"}]}\"));\n    private final static RecordDataSchema.Field FIELD_Id = SCHEMA.getField(\"id\");\n    private final static RecordDataSchema.Field FIELD_Tone = SCHEMA.getField(\"tone\");\n    ...\n}\n\n\nMethod Annotation and Parameters\n\nThe @BatchFinder annotation takes 2 required parameter:\n\n  value : which indicates the BATCH_FINDER method name\n  batchParam : which indicates the name of the batch criteria parameter, each BATCH_FINDER method must have and can only have one batch parameter\n\n\nFor example:\n\n// eg. The curl call for this resource method is like:\n// curl -v -X GET http://localhost:8080/greetings?bq=searchGreetings&amp;criteria=List((id:1,tone:SINCERE),(id:2,tone:FRIENDLY))&amp;message=hello -H 'X-RestLi-Protocol-Version: 2.0.0' \n\n@BatchFinder(value = \"searchGreetings\", batchParam = \"criteria\")\npublic Task&lt;BatchFinderResult&lt;GreetingCriteria, Greeting, EmptyRecord&gt;&gt; searchGreetings(@PagingContextParam PagingContext context,\n                                                             @QueryParam(\"criteria\") GreetingCriteria[] criteria,\n                                                             @QueryParam(\"message\") String message)\n{\n return Task.blocking(\"searchGreetings\", () -&gt; {\n     BatchFinderResult&lt;GreetingCriteria, Greeting, EmptyRecord&gt; batchFinderResult = new BatchFinderResult&lt;&gt;();\n \n     for (GreetingCriteria currentCriteria: criteria) {\n       if (currentCriteria.getId() == 1L) {\n         // on success\n         CollectionResult&lt;Greeting, EmptyRecord&gt; c1 = new CollectionResult&lt;Greeting, EmptyRecord&gt;(Arrays.asList(g1), 1);\n         batchFinderResult.putResult(currentCriteria, c1);\n       } else if (currentCriteria.getId() == 2L) {\n         CollectionResult&lt;Greeting, EmptyRecord&gt; c2 = new CollectionResult&lt;Greeting, EmptyRecord&gt;(Arrays.asList(g2), 1);\n         batchFinderResult.putResult(currentCriteria, c2);\n       } else if (currentCriteria.getId() == 100L){\n         // on error: to construct error response for test\n         batchFinderResult.putError(currentCriteria, new RestLiServiceException(HttpStatus.S_404_NOT_FOUND, \"Fail to find Greeting!\"));\n       }\n     }\n \n     return batchFinderResult;\n }, _executor);\n\n}\n\n\nEvery parameter of a BATCH_FINDER method must be annotated with one of:\n\n\n  @Context - indicates that the parameter provides framework context\n   to the method. Currently all @Context parameters must be of type\n   PagingContext.\n  @QueryParam - indicates that the value of the parameter is\n   obtained from a request query parameter. The value of the annotation\n   indicates the name of the query parameter. Duplicate names are not\n   allowed for the same BATCH_FINDER method.\n   For the batch parameter, the name must match the name in the method annotation.\n  @AssocKey - indicates that the value of the parameter is a partial\n   association key, obtained from the request. The value of the\n   annotation indicates the name of the association key, which must\n   match the name of an @Key provided in the assocKeys field of the\n   @RestLiAssociation annotation.\n\n\nParameters marked with @QueryParam and @AssocKey\nmay also be annotated with @Optional, which indicates that the\nparameter is not required. caution: the batch parameter can not be optional.\nThe @Optional annotation may specify a String value, indicating the default value to be used if the parameter\nis not provided in the request. If the method parameter is of primitive\ntype, a default value must be specified in the @Optional annotation.\n\nValid types for regular query parameters are:\n\n\n  String\n  boolean / Boolean\n  int / Integer\n  long / Long\n  float / Float\n  double / Double\n  A Pegasus Enum (any enum defined in a .pdl schema)\n  Custom types (see the bottom of this section)\n  Record template types (any subclass of RecordTemplate generated\n   from a .pdl schema)\n  Arrays of one of the types above, e.g. String[], long[], …\n\n\nValid type for batch criteria parameter:\n\n\n  Can only be Arrays of Record template type, if have to use some other data types like Pegasus Enum, etc as the array item,\nneed to wrap it into a Record Template (.pdl schema)\n\n\nPlease note that “q” cannot be used as QueryParam name for batch finder. It is reserved for passing Finder’s method name.\n\nBatchFinderResult\n\nBATCH_Finder methods must return BatchFinderResult&lt;QK extends RecordTemplate, V extends RecordTemplate, MD extends RecordTemplate&gt;:\n\n\n  QK : The type of the BATCH_FINDER criteria filter\n  V :  The type of the resource, aka, the entity type\n  MD : The type of the meta data, if do not need metadata, just set it EmptyRecord\n\n\nFor each search criteria in the BatchFinderRequest, it can get either a successful reponse\nwhich is a CollectionResult(a list of entities), Or an error/failure which maybe represented by\na RestLiServiceException, which will be wrapped into an ErrorResponse later when building BatchFinderResponse\nto return to client.\n\npublic class BatchFinderResult&lt;QK,V extends RecordTemplate,MD extends RecordTemplate&gt;\n{\n   private final Map&lt;QK,CollectionResult&lt;V,R&gt;&gt; _elements;\n   private final Map&lt;QK,RestLiServiceException&gt; _errors;\n   ...\n}\n\n\nError Handling\n\n1) Custom error per search criteria\n\nFor each input criteria, the developer is responsible to update either the “_elements” or the “_errors” map in BatchFinderResult.\nIf the developers set a customized error which is wrapped into a RestLiServiceException for one search criteria,\nRest.li framework will not treat it as a failure for the whole BATCH_FINDER request, but just the failure for that specific criteria.\nThe return http status for the BATCH_FINDER request is still 200. An example is below\nResource API.\n\n2) Rest.li framework will cover the non-present criteria error\n\nWhen processing the BatchFinderResult in the ResponseBuilder, if a criteria is not present, either in _elements, nor in _errors, \nthe framework will generate a “404” error for this criteria.\nThe whole http status is still 200.\n\nnew RestLiServiceException(S_404_NOT_FOUND, \"The server didn't find a representation for this criteria\"));\n\n\n3) Return nulls\n\nIn some situation, the return results may contain null value. Resource methods should never explicitly return null. \nIf the Rest.li framework detects this, it will return an HTTP 500 back to the client with a message indicating\n‘Unexpected null encountered’. See more details in Returning Nulls.\n\nHere are some possible cases:\n\n  BatchFinderResult is null.\n  Element is null in the returned list of entities in the successful case.\n  For one criteria, the whole list of entities is null.\n\n\nFAQ\n\nDoes Batch_Finder support primitive type like String, Integer as a batch criteria filter?\n\nNo. \nWe currently don’t support primitive data type as batch criteria, even Enum type.\nThat criteria must be a type which extends from RecordTemplate which is actually a record.\nThe reason is that using a record will be easier to support any evolution .i.e. adding additional criteria to the same batch finder method.\n",
      tags: null,
      id: 26
    });
    
    

  
    index.add({
      title: "Building Rest.li from Source",
      category: null,
      content: "Building Rest.li from Source\n\nContents\n\nYou can use the latest released version of Rest.li or build your own\nlocal version:\n\n\n  Using Rest.li JARs\n  Building Your Own Copy of Rest.li\n\n\nUsing Rest.li JARs\n\nIf you are not modifying the Rest.li source code, you don’t need to\nbuild Rest.li. You can simply depend on the\nartifacts in the maven central repository\n\nFor details on how to use Rest.li from its maven central artifacts, see:\nQuickstart\n\nBuilding Your Own Copy of Rest.li\n\nYou can also checkout, modify, or build your own copy of Rest.li.\n\nChecking Out Source\n\nYou can get your own copy of the Rest.li repository with:\n\ngit clone https://github.com/linkedin/rest.li.git\n\n\nOr if you already have a copy of the repository, you can update it with:\n\ngit pull\n\n\nBuilding Rest.li\n\nTo do a clean build type, do this:\n\ngradle clean build\n\n\nTo install the gradle JARs in your own local repository, do this:\n\ngradle install\n\n\nOut of Memory While Trying to Build?\n\nIf the build fails with an error message saying that there isn’t enough\nmemory, increase the memory using the following command and try again:\n\nexport GRADLE_OPTS=“-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=1024M”  \n\n",
      tags: null,
      id: 27
    });
    
    

  
    index.add({
      title: "Rest.li Snapshots and Resource Compatibility Checking",
      category: null,
      content: "Rest.li Snapshots and Resource Compatibility Checking\n\nContents\n\n\n  Introduction\n  Running the Compatibility Checker\n  Compatibility Levels\n  Why is Adding to an Enum Considered Backwards Incompatible?\n  Error Messages\n  Location of Snapshots/IDLs\n  The Compatibility Checker and IDLs\n  Continuous Integration Environments\n\n\nIntroduction\n\nDue to the fact that resources and clients can be upgraded separately, it is very important to developers\nthat they be notified of any changes they make that could cause backwards or forwards compatibility issues.\nTo that end, Rest.li uses a form of expanded IDLs,\ncalled Snapshots, to keep track of the state of resources and check compatibility between resource iterations.\n\nRunning the Compatibility Checker\n\nThe Snapshot Compatibility checker will be automatically run during a basic gradle build. However, if you\nwish to run the compatibility checker as a stand-alone on a particular target, you can do so by running\ngradle :[target]:checkRestModel. Additionally, there are four compatibility levels that the checker\ncan be run on, by adding the argument -Prest.model.compatibility=[compatibility-level] to the gradle command.\n\nThe compatibility checker generates .snapshot.json files.\n\nCompatibility Levels\n\nThere are four levels of compatibility. From least permissive to most, they are:\n\nequivalent - If the check is run in equivalent mode, no changes to Resources or schemas will pass.\n\nbackwards - Changes that are considered backwards compatible will pass, otherwise, changes will fail.\n\nignore - The compatibility checker is run, but all changes will pass. All changes, backwards compatible\nand backwards incompatible, will be printed.\n\noff - The compatibility checker will not be run at all.\n\nBy default, the compatibility checker will be run on backwards compatibility mode. There are two ways to\nchange the mode the compatibility checker is run on. For one, you can add a flag to the gradle build itself:\n--Prest.model.compatibility=&lt;compatLevel&gt;\n\nAlternately, if you want to change the default compatibility level for all builds on a machine, you can\ncreate or edit the file ~/.gradle/gradle.properties to contain the line rest.model.compatibility=&lt;compatLevel&gt;\n\nWhy is Adding to an Enum Considered Backwards Incompatible?\n\nMany developers are surprised that adding to an enum is considered a backwards incompatible change.\n\nBut, while Rest.li is designed with features to make it easier to add symbols to enums, it cannot possibly\nguarantee that adding a enum symbols is backward compatible.\n\nTo make it easier to add values in the enum data schema, java enum classes generated by Rest.li that correspond to enum\ndata schema always contains a special “$UNKNOWN” symbol. Whenever Rest.li deserializes enum data that contains a symbol\nthat is not present in the java enum, Rest.li maps it to “$UNKNOWN”. When the enum is accessed via accessor implemented\nby a data template, the accessor will return the new symbol as the java “$UNKNOWN” symbol. This gives readers of the\nenum the opportunity to check if the enum is “$UNKNOWN”, and if it is, handle is in the best possible way.\n\nHowever, it’s still not possible to guarantee backward compatibility, even with the “$UNKNOWN” symbol available. It’s\npossible that clients did not handle the “$UNKNOWN” symbol in the best possible way,  and even if they did it may be\nthat they cannot do anything other than fail if they encounter a enum symbol they do not recognize. In many practical\napplications,  it is not feasible to assess how all clients have been coded to handle new enum symbols, particularly\nwhen there are many clients.  In such cases, adding a new enum symbol might break a unknown number of clients.\n\nIt’s true that there may be well controlled use case were an enum is used only by a single client and server that are\nmaintained by the same developers.   In these very specific cases, it may be easy for the developers to know that new\nenum symbols can be safely added at any time.  But if additional clients might be added in the future, it is still risky\nto get in the habit of adding enum symbols “as-if” they are backward compatible changes. Once additional clients start\nusing the API,  adding a symbol could break them.\n\nGiven all these potential issues with adding a enum symbol, it’s important to think of adding enum symbols as backward\nincompatible.  If a new symbols is to be added, a migration strategy for adding the enum symbol(s) must be performed\njust as for any other backward incompatible change.  Note that this is only possible when all clients are known and it\nis possible to coordinate changes with them.  If this is not the case,  one should consider making a backward compatible\nchange (such as adding a new optional field containing a new enum field with more symbols) and supporting the existing\nclients, with the existing enum symbols, indefinitely.\n\nOne possible backwards incompatible migration strategy for adding a enum symbol might be:\n\n\n  Add the symbol as a backward incompatible change to the data schema,  use the rest.li\ngradle rest.model.compatibility flag to “ignore”\nthe backward incompatible change.  If you are using semantic versioning,\nyou should also increment your MAJOR version number.  Do NOT start writing data that contains\nthe new enum value yet.\n  Once the backward incompatible change has been published, REST API clients must be notified to the change.\nThey should be provided with details on the enum symbol that has been added and how to migrate their applications.\nClients should NOT write using the new symbol yet (the resource implementation may choose to reject requests to POST\nor PUT data with the new symbol).\n  Only once all clients have migrated to the new (backward incompatible version) of the API, clients and the resource\nimplementation may start writing data using the new symbol.\n\n\nSome things to watch out for when adding enum symbols:\n\n\n  Rest.li provides schema and data translation to avro.  While unknown symbols can be deserialized by older Rest.li\nconsumers (because rest.li does not require the schema to de-serialize), it doesn’t work for data persisted as Avro.\nAny attempt to deserialize an avro record containing the new enumeration value with an older schema lacking that enum\nwill fail.\n\n\nIt is safe to ignore the incompatibility message if you are sure this will not happen, or you can work with your clients\nto make sure that it doesn’t. In general, we only provide notifications about backwards incompatible changes as a tool\nfor the developer.  You are always free to ignore backwards-incompatible change messages if you know that the change\nwill not cause problems, or are willing to take steps to ensure that it will not.\n\nError Messages\n\nThere are a number of error messages that can appear during compatibility checking. They will look something like this:\n\nidl compatibility report between published \"/publishedlocation/com.namespace.resource.snapshot.json\" and current \"/currentlocation/com.namespace.resource.snapshot.json\":\n  Incompatible changes:\n    1) /location : detailed error message\n\n\nThis will tell you what resource is causing the problem, whether the change is backwards incompatible or not, and where\nexactly in the resource the problem is. You can then either use this information to fix the problem, or ignore it and\nre-run the build with a compatibility level that will cause it to pass.\n\nLocation of Snapshots/IDLs\n\nThe canonical snapshots for a set of resources will be located in the api project associated with those resources. If\nthis project is located in some directory project-api, then the snapshots will be located in\nproject-api/src/main/snapshot. Similarly, canonical IDLs will be located in project-api/src/main/idl. Though the\nfiles in these directories are generated, we strongly suggest that they be checked in. The IDLs must be checked in to\ncorrectly generate builders, and checking in the snapshots ensures that developers will receive accurate compatibility\nmessages when making changes.\n\nThe temporary snapshots for a set of resources will be located within the same project as the java Resource files\nthemselves. If the project for the Resource file is in a directory project-impl, then the snapshots will be located in\nproject-impl/src/mainGeneratedRest/snapshot. Again, similarly, the temporary IDLs will be located in\nproject-impl/src/mainGeneratedRest/idl. Unlike the canonical files, these generated files should NOT be checked\nin.\n\nThe Compatibility Checker and IDLs\n\nThe compatibility checker will also do limited IDL checks. By default, IDLs will only be checked to make sure there are\nno orphan IDLs from newly removed Resources and no missing IDLs from newly added Resources. A compatibility message may\nbe printed saying that a resource has been added or removed. If a resource has been removed, you will need to remove the\ncanonical IDL yourself. (This is also true for Snapshots).\n\nContinuous Integration Environments\n\nIf you are running a continuous integration environment on a Rest.li project, you will want to run your compatibility\nchecker on equivalent. This will prevent your canonical IDLs and Snapshots from getting out of sync with the Resources\nthey represent. You can do this either by running each build with the flag -Prest.model.compatibility=equivalent, or\nby creating or editing the ~/.gradle/gradle.properties file to contain the line rest.model.compatibility=equivalent\n",
      tags: null,
      id: 28
    });
    
    

  
    index.add({
      title: "How to contribute to Rest.li",
      category: null,
      content: "How to contribute to Rest.li\n\nWe are always very happy to have contributions, whether for trivial cleanups or big new features,and we hope this document will help to make the process as smooth as possible.\n\nNor is code the only way to contribute to the project. We strongly value documentation and gladly accept improvements to the documentation.\n\nTalk to us early on!\n\nTo avoid duplication of effort, let us know what you’re planning to contribute before you write the code. It will also help us to plan some time to review your changes.\n\nCreate an issue for the change\n\nWe recommend creating an issue on Github detailing the change you would like to make. It can be a bug fix, a documentation update or a new feature request.\n\nCreate a design doc\n\nIf you are adding new major feature, we suggest that you add a design document and solicit comments before submitting any code. You can link your design doc to your Github issue.\n\nIf you’re only fixing a bug, or updating the documentation, you can submit a pull request right away.\n\nUpdate the documentation\n\nPlease ensure your code is adequately documented. Add a summary of your contribution to the top of the CHANGELOG file.\n\nDepends on the nature of your change, we also recommend to update our user guide. Please follow the instructions: https://github.com/linkedin/rest.li/tree/gh-pages\n\nSending a Pull Request\n\nFollow coding conventions, include documentation and write tests. Look to the surrounding code for example.\n\nContributions should be submitted as GitHub pull requests.\n\nThe Rest.Li team at LinkedIn is monitoring for pull requests. We will review, and update your changes with an explanation. Rest.li is under active development and massively used at LinkedIn. Your changes might require additional efforts on our side to fix our internal uses, which could cause some delay. We’ll do our best to provide updates and feedback throughout the process.\n",
      tags: null,
      id: 29
    });
    
    

  
    index.add({
      title: "Rest.li D2 Dynamic discovery tutorial",
      category: null,
      content: "Dynamic Discovery (D2) Quick Start\n\nContents\n\nIn this tutorial, we will explain the basic concepts of D2 using a\nsimple client server project. Apache\nZookeeper is required for doing this\ntutorial. The completed code for this tutorial is available in Rest.li’s\nexamples/d2-quickstart:\n\n\n  What is D2 in a Nutshell\n  The Tutorial\n    \n      Step 1. Create a Server\n      Step 2. Create a Config Runner\n      Step 3. Create a Client\n    \n  \n  Next Steps\n\n\nWhat is D2 in a Nutshell\n\nImagine we have a Service Oriented Architecture. Let’s say we have\nhundreds of servers. Each server can host different set of services.\nSome of those services may be partitioned, so a server may belong to some\nspecific partitions for those services. We use D2 to store information\nabout which server can serve what service. So this means with D2, a\nclient requesting a particular service doesn’t need to know where the\nphysical servers are. The client can ask D2 to route a request to the\nright server. D2 is similar to DNS in some ways. At the core, D2 is a layer of indirection between a client and a\nserver. However, D2 supports many other goodies like client side load\nbalancing, partitioning, and multi-data-center routing.\n\nOur smallest unit of indirection is called a service. A service can\nbe a URL endpoint, a Rest.li resource, or anything else as long as the\nname of the service is unique. A collection of services is called a\ncluster. A service that belongs to one cluster cannot belong to a\ndifferent cluster. A cluster has one-to-many relationship to a\nservice. All this information about clusters and services is stored in\nZookeeper.\n\nA server joins a cluster by creating an ephemeral node in Zookeeper.\nWhen a server dies, Zookeeper will notice because the heart beat message\nis not refreshed. Then, the ephemeral node is automatically removed.\n\nA client attempting to send a request to a service first consults\nZookeeper to find out which cluster owns the service. Then, the client\nqueries Zookeeper for all the ephemeral nodes (servers) for that\ncluster. Given a list of ephemeral nodes, the client will deliberately\nchoose a server to send the request to.\n\nThat is all you need to know about D2 in a nutshell.\n\nThe Tutorial\n\nWe will create a basic client server application in Java. We use gradle\nfor our build process. The top level structure of our project will have\n3 subdirectories:\n\n/server\n/client\n/config\n\n\nYou also need a settings.gradle and build.gradle file in the root\ndirectory.\n\nFor settings.gradle:\n\ninclude 'server'\ninclude 'client'\ninclude 'config'\n\n\nThis will tell gradle that gradle should search for server, client,\nconfig directories and mark them as part of the project.\n\nFor build.gradle:\n\nallprojects {\n    apply plugin: 'idea'\n    apply plugin: 'eclipse'\n}\n\nfinal pegasusVersion = '1.20.0'\next.spec = [\n    'product' : [\n        'pegasus' : [\n                'r2' : 'com.linkedin.pegasus:r2:' + pegasusVersion,\n                'd2' : 'com.linkedin.pegasus:d2:' + pegasusVersion\n        ]\n    ]\n]\n\nsubprojects {\n    repositories {\n        mavenLocal()\n        mavenCentral()\n    }\n}\n\n\nThis tells gradle that it should use pegasus artifact version 1.20.0\nfrom the maven central repository. This also tells gradle we have\ndependencies to r2 and d2 libraries.\n\nStep 1. Create a Server\n\nCreate the following project structure in the server sub-directory:\n\n\n  d2-quickstart/\n    \n      client/\n      config/\n      server/\n        \n          build.gradle\n          src/\n            \n              main/\n                \n                  java/\n                    \n                      com/\n                        \n                          example/\n                            \n                              d2/\n                                \n                                  server/\n                                    \n                                      EchoServer.java\n                                      ExampleD2Server.java\n                                    \n                                  \n                                \n                              \n                            \n                          \n                        \n                      \n                    \n                  \n                  config/\n                    \n                      server.json\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n  \n\n\nIn this example, we are creating an echo server to illustrate a real\nproduction server. The echo server always returns HTTP status code 200\nand prints to stdout when a request comes in.\n\nFirst, we add our compile dependencies to the server’s build.gradle:\n\napply plugin: 'java'\n\ndependencies {\n    compile 'com.googlecode.json-simple:json-simple:1.1.1'\n    compile spec.product.pegasus.r2\n    compile spec.product.pegasus.d2\n}\n\n\nHere is the implementation of the echo server:\n\npackage com.example.d2.server;\n\nimport com.sun.net.httpserver.HttpExchange;\nimport com.sun.net.httpserver.HttpHandler;\nimport com.sun.net.httpserver.HttpServer;\n\nimport java.io.IOException;\nimport java.io.OutputStream;\nimport java.net.InetSocketAddress;\nimport java.util.Date;\nimport java.util.List;\n\npublic class EchoServer\n{\n  private final int        _port;\n  private final HttpServer _server;\n\n  public EchoServer (int port, final String name, List&lt;String&gt; contextPaths)\n      throws IOException\n  {\n    _port = port;\n    _server = HttpServer.create(new InetSocketAddress(_port), 0);\n    for (String contextPath : contextPaths)\n    {\n      _server.createContext(contextPath, new MyHandler(contextPath, name));\n    }\n    _server.setExecutor(null);\n  }\n\n  static class MyHandler implements HttpHandler\n  {\n    private final String _name;\n    private final String _serverName;\n\n    private MyHandler(String name, String serverName)\n    {\n      _name = name;\n      _serverName = serverName;\n    }\n\n    public void handle(HttpExchange t) throws IOException\n    {\n      System.out.println(new Date().toString() + \": \" + _serverName\n                             + \" received a request for the context handler = \" + _name );\n      String response = \"Successfully contacted server \" + _serverName;\n      t.sendResponseHeaders(200, response.length());\n      OutputStream os = t.getResponseBody();\n      os.write(response.getBytes());\n      os.close();\n    }\n  }\n\n  public void start()\n      throws IOException\n  {\n    _server.start();\n  }\n\n  public void stop()\n      throws IOException\n  {\n    _server.stop(0);\n  }\n\n}\n\n\nWe store the configuration for the servers in server.json. Here is the\ncontent of server.json:\n\n{\n    \"echoServers\" :\n        [\n            {\n                \"name\" : \"RecommendationService-1\",\n                \"port\" : 39901,\n                \"threadPoolSize\" : 1,\n                \"contextPaths\" : [\n                    \"/articleRecommendation\",\n                    \"/jobRecommendation\"\n                ]\n            },\n            {\n                \"name\" : \"RecommendationService-2\",\n                \"port\" : 39902,\n                \"threadPoolSize\" : 1,\n                \"contextPaths\" : [\n                    \"/articleRecommendation\",\n                    \"/jobRecommendation\"\n                ]\n            },\n            {\n                \"name\" : \"RecommendationService-3\",\n                \"port\" : 39903,\n                \"threadPoolSize\" : 1,\n                \"contextPaths\" : [\n                    \"/articleRecommendation\",\n                    \"/jobRecommendation\"\n                ]\n            },\n            {\n                \"name\" : \"NewsService-1\",\n                \"port\" : 39904,\n                \"threadPoolSize\" : 1,\n                \"contextPaths\" : [\n                    \"/newsArticle\"\n                ]\n            },\n            {\n                \"name\" : \"NewsService-2\",\n                \"port\" : 39905,\n                \"threadPoolSize\" : 1,\n                \"contextPaths\" : [\n                    \"/newsArticle\"\n                ]\n            },\n            {\n                \"name\" : \"NewsService-3\",\n                \"port\" : 39906,\n                \"threadPoolSize\" : 1,\n                \"contextPaths\" : [\n                    \"/newsArticle\"\n                ]\n            }\n        ],\n    \"d2Servers\" :\n        [\n            {\n                \"serverUri\" : \"http://localhost:39901\",\n                \"d2Cluster\" : \"RecommendationService\",\n                \"partitionData\" : {\n                    \"0\" : {\n                        \"weight\" : \"1.0\"\n                    }\n                }\n            },\n            {\n                \"serverUri\" : \"http://localhost:39902\",\n                \"d2Cluster\" : \"RecommendationService\",\n                \"partitionData\" : {\n                    \"0\" : {\n                        \"weight\" : \"1.0\"\n                    }\n                }\n            },\n            {\n                \"serverUri\" : \"http://localhost:39903\",\n                \"d2Cluster\" : \"RecommendationService\",\n                \"partitionData\" : {\n                    \"0\" : {\n                        \"weight\" : \"1.0\"\n                    }\n                }\n            },\n            {\n                \"serverUri\" : \"http://localhost:39904\",\n                \"d2Cluster\" : \"NewsService\",\n                \"partitionData\" : {\n                    \"0\" : {\n                        \"weight\" : \"1.0\"\n                    }\n                }\n            },\n            {\n                \"serverUri\" : \"http://localhost:39905\",\n                \"d2Cluster\" : \"NewsService\",\n                \"partitionData\" : {\n                    \"0\" : {\n                        \"weight\" : \"1.0\"\n                    }\n                }\n            },\n            {\n                \"serverUri\" : \"http://localhost:39906\",\n                \"d2Cluster\" : \"NewsService\",\n                \"partitionData\" : {\n                    \"0\" : {\n                        \"weight\" : \"1.0\"\n                    }\n                }\n            }\n        ],\n    \"zkConnectString\" : \"localhost:2181\",\n    \"zkSessionTimeout\" : 5000,\n    \"zkBasePath\" : \"/d2\",\n    \"zkRetryLimit\" : 10,\n    \"announcerStartTimeout\" : 5000,\n    \"announcerShutdownTimeout\" : 5000\n}\n\n\nIn the configuration, above we have 6 echo servers and 6 d2 announcers.\nThe first 3 echo servers belong to RecommendationService, and the\nremaining echo servers belong to NewsService.\n\nFinally, we add the task of running this server to the server’s\nbuild.gradle:\n\ntask runServer(type: JavaExec) {\n    main = 'com.example.d2.server.ExampleD2Server'\n    classpath = sourceSets.main.runtimeClasspath\n    standardInput = System.in\n}\n\n\nIn order to run the server, you run this command:\n\n../../gradlew runServer\n\n\nStep 2. Create a Config Runner\n\nCreate the following project structure in the ‘config’ sub-directory:\n\n\n  d2-quickstart/\n    \n      client/\n      config/\n        \n          build.gradle\n          src/\n            \n              main/\n                \n                  java/\n                    \n                      com/\n                        \n                          example/\n                            \n                              d2/\n                                \n                                  config/\n                                    \n                                      ConfigRunner.java\n                                    \n                                  \n                                \n                              \n                            \n                          \n                        \n                      \n                    \n                  \n                  d2Config/\n                    \n                      d2Config.json\n                    \n                  \n                \n              \n            \n          \n        \n      \n      server/\n    \n  \n\n\nWe specify the mapping of clusters and services in d2Config.json. In\nreal production scenario, we can do a lot more with d2Config. We can\nconfigure how the load balancer behaves. We can also set up partitioning\nand sticky routings.\n\nBut for simplicity we won’t include all these in this example. Here is\nhow our d2Config.json going to look like:\n\n{\n    \"d2Clusters\" : {\n        \"RecommendationService\": {\n            \"services\":\n                {\n                    \"articleRecommendation\": {\n                        \"path\" : \"/articleRecommendation\"\n                    },\n                    \"jobRecommendation\": {\n                        \"path\" : \"/jobRecommendation\"\n                    }\n                }\n\n        },\n        \"NewsService\": {\n            \"services\":\n                {\n                    \"newsArticle\" : {\n                        \"path\" : \"/newsArticle\"\n                    }\n                }\n\n        }\n    },\n    \"defaultServiceProperties\" : {\n        \"loadBalancerStrategyList\" : [\n            \"degraderV3\",\n            \"degraderV2\"\n        ],\n        \"prioritizedSchemes\" : [\n            \"http\"\n        ],\n        \"loadBalancerStrategyProperties\" : {\n            \"http.loadBalancer.updateIntervalMs\" : \"5000\",\n            \"http.loadBalancer.pointsPerWeight\" : \"100\"\n        },\n        \"transportClientProperties\" : {\n            \"http.requestTimeout\" : \"10000\"\n        },\n        \"degraderProperties\" : {\n            \"degrader.minCallCount\" : \"10\",\n            \"degrader.lowErrorRate\" : \"0.01\",\n            \"degrader.highErrorRate\" : \"0.1\"\n        }\n    },\n    \"zkConnectString\" : \"localhost:2181\",\n    \"zkSessionTimeout\" : 5000,\n    \"zkBasePath\" : \"/d2\",\n    \"zkRetryLimit\" : 10\n}\n\n\nFrom reading the configuration above you probably have questions about\nthese properties. So here are some explanation for some non-obvious\nones.\n\n\n  loadBalancerStrategyList was set to a list consisting of\ndegraderV3 and degraderV2. This means, we try to use\ndegraderV3 if possible. The difference between degraderV3\nand degraderV2 is degraderV3 supports partitioning while\ndegraderV2 does not.\n  For schemes we support https and http, but for simplicity\nwe’ll use only http. Using https require us to wire in SSL\nparameter and that’s beyond the scope of this example.\n\n\nThe rest of the config values should be pretty obvious. For the list of\nall configuration please see D2 Zookeeper Properties.\n\nThen we modify the config’s build.gradle to add our java dependencies.\n\napply plugin: 'java'\n\ndependencies {\n    compile 'com.googlecode.json-simple:json-simple:1.1.1'\n    compile spec.product.pegasus.d2\n}\n\n\nNext we will create a java class that reads this config and publish it\nto zookeeper. We have a utility class called D2Config that does this for\nyou. But we have to feed D2Config some parameters in order for it to\nwork.\n\nHere’s the java class for running the D2Config.\n\npackage com.example.d2.config;\nimport com.linkedin.d2.discovery.util.D2Config;\nimport org.json.simple.JSONObject;\nimport org.json.simple.parser.JSONParser;\n\nimport java.io.File;\nimport java.io.FileReader;\nimport java.util.Collections;\nimport java.util.Map;\n\npublic class ConfigRunner\n{\n  public static void main(String[] args)\n      throws Exception\n  {\n    //get server configuration\n    String path = new File(new File(\".\").getAbsolutePath()).getCanonicalPath() +\n        \"/src/main/d2Config/d2Config.json\";\n    JSONParser parser = new JSONParser();\n    Object object = parser.parse(new FileReader(path));\n    JSONObject json = (JSONObject) object;\n    System.out.println(\"Finished parsing d2 topology config\");\n\n    String zkConnectString = (String)json.get(\"zkConnectString\");\n    int zkSessionTimeout = ((Long)json.get(\"zkSessionTimeout\")).intValue();\n    String zkBasePath = (String)json.get(\"zkBasePath\");\n    int zkRetryLimit = ((Long)json.get(\"zkRetryLimit\")).intValue();\n\n    Map&lt;String,Object&gt; serviceDefaults = (Map&lt;String, Object&gt;)json.get(\n        \"defaultServiceProperties\");\n\n    //this contains the topology of our system\n    Map&lt;String,Object&gt; clusterServiceConfigurations =\n        (Map&lt;String, Object&gt;)json.get(\"d2Clusters\");\n\n    System.out.println(\"Populating zookeeper with d2 configuration\");\n\n    //d2Config is the utility class for populating zookeeper with our topology\n    //some the params are not needed for this simple example so we will just use\n    //default value by passing an empty map\n    D2Config d2Config = new D2Config(zkConnectString, zkSessionTimeout, zkBasePath,\n                                     zkSessionTimeout, zkRetryLimit,\n                                     (Map&lt;String, Object&gt;)Collections.EMPTY_MAP,\n                                     serviceDefaults,\n                                     clusterServiceConfigurations,\n                                     (Map&lt;String, Object&gt;)Collections.EMPTY_MAP,\n                                     (Map&lt;String, Object&gt;)Collections.EMPTY_MAP);\n\n    //populate zookeeper\n    d2Config.configure();\n    System.out.println(\"Finished populating zookeeper with d2 configuration\");\n  }\n}\n\n\nFinally, we add a task in config’s build.gradle to run the above Java\nclass.\n\ntask runConfigRunner(type: JavaExec) {\n    main = 'com.example.d2.config.ConfigRunner'\n    classpath = sourceSets.main.runtimeClasspath\n    standardInput = System.in\n}\n\n\nIn order to run D2Config run ../../gradlew runConfigRunner\n\nStep 3. Create a Client\n\nCreate the following project structure in the ‘client’ subdirectory:\n\n\n  d2-quickstart/\n    \n      client/\n        \n          build.gradle\n          src/\n            \n              main/\n                \n                  java/\n                    \n                      com/\n                        \n                          example/\n                            \n                              d2/\n                                \n                                  client/\n                                    \n                                      ExampleD2Client.java\n                                    \n                                  \n                                \n                              \n                            \n                          \n                        \n                      \n                    \n                  \n                  config/\n                    \n                      client.json\n                    \n                  \n                \n              \n            \n          \n        \n      \n      config/\n      server/\n    \n  \n\n\nFirst we create build.gradle to declare our java dependencies\n\napply plugin: 'java'\ndependencies {\n    compile 'com.googlecode.json-simple:json-simple:1.1.1'\n    compile spec.product.pegasus.d2\n}\n\n\nThen we need the client code to instantiate D2 client and to send\ntraffic through the D2 client. Here’s how our ExampleD2Client looks\nlike:\n\npackage com.example.d2.client;\n\nimport com.linkedin.common.callback.Callback;\nimport com.linkedin.common.util.None;\nimport com.linkedin.d2.balancer.D2Client;\nimport com.linkedin.d2.balancer.D2ClientBuilder;\nimport com.linkedin.r2.message.rest.RestRequest;\nimport com.linkedin.r2.message.rest.RestRequestBuilder;\nimport org.json.simple.JSONObject;\nimport org.json.simple.parser.JSONParser;\nimport org.json.simple.parser.ParseException;\n\nimport java.io.File;\nimport java.io.FileReader;\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.util.Map;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.ScheduledFuture;\nimport java.util.concurrent.TimeUnit;\n\npublic class ExampleD2Client\n{\n  public static void main(String[] args)\n      throws IOException, ParseException, InterruptedException\n  {\n    //get client configuration\n    JSONObject json = parseConfig();\n    String zkConnectString = (String) json.get(\"zkConnectString\");\n    Long zkSessionTimeout = (Long) json.get(\"zkSessionTimeout\");\n    String zkBasePath = (String) json.get(\"zkBasePath\");\n    Long zkStartupTimeout = (Long) json.get(\"zkStartupTimeout\");\n    Long zkLoadBalancerNotificationTimeout = (Long) json.get(\"zkLoadBalancerNotificationTimeout\");\n    String zkFlagFile = (String) json.get(\"zkFlagFile\");\n    String fsBasePath = (String) json.get(\"fsBasePath\");\n    final Map&lt;String, Long&gt; trafficProportion = (Map&lt;String, Long&gt;) json.get(\"trafficProportion\");\n    final Long clientShutdownTimeout = (Long) json.get(\"clientShutdownTimeout\");\n    final Long clientStartTimeout = (Long) json.get(\"clientStartTimeout\");\n    Long rate = (Long) json.get(\"rateMillisecond\");\n    System.out.println(\"Finished parsing client config\");\n\n    //create d2 client\n    final D2Client d2Client = new D2ClientBuilder().setZkHosts(zkConnectString)\n                                                      .setZkSessionTimeout(\n                                                          zkSessionTimeout,\n                                                          TimeUnit.MILLISECONDS)\n                                                      .setZkStartupTimeout(\n                                                          zkStartupTimeout,\n                                                          TimeUnit.MILLISECONDS)\n                                                      .setLbWaitTimeout(\n                                                          zkLoadBalancerNotificationTimeout,\n                                                          TimeUnit.MILLISECONDS)\n                                                      .setFlagFile(zkFlagFile)\n                                                      .setBasePath(zkBasePath)\n                                                      .setFsBasePath(fsBasePath)\n                                                      .build();\n\n    System.out.println(\"Finished creating d2 client, starting d2 client...\");\n\n    ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor();\n    final CountDownLatch latch = new CountDownLatch(1);\n\n    //start d2 client by connecting to zookeeper\n    startClient(d2Client, executorService, clientStartTimeout,\n                new Callback&lt;None&gt;()\n                {\n                  @Override\n                  public void onError (Throwable e)\n                  {\n                    System.exit(1);\n                  }\n\n                  @Override\n                  public void onSuccess (None result)\n                  {\n                    latch.countDown();\n                  }\n                });\n    latch.await();\n    System.out.println(\"D2 client is sending traffic\");\n\n    ScheduledFuture task = executorService.scheduleAtFixedRate(new Runnable()\n    {\n      @Override\n      public void run ()\n      {\n        try\n        {\n          sendTraffic(trafficProportion, d2Client);\n        }\n        catch (URISyntaxException e)\n        {\n          e.printStackTrace();\n        }\n      }\n    }, 0, rate, TimeUnit.MILLISECONDS);\n\n    System.out.println(\"Press enter to stop D2 client...\");\n    System.in.read();\n    task.cancel(false);\n    System.out.println(\"Shutting down...\");\n    shutdown(d2Client, executorService, clientShutdownTimeout);\n  }\n\n  private static void startClient(final D2Client d2Client,\n                                  ExecutorService executorService,\n                                  Long timeout,\n                                  final Callback&lt;None&gt; callback)\n  {\n    try\n    {\n      executorService.submit(new Runnable()\n      {\n        @Override\n        public void run ()\n        {\n          d2Client.start(new Callback&lt;None&gt;()\n          {\n            @Override\n            public void onError (Throwable e)\n            {\n              System.err.println(\"Error starting d2Client. Aborting... \");\n              e.printStackTrace();\n              System.exit(1);\n            }\n\n            @Override\n            public void onSuccess (None result)\n            {\n              System.out.println(\"D2 client started\");\n              callback.onSuccess(None.none());\n            }\n          });\n        }\n      }).get(timeout, TimeUnit.MILLISECONDS);\n    }\n    catch (Exception e)\n    {\n      System.err.println(\"Cannot start d2 client. Timeout is set to \" +\n                             timeout + \" ms\");\n      e.printStackTrace();\n    }\n  }\n\n  private static void shutdown(final D2Client d2Client,\n                               ExecutorService executorService,\n                               Long timeout)\n  {\n    try\n    {\n      executorService.submit(new Runnable()\n      {\n        @Override\n        public void run ()\n        {\n          d2Client.shutdown(new Callback&lt;None&gt;()\n          {\n            @Override\n            public void onError (Throwable e)\n            {\n              System.err.println(\"Error shutting down d2Client.\");\n              e.printStackTrace();\n            }\n\n            @Override\n            public void onSuccess (None result)\n            {\n              System.out.println(\"D2 client stopped\");\n            }\n          });\n        }\n      }).get(timeout, TimeUnit.MILLISECONDS);\n    }\n    catch (Exception e)\n    {\n      System.err.println(\"Cannot stop d2 client. Timeout is set to \" +\n                             timeout + \" ms\");\n      e.printStackTrace();\n    }\n    finally\n    {\n      executorService.shutdown();\n    }\n  }\n\n  private static JSONObject parseConfig()\n      throws IOException, ParseException\n  {\n    String path = new File(new File(\".\").getAbsolutePath()).getCanonicalPath() +\n        \"/src/main/config/client.json\";\n    JSONParser parser = new JSONParser();\n    Object object = parser.parse(new FileReader(path));\n    return (JSONObject) object;\n  }\n\n  private static void sendTraffic(Map&lt;String, Long&gt; trafficProportion, D2Client d2Client)\n      throws URISyntaxException\n  {\n    for (Map.Entry&lt;String, Long&gt; entry : trafficProportion.entrySet())\n    {\n      URI uri = new URI(\"d2://\" + entry.getKey());\n      RestRequest request = new RestRequestBuilder(uri).setMethod(\"get\").build();\n      for (long i = 0; i &lt; entry.getValue(); i++)\n      {\n        //we don't care about the result from the server after all,\n        //you can see the traffic hits the echo server from stdout\n        d2Client.restRequest(request);\n      }\n    }\n  }\n}\n\n\nIn the above code, sendTraffic() will send request based on the\ntrafficProportion configured in client.json. Let’s configure the config\nso that the client sends:\n\n\n  3 requests to “newsArticle” service every 1000 ms.\n  2 requests to “jobRecommendation” service every 1000 ms.\n  1 request to “articleRecommendation” service every 1000 ms.\n\n\nHere’s our client.json:\n\n{\n    \"zkConnectString\" : \"localhost:2181\",\n    \"zkSessionTimeout\" : 5000,\n    \"zkStartupTimeout\" : 5000,\n    \"zkLoadBalancerNotificationTimeout\" : 5000,\n    \"zkFlagFile\" : \"/tmp/suppressZkFlag\",\n    \"zkBasePath\" : \"/d2\",\n    \"fsBasePath\" : \"/tmp/backup\",\n    \"clientShutdownTimeout\" : 5000,\n    \"clientStartTimeout\" : 5000,\n    \"trafficProportion\" : {\n        \"newsArticle\": 3,\n        \"jobRecommendation\": 2,\n        \"articleRecommendation\" : 1\n    },\n    \"rateMillisecond\" : 1000\n}\n\n\nNow we are ready to add the following task to build.gradle:\n\ntask runClient(type: JavaExec) {\n    main = 'com.example.d2.client.ExampleD2Client'\n    classpath = sourceSets.main.runtimeClasspath\n    standardInput = System.in\n}\n\n\nTo run the client, run the following command in a different terminal\nconsole:\n\n../../gradlew runClient\n\n\nNext Steps\n\nCongratulations! You have finished this tutorial. Now you can build\nyour own D2 client/server applications. Next, you can learn the advanced\nfeatures of D2 examples like the following:\n\n\n  Partitioning and sticky routing\n  Tuning load balancer\n  Overriding client properties\n  And many more\n\n\nTo do so, check out the examples in the Res.tli source code. Go to\n/example/d2-advanced-examples.\n\nSee also Dynamic Discovery.\n",
      tags: null,
      id: 30
    });
    
    

  
    index.add({
      title: "Rest.li Quick Start Guide",
      category: null,
      content: "Rest.li Quick Start Guide\n\nContents\n\nFollow the steps below to try Rest.li quickly and get a basic idea of how it works:\n\n\n  Build\n  Run the Examples\n  API\n  Code Generator\n\n\nBuild\n\nRest.li uses Gradle as the build system. The following points summarize some basic tasks that can be run:\n\n\n  \n    Build (implies test)\n\n     ./gradlew build\n    \n  \n  \n    Test\n\n     ./gradlew test\n    \n  \n  \n    Clean\n\n     ./gradlew clean\n    \n  \n  \n    Generate and clean IntelliJ IDEA project stub\n\n     ./gradlew idea\n ./gradlew cleanIdea\n    \n  \n  \n    Generate and clean Eclipse project stub\n\n     ./gradlew eclipse\n ./gradlew cleanEclipse\n    \n  \n\n\nTasks can be executed on a per-module basis. For example, do this to only build the restli-server and its dependencies:\n\n./gradlew :restli-server:build\n\n\nRun the Examples\n\nRest.li comes with a set of examples to illustrate how the server and client interact. Rest.Li provides gradle tasks to run the server and client. There are 4 variants; all of them reside in restli-example-server and restli-example-client modules:\n\n\n  Basic example server without D2 RestLiExampleBasicServer\n  Example server with D2 RestLiExampleD2Server\n  Basic example client without D2 RestLiExampleBasicClient\n  Example client with D2 RestLiExampleD2Client\n\n\nBasic\nTo start, run the basic example server by doing this:\n\n./gradlew startExampleBasicServer\n\n\nThe build will be paused after printing “Basic example server running on port 7279. Press any key to stop server.” until you hit return. To quickly verify, use cURL as:\n\ncurl http://localhost:7279/photos/1\n\n\nYou should see a JSON object with some “photo” information. Do this to run the client:\n\n./gradlew startExampleBasicClient\n\n\nThe client will make a variety of requests to the server, print informative messages, and then shutdown. Each time, the result may be slightly different.\n\nD2\n\nTo use the D2 variants, you need ZooKeeper 3.3.4 and upward to be downloaded and running on port 2121. Before starting the server, some D2 related data must be initialized in ZooKeeper with D2ConfigDiscovery:\n\n./gradlew exampleConfigDiscovery\n\n\nThe D2 example server and client are started by this:\n\n./gradlew startExampleD2Server\n./gradlew startExampleD2Client\n\n\nThe client should successfully retrieve some “album” information from server and intentionally make a bad request to retrieve a non-existent photo, followed by a stack trace.\n\nAPI\n\nThroughout the examples, we can frequently see “photo” and “album” objects. These data schemas are defined in the restli-example-api module. API module are the interface modules with contents shared by or exchanged between the server and client. Generally speaking, we usually put 3 kinds of files in API:\n\n\n  Pegasus Data Schema — These files define data schemas such as “photo” and “album” above. The syntax of PDL is Java-like. Take a look at Photo.pdl, and the comments inside could be useful. For more information, see Data.\n  restspec.json — These files are Rest.li IDLs that define the interface and protocol a Rest.li resource exposes. You can find the “photo” resource IDL at com.linkedin.restli.example.photos.photos.restspec.json. For more information, check Rest.li User Guide.\n  Common Java classes — Shared by server and client.\n\n\nCode Generator\n\nPegasus comes with many code generators:\n\n\n  Schema (PDL) binding generator — Java classes can be generated from all data schema files. These generated classes come with methods and fields to interact with the underlying data object and provide native Java interop interface.\n  restspec.json generator — While schema files are usually handwritten, restspec.json files are generated from resource classes using the com.linkedin.restli.tools.idlgen.RestLiResourceModelExporterCmdLineApp class.\n  Builder generator — Java classes can also be generated from all .restspec.json files. These builder classes provide convenient methods to construct Rest.li requests with various parameters.\n\n\nYou can find example Gradle scripts of how to call the generators in the build_script directory.\n",
      tags: null,
      id: 31
    });
    
    

  
    index.add({
      title: "Gradle build integration",
      category: null,
      content: "Gradle Build Integration\n\nContents\n\n\n  Introduction\n  An Example\n  Compatibility\n  Publishing Maven Artifacts\n  Pegasus Plugin in Detail\n  Underlying Java Classes for Build Integration\n\n\nIntroduction\nGradle integration is provided as part of Rest.li.  Pegasus simplifies use of Rest.li’s code generators and compatibility checking by fully integrating them into the build system. (Note ‘pegasus’ is also the code name for the Rest.li project).\n\nThe underlying Java classes that enable code generation and validation are part of the Rest.li source and could be used to integrate with other build tools.\n\nAdding the plugin is simple. First add a buildscript dependency on the com.linkedin.pegasus:pegasus-plugin artifact and then use apply plugin 'pegasus' in your build.gradle files.  This topic is explained in detail below.\n\nGradle 1.8+ is required.\n\nAn Example\n\nAs an example,  let’s consider a simple Rest.li project with three modules:\n\n\n  An /api module containing pegasus schema definitions in the src/main/pegasus directory.  This is where java client bindings for the service will be generated. (The client-bindings are sometimes not a separate project, but they are put into the /api project along with the schemas.)\n  A /server module containing resources defined in java classes in the src/main/java directory under the com.linkedin.restli.example.impl namespace  (E.g., com.linkedin.restli.example.impl.RestLiExampleBasicServer.java).\n  An example java client that uses the client-bindings.\n\n\nRoot build.gradle\n\n/build.gradle:\n\napply plugin: 'idea'\napply plugin: 'eclipse'\n\nproject.ext.externalDependency = [\n  'pegasusVersion' : '&lt;version&gt;'\n]\n\nbuildscript {\n  repositories {\n    mavenLocal()\n    mavenCentral()\n  }\n  dependencies {\n    classpath group: 'com.linkedin.pegasus', name: 'gradle-plugins', version: '&lt;version&gt;'\n  }\n}\n\nsubprojects {\n  apply plugin: 'maven'\n\n  afterEvaluate {\n    if (project.plugins.hasPlugin('java')) {\n      sourceCompatibility = JavaVersion.VERSION_1_6\n    }\n\n    // add the standard pegasus dependencies wherever the plugin is used\n    if (project.plugins.hasPlugin('pegasus')) {\n      dependencies {\n        dataTemplateCompile spec.product.pegasus.data\n        restClientCompile spec.product.pegasus.restliClient\n\n        // needed for Gradle 1.9+\n        restClientCompile spec.product.pegasus.restliCommon\n      }\n    }\n  }\n\n  repositories {\n    mavenLocal()\n    mavenCentral()\n  }\n}\n\n\n/settings.gradle:\n\ninclude 'api'\ninclude 'server'\ninclude 'client'\n\n\nbuild.gradle for Data API Project\n\n/api/build.gradle:\n\napply plugin: 'pegasus'\n\n\nIn /api, pegasus data schemas (.pdl files) should be added under /src/main/pegasus.  E.g.\n/src/main/pegasus/com/linkedin/restli/example/Hello.pdl.\n\nThe pegasus plugin will detect the presence of .pdl files and automatically use the dataTemplateGenerator task to\ngenerate Java bindings for them.  In this example,  a Hello.java class would be generated.\n\nThe dataTemplateCompile task automatically adds pegasus schemas that Hello.pdl depends on, in this case, Hello.pdl\ndepends only on the core data libraries of pegasus, but projects containing other .pdl files could be depended on.\n\nPegasus will detect when a project contains interface definitions (called IDL and located in .restspec.json files) in\nit’s /src/mainGeneratedRest/idl directory (usually copied in from an idl extraction task from the server, see below)\nand will generate java bindings.   For example, HelloBuilder.java is generated from the idl of the hello resource\n(/src/main/idl/com/linkedin/restli/example/impl/Hello.restspec.json) and it written to the\n/src/mainGeneratedRest/java’ directory of the /api project.\n\nbuild.gradle for Server project\n\n/server/build.gradle:\n\napply plugin: 'java'\napply plugin: 'pegasus'\n\next.apiProject = project(':api')\npegasus.main.idlOptions.addIdlItem(['com.linkedin.restli.example.impl']) // optional, if not set, all packages are scanned for resource classes\n\ndependencies {\n  compile project(path: ':api', configuration: 'dataTemplate')\n  compile \"com.linkedin.pegasus:restli-server:&lt;version&gt;\"\n  // ...\n}\n\n\nIn /server, pegasus “Resource” java classes should be defined and should be in the package(s) referred to by\npegasus.main.idlOptions.  E.g.  /src/main/java/com/linkedin/restli/example/impl.HelloResource.java.\n\nPegasus will extract an interface definition (.restspec.json) from the resource class and write it to\n/src/mainGeneratedRest/idl directory.\n\nOnce the idl has been generated, it will be copied to the project identified by ext.apiProject.  In this example, it\nwill be copied to /api/src/main/idl.  Before it is copied, api/src/main/idl is scanned for pre-existing idl.  If any\nis found,  it is compared with the replacement idl that will be copied in and a compatibility checker is run that will\nreturn errors if the replacement idl is not backward compatible with the existing idl.  The compatibility checks can be\ndisabled by setting (but be warned, compatibility errors mean that a server running the new interface definition is now\nincompatible with clients running older versions, and should not be pushed to production systems).  If the compatibility\nchecks pass, the idl is copied into the client directories.  Once copied, new ‘Client Bindings’ may be generated for the\nclient, see below.\n\nThe compile dependency on :api is required if the HelloResource.java depends on Hello.pdl and it’s generated binding\nHello.java.   Note that the dependency includes a ‘configuration’ identifying this as a ‘dataTemplate’ dependency.\n\nbuild.gradle for Example Java Client\n\n/client/build.gradle:\n\napply plugin: 'java'\n\ndependencies {\n  compile project(path: ':api', configuration: 'restClient')\n}\n\n\nOnce rest client bindings in the api project have been generated,  it is trivial for a engineer to depend on the api project and use the generated client bindings to make calls to the new rest.li interface from any remote service.\n\nOne must add a compile dependency the ‘api’ project (or depend on it’s published artifacts, more about this below) and be sure to set the dependency configuration to ‘restClient’.  Once this is done, it’s easy to use the HelloBuilder class to construct a request.\n\nCompatibility\n\nTo manage compatibility checking use the rest.model.compatibility flag.   There are 4 different options:  off, equivalent, backwards and ignore.\n\nBy default, the compatibility strategy is backwards. It will only fail on backwards incompatible changes and is the recommended setting to run during normal development.\n\nIf you are building rest.li services in a continuous integration environment, we suggest that you set builds to run on equivalent, meaning that ALL changes to an interface will cause a build failure. This will ensure that checked in code exactly corresponds with the interface.\n\nIf set to off, the compatibility check is skipped entirely. ignore will run the compatibility checker but will not fail for backward incompatible changes (and will print out the incompatibilities).\n\nIf desired, you may set a local default compatibility level. To do so, modify or create a ~/.gradle/gradle.properties to include:\n\n~/.gradle/gradle.properties:\n\nPrest.model.compatibility=&lt;desired compatibility level here&gt;\n\n\nFor example, to run a build ignoring backward incompatable interface changes (WARNING: remember that backward incompatible changes could break your clients):\n\ngradle build -Prest.model.compatibility=ignore\n\n\nTo acknowledge a backwards compatible interface change use:\n\ngradle build -Prest.model.compatibility=backwards\n\n\nFor additional details on compatibility checking, see Resource Compatibility Checking.\n\nPublishing Maven Artifacts\n\nOften, the client bindings need to be accessible to developers outside the project workspace where the service is developed.\n\nTo publish rest client bindings to any maven repo first modify the api project’s gradle to look like:\n\n/api/build.gradle:\n\n// ... /api/build.gradle code from above ...\n\nartifacts {\n  archives mainRestClientJar\n  archives mainDataTemplateJar\n} \n\nconfigure(install.repositories.mavenInstaller) {\n  addFilter('rest-client') {artifact, file -&gt;\n    artifact.name == 'api-rest-client'\n  }.artifactId = 'rest-client'\n  \n  addFilter('data-template') {artifact, file -&gt;\n    artifact.name == 'api-data-template'\n  }.artifactId = 'data-template'\n  // artifact names for 'data-model', 'avro-schema' and 'rest-model' may be added as well if needed\n}\n\n\nThe artifacts section tells gradle to build jar files for the rest client bindings and the data templates.\n\nThe configure part instructs gradle to publish both artifacts into maven. Set names for each. (By default, gradle names the artifact publish to maven to api. Since there are two artifacts, they need to be given distinct names.)\n\nNext, update the root build.gradle file to include project information withing the subprojects section:\n\n/build.gradle\n\n// ... /build.gradle code from above ...\n\nsubprojects {\n  // ...\n\n  project.group = 'org.example'\n  project.version = '0.1'\n}\n\n\nOnce the api build.gradle is updated, one can publish the maven artifacts. To publish to the maven local repo, simply run:\n\ngradle install\n\n\nto publish to a remove maven repository follow the gradle documentation\n\nOnce published, other projects may import the client bindings by depending on the two maven artifacts. For example:\n\ndependencies {\n  compile \"org.example:rest-client:0.1\"\n  compile \"org.example:data-template:0.1\"\n}\n\n\nPegasus Plugin in Detail\n\nThe gradle tasks for pegasus are provided by the ‘pegasus’ plugin.  The source for this plugin is in\nPegasusGeneratorV2Plugin.groovy.  This plugin defines custom of gradle SourceDirectorySets for the ‘idl’, ‘pegasus’\nsource types and tasks for the rest.li code generators.  It also defines custom published artifact “configurations”\nand dependencies on between these custom published artifact “configurations”.\n\nSource Directory Sets\n\nThe plugin recognizes a number of source directories in rest.li projects.  When any of these directories are detected (and they contain at least one source file), the plugin dynamically adds tasks the gradle build dependency tree for these directories.\n\nIn this section we below refers to gradle sourceSets.  The most common sourceSets are main and test.\n\nsrc/{sourceSet}/pegasus\n\nUsed by ‘api’ modules.\n\nContains data schemas (.pdl) files.  If data schema files are present in this directory, the\ngenerate{sourceSet}DataTemplate tasks (e.g. generateMainDataTemplate) will generate java data templates\n(RecordTemplate java classes) in the src/{sourceSet}GeneratedDataTemplate directory.\n\nThe data schemas files are published into a *-data-template.jar artifact.  If ivy is used this artifact is published with the module name and under the ‘data-template’ classification.\n\nThe generated java data templates (RecordTemplate java classes) are are published as a -data-model.jar artifact.   If ivy is used this artifact is published with the module name and under the ‘data-model’ classification.\n\nsrc/{sourceSet}GeneratedRest/idl\n\nUsed by ‘server’ modules.\n\nThese files are generated by the generateRestModel task, for modules containing {*Resource.java} files (which must be\nin a package referenced by pegasus.{sourceSet}.idlOptions.addIdlItem(namespaces)).   One important aspect of idl is\nthat by convention they are generated by a ‘server’ module (and written to the src/{sourceSet}GeneratedRest/idl) and\nthen are copied to the /src/{sourceSet}/idl directory of an api module (via the ext.apiProject property).\n\nNo artifacts are published directly from the server for these files,  see src/{sourceSet}/idl for details on how they are published from the ‘api’ project.\n\nsrc/{sourceSet}/idl\n\nUsed by ‘api’ modules.\n\nContains published idl (.restspec.json) files.  These files represent the interface definition of the rest.li resources\nprovided by some service.   They should be checked in to source control.   They are copied into the idl directory from\nserver module by the publishRestliIdl task.  For this copy to happen the server module must contain a ext.apiProject\nproperty referencing this ‘api’ module.  As part of this copy, idl compatibility validation will be run (see above for\ndetails).\n\nThe idl is published as a *-rest-model.jar artifact.  If ivy is used this artifact is published with the module name and under the ‘rest-model’ classification.\n\nsrc/{sourceSet}GeneratedAvroSchema/avro\n\nUsed by ‘api’ modules.\n\nAvro schema files (.avsc) generated from pegasus data schema files (.pdl) by the generateAvroSchema task.\n\nGenerator Tasks\n\nAll the following tasks are automatically added by the ‘pegasus’ gradle plugin into the gradle task dependency\nhierarchy.  They run automatically and in the correct order run as part of ‘gradle build’, ‘gradle jar’ and ‘gradle\ncompileJava’ when the plugin detects that they are needed.\n\ngenerateRestModel\n\nGenerates .restspec.json files from java files annotated as rest.li resources in the namespaces that have been added to\nthe idl list using pegasus.{sourceSet}.idlOptions.addIdlItem(). Writes these .restspec.json files into the\nsrc/{sourceSet}GeneratedRest/idl directory.  This tasks is depended on by the publishRestliIdl task.\n\npublishRestliIdl\n\nCopies idl (restspec.json) from server to api project (or whatever the ext.apiProject property is set to).  These files\nare normally located in the src/mainGeneratedRest/idl directory in the server project and the src/main/idl direcotry\nin the api project.  This tasks runs compatibility validation (see above).  While not strictly a ‘generate’ task, it is\na essential part of the generator flow.  It is depended on by the jar task.\n\npublishRestliSnapshot\n\nWorks the same as publishRestliIdl except that it copies “snapshot.json” files usually located in src/mainGeneratedRest/snapshot from the server project to the src/main/snapshot directory in the api project.\n\nSnapshot files are used for compatibility checking whereas idl files are the formal interface definition and are used to generate client bindings.\n\ngenerate{sourceSet}GeneratedRestRestClient\n\nGenerates java client bindings (*Builders.java classes) into the src/{sourceSet}GeneratedRest/java.  It depends on\nthe .restspec.json  files in src/{sourceSet}/idl directory and the pegasus schemas (.pdl files) in\nsrc/{sourceSet}/pegasus as well as from ‘dataModel’ dependencies (in ivy, these are dependencies from the “data-model”\nclassification).   Depended on by the compileJava task.\n\ngenerateDataTemplate\n\nGenerates java data template bindings (RecordTemplate java classes).  It depends on the pegasus schemas (.pdl files)\nin src/{sourceSet}/pegasus as well as from ‘dataModel’ dependencies (in ivy, these are dependencies from the\n“data-model” classification).  Depended on by the compileJava task.\n\ngenerateAvroSchema\n\nGenerates avro schemas (.avsc files) from the pegasus schemas (.pdl files) in src/{sourceSet}/pegasus.  Requires the\nsame ‘dataModel’ dependencies as required by the pegasus schemas (in ivy, these are dependencies from the “data-model”\nclassification).  Depended on by generateDataTemplate task.\n\nTo run this task, the avroSchemaGenerator “configuration” must be configured with rest.li’s data-avro-generator artifact.  This is done by adding the follow dependency:\n\n\ndependencies {\n  avroSchemaGenerator \"com.linkedin.pegasus:data-avro-generator:&lt;pegasus-version&gt;\"\n}\n\nAnd then adding the following configuration in build.gradle to enable avro schema generation:\npegasus.main.generationModes = [PegasusGenerationMode.AVRO]\n\n\nPublished artifacts and their classifications\n\n*-data-model.jar artifact\n\nContains data schema files, generated by the generateDataModel task.   This is only generated from a project if it contains one or more schema files in it’s src/{sourceSet}/pegasus directory.\n\n\n  Ivy coordinates: use module’s group, name and version, use ‘data-model’ as classification\n  Maven coordinates: use module’s group and version.  Use whatever name was configured for the mavenInstaller, which by convention should be ‘{modulename}-data-model’ (see above section about publish maven artifacts).\n\n\n*-data-template.jar artifact\n\nContains java generated bindings (.class files) for accessing the pegasus schemas (.pdl files) in the module’s\nsrc/{sourceSet}/pegasus directory.  This artifact is generated by the generateDataTemplate task.  This artifact is\nonly generated from a project if it contains one or more schema files in it’s src/{sourceSet}/pegasus directory.\n\nThis artifact will also define dependencies in its .pom or .ivy file to data-template artifacts it depends on (these are specified as dataTemplate dependencies in the module’s build.gradle).\n\n\n  Ivy coordinates: use module’s group, name and version, use ‘data-template’ as classification\n  Maven coordinates: use module’s group and version.  Use whatever name was configured for the mavenInstaller, which by convention should be ‘{modulename}-data-template’ (see above section about publish maven artifacts).\n\n\n*-avro-schema.jar artifact\n\nContains .avro schema files for the pegasus schemas (.pdl files) in this module’s src/{sourceSet}/pegasus directory.  This .avro files are generated by the generateAvroSchema task.\n\nThis artifact will also define dependencies in it’s .pom or .ivy file to avro-schemas artifacts it depends on (these are specified as dataTemplate dependencies in the module’s build.gradle).\n\n\n  Ivy coordinates: use module’s group, name and version, use ‘avro-schema’ as classification\n  Maven coordinates: use module’s group and version.  Use whatever name was configured for the mavenInstaller, which by convention should be ‘{modulename}-avro-schema’ (see above section about publish maven artifacts).\n\n\n*-rest-model.jar artifact\n\nContains .idl (restspec.json) files for the idl in the module’s `src/{sourceSet}/idl’ directory.  These .idl files are generated by the generateRestModel task from a server then copied to an api project by the publishRestliIdl task (via the ext.apiProject property).\n\n\n  Ivy coordinates: use module’s group, name and version, use ‘rest-model’ as classification\n  Maven coordinates: use module’s group and version.  Use whatever name was configured for the mavenInstaller, which by convention should be ‘{modulename}-rest-model’ (see above section about publish maven artifacts).\n\n\n*-rest-client.jar\n\nContains rest client java bindings (*Builders.java classes) generated from the idl of the source module.\n\nThis artifact will also define dependencies in its .pom or .ivy file to java data template binding artifacts\n(*-data-model.jar) it requires, including the one for the module itself and for any other pegasus schemas it depends\non (these are specified as dataModel dependencies in the module’s build.gradle).\n\n\n  Ivy coordinates: use module’s group, name and version, use ‘rest-client’ as classification\n  Maven coordinates: use module’s group and version.  Use whatever name was configured for the mavenInstaller, which by convention should be ‘{modulename}-rest-client’ (see above section about publish maven artifacts).\n\n\nDependency types\n\nThere are two types of pegasus plugin dependency types.  The first type is one required by the plugin for running code\ngenerators and compiling code.  The second type is those developers can use to define different sorts dependencies\nbetween the various source languages, primarily pegasus schemas (.pdl files).\n\nDependencies used by build tooling\n\nrestTools - required by ‘api’ and ‘server’ modules to generate rest client bindings (*Builders.java files), run compatibility checks, and use rest.li document generation (docgen).   The dependency must refer to a compatible version of the pegasus:rest-tools artifact.\n\ndataTemplateCompile - Required by ‘api’ modules to do data template compilation.  The dependency must refer to a compatible version of the pegasus:data artifact.\n\ndataTemplateGenerator - Required by ‘api’ modules to do data template generation. The dependency must refer to a compatible version of the pegasus:generator artifact.\n\nrestClientCompile - Required by ‘api’ modules to compile client java bindings (*Builders.java files) to .class files. The dependency must refer to a compatible version of the pegasus:restli-client artifact.\n\nExample build.gradle for an ‘api’ module:\n\n...\ndependencies {\n  compile \"com.linkedin.pegasus:restli-client:&lt;version&gt;\"\n\n  dataTemplateCompile \"com.linkedin.pegasus:data:&lt;version&gt;\"\n  dataTemplateGenerator \"com.linkedin.pegasus:generator:&lt;version&gt;\"\n  restTools \"com.linkedin.pegasus:restli-tools:&lt;version&gt;\"\n  restClientCompile \"com.linkedin.pegasus:restli-client:&lt;version&gt;\"\n}\n\n\nPegasus Schema Dependencies\n\ndataTemplate - Adds a dependency on the pegasus schemas from another module or artifact.  This is required when the current module’s data schema files refer to schema types that reside in another module or artifact.\n\nExample build.gradle:\n\n...\ndependencies {\n  // for ivy:\n  dataTemplate group: 'org.example', name: 'common-pegasus-schemas', version: '1.0', classifier: 'dataTemplate'\n  // for maven, remove the classifier and change the name to match the artifact name of the published dataTemplate, by convention it should be '{modulename}-data-template}'\n  ...\n}\n\n\nUnderlying Java Classes for Build Integration\n\nThis is provided for reference only.  A understanding of these classes is not required to use pegasus.  These classes would be useful primarily if one were deeply integrating pegasus with a build system not already supported by pegasus.\n\nAvro Schema Generator\n\nGenerate Avro avsc files from Pegasus data schemas (.pdl files):\n\njava [-Dgenerator.resolver.path=&lt;dataSchemaRelativePath&gt;] \\\n  [-Dgenerator.avro.optional.default=&lt;optionalDefault&gt;] \\\n  [-Dgenerator.avro.namespace.override=&lt;overrideNamespace&gt;] \\\n  -cp &lt;CLASSPATH&gt; com.linkedin.data.avro.generator.AvroSchemaGenerator \\\n  &lt;outputDir&gt; [&lt;inputFileOrDir&gt; ...]\n\n\n\n  dataSchemaRelativePath - Path to .pdl files. (e.g., /src/main/pegasus).\n  optionalDefault - Specifies how an optional field with a default value should be translated (see Converting Rest.li to Avro).\n  overrideNamespace - If true, each translated .avsc file will have its namespace prepended with \"avro.\" (see Converting Rest.li to Avro).\n  CLASSPATH - com.linkedin.pegasus:data:[CURRENT_VERSION] AND com.linkedin.pegasus:data-avro:[CURRENT_VERSION] artifacts and all their dependencies.\n  outputDir - output directory for generated .avsc files.\n  inputFileOrDir - file name of a Pegasus data schema file, a directory containing Pegasus data schema files, or a fully qualified schema name.\n\n\nBuild integration: for builds requiring avro schemas, assembly (creation of jar) should depend on this task\n\nPegasus Data Template Generator\n\nGenerates Java data templates (.java files) from Pegasus Data Model schemas (.pdl files):\n\njava [-Dgenerator.resolver.path=&lt;dataSchemaRelativePath&gt;] -cp &lt;CLASSPATH&gt; \\\n  com.linkedin.pegasus.generator.PegasusDataTemplateGenerator \\\n  &lt;outputDir&gt; [&lt;inputFileOrDir&gt; ...]\n\n\n\n  dataSchemaRelativePath - Path to data schema files. (e.g., /src/main/pegasus).\n  CLASSPATH - com.linkedin.pegasus:generator:[CURRENT_VERSION] artifact and all its dependencies.\n  outputDir - output directory for generated java source files\n  inputFileOrDir - file name of a Pegasus data schema file, a directory containing Pegasus data schema files, or a fully qualified schema name\n\n\nGenerate Rest Model IDL\n\nSerializes a set of resource models to a RESTspec IDL file:\n\njava -cp &lt;CLASSPATH&gt; com.linkedin.restli.tools.idlgen.RestLiResourceModelExporterCmdLineApp \\\n  -outdir &lt;outputDirPath&gt; -sourcepath &lt;sourcePath&gt; -resourcepackages &lt;resourcePackages&gt;\n\n\n\n  CLASSPATH - com.linkedin.pegasus:restli-tools:[CURRENT_VERSION] artifact and all its dependencies. Compiled classes within the java packages referred to by resourcePackages\n  outputDirPath - Directory in which to output the generated IDL files (default=current working dir)\n  sourcePath - Space-delimited list of directories in which to find resource Java source files\n  resourcePackages - Space-delimited list of packages to scan for resource classes\n\n\nBuild integration: assembly (creation of jar) should depend on this task. This task depends on compilation of classes within the java packages referred to by resourcePackages.\n\nValidate and Publish IDL\n\nCopies IDL (.restspec.json) files to client module and check backwards compatibility between pairs of idl (.restspec.json) files. The check result messages are categorized:\n\njava [-Dgenerator.resolver.path=&lt;dataSchemaRelativePath&gt;] -cp CLASSPATH \\\n  com.linkedin.restli.tools.idlcheck.RestLiResourceModelCompatibilityChecker \\\n  [--compat OFF|IGNORE|BACKWARDS|EQUIVALENT] [pairs of &lt;prevRestspecPath currRestspecPath&gt;]\n\n\n\n  dataSchemaRelativePath - Path to data schema files required by the interface definition (e.g.  /src/main/pegasus).\n  CLASSPATH - com.linkedin.pegasus:restli-tools:[CURRENT_VERSION] artifact and all it’s dependencies.\n  prevRestspecPath -\n  currRestspecPath -\n\n\nBuild integration: assembly (creation of jar) should depend on this task. If compatibility checker passes, all\n.restspec.json files should be copied from the server module to the module where client bindings are generated.  This\ntask depends on the Generate Rest Model IDL task.  A property named rest.model.compatibility should be overridable\nby the developer (allowing them to set it to ignore or backwards) and should default to ‘equivalent’ if they do not\nprovide it.\n\nRest Client Generation\nGenerates Java request builders from Rest.li idl:\n\njava [-Dgenerator.resolver.path=&lt;dataSchemaRelativePath&gt;] \\\n     [-Dgenerator.rest.generate.datatemplates=&lt;true|false&gt;] \\\n  -cp &lt;CLASSPATH&gt; com.linkedin.restli.tools.clientgen.RestRequestBuilderGenerator \\\n  &lt;targetDirectoryPath&gt; [&lt;sourceFileOrDir&gt; ...]\n\n\n\n  dataSchemaRelativePath - Path to data schema files required by the interface definition.\n  generator.rest.generate.datatemplates - false unless task should also generate java data template bindings\n  CLASSPATH - com.linkedin.pegasus:restli-tools:[CURRENT_VERSION] artifact and all its dependencies.\n  targetDirectoryPath - path to target root java source directory\n  sourceFileOrDir - paths to IDL files or directories\n\n\nBuild integration: Compilation of java source should depend on this task.\n\nConfig Build Script\n\nTo construct these build tasks, it can help to add a utility task that constructs a list of all the source paths used for data template generation, avro schema generation, rest model generation and rest client generation.\n\nClean Generated\n\nNo java class for this.  All directories written to by data template generation, avro schema generation, rest model generation and rest client generation should be deleted.\n\nBuild integration: clean task should depend on this\n",
      tags: null,
      id: 32
    });
    
    

  
    index.add({
      title: "Rest.li client user guide",
      category: null,
      content: "Rest.li Client User Guide\n\nContents\n\n\n  Introduction\n  Depending on a Service’s Client\nBindings\n  Depending on Data Templates\n  Type-Safe Builders\n  Built-in Request and RequestBuilder\nclasses\n  Restspec IDL\n  RestClient\n  Request Options\n  ParSeq Integrated Rest Client\n  Client Code Generator Tool\n  Rest.li-extras\n\n\nIntroduction\n\nThe Rest.li client framework provides support for accessing resources\ndefined using Rest.li. The client framework consists of two parts:\n\n\n  RequestBuilder classes, which provide an interface for\ncreating REST requests to access a specific method of a resource.\nRequest builders work entirely in-memory and do not communicate with\nremote endpoints.\n  RestClient, which provides an interface for sending\nrequests to remote endpoints and receiving responses.\n\n\nThe request builder portion of the framework can be further divided into\ntwo layers:\n\n\n  Built-in request builder classes, which provide generic support for\naccessing Rest.li resources. The built-in request builders\nunderstand how to construct requests for the different Rest.li\nresource methods, but they do not have knowledge of any specific\nresources or the methods they support. Therefore, the built-in\nrequest builders cannot validate that a request will be supported by\nthe remote endpoint.\n  Type-safe request builder classes, which are generated from the\nserver resource’s IDL. The type-safe request builders are tailored\nto the specific resource methods supported by each resource. The\ntype-safe builders provide an API that guides the developer towards\nconstructing valid requests.\n\n\nMost developers should work with the type-safe request builders, unless\nthere is a specific need to work with arbitrary resources whose\ninterfaces are unknown at the time the code is written.\n\n\n\nDepending on a Service’s Client Bindings\n\nUsually, developers building Rest.li services publish Java client\nbindings for the Rest.li resources their service provides as artifacts\ninto a shared repository, such as a maven repo. By adding a dependency\nto these artifacts, other developers can quickly get their hands on the\nrequest builder classes defined in these client bindings to make\nrequests to the resources provided by that service.\n\nTo add a dependency from a gradle project, add the artifact containing\nthe rest client bindings to your dependency list. If you are unsure of\nthe name of the artifact, ask the service owners. They are usually the\nartifact with a name ending in -client, -api or -rest. Note that the\nconfiguration\nfor the dependency must be set to restClient:\n\nbuild.gradle:\n...\ndependencies {\n// for a local project:\ncompile project(path: ':example-api', configuration: 'restClient')\n// for a versioned artifact:\ncompile group: 'org.somegroup', name: 'example-api', version: '1.0', configuration: 'restClient'\n}\n...\n\n\n\n\nDepending on Data Templates\n\nTo add a dependency to Java bindings for data models, add a\ndataTemplate configured dependency in your build.gradle,\nfor example:\n\nbuild.gradle:\n...\n﻿dependencies {\n    // for a local project:\n    compile project(path: ':example-api', configuration: 'dataTemplate')\n    // for a versioned artifact:\n    compile group: 'org.somegroup', name: 'example-api', version: '1.0', configuration: 'dataTemplate'\n}\n...\n\n\nNote that you should not usually need to add such a dependency when\nadding a restClient dependency, as the\nrestClient should bring in the dataTemplate\ntransitively.\n\nNote: If you are writing pegasus schemas (.pdl files) and need to add a\ndependency on other pegasus schemas, you need to add a\ndataModel dependency:\n\nbuild.gradle\n...\n    dataModel spec.product.example.data\n...\n\n\n\n\nType-Safe Builders\n\nThe client framework includes a code-generation tool that reads the IDL\nand generates type-safe Java binding for each resource and its supported\nmethods. The bindings are represented as RequestBuilder classes.\n\n\n\nResource Builder Factory\n\nFor each resource described in an IDL file, a corresponding builder\nfactory will be generated. For Rest.li version &lt; 1.24.4, the builder\nfactory will be named &lt;Resource name&gt;Builders. For Rest.li version &gt;=\n1.24.4, the builder factory is named &lt;Resource name&gt;RequestBuilders.\nThe factory contains a factory method for each resource method supported\nby the resource. The factory method returns a request builder object\nwith type-safe bindings for the given method.\n\nStandard CRUD methods are named create(), get(), update(),\npartialUpdate(), delete(), and batchGet(). Action methods use the\nname of the action, prefixed by “action”, action&lt;ActionName&gt;(). Finder\nmethods use the name of the finder, prefixed by “findBy”,\nfindBy&lt;FinderName&gt;(). BatchFinder methods use the name of the batchFinder, \nprefixed by “batchFindBy”, batchFindBy&lt;BatchFinderName&gt;().\n\nAn example for a resource named “Greetings” is shown below. Here is the\nbuilder factory for Rest.li &lt; 1.24.4:\n\npublic class GreetingsBuilders {\n    public GreetingsBuilders()\n    public GreetingsBuilders(String primaryResourceName)\n    public GreetingsCreateBuilder create()\n    public GreetingsGetBuilder get()\n    public GreetingsUpdateBuilder update()\n    public GreetingsPartialUpdateBuilder partialUpdate()\n    public GreetingsDeleteBuilder delete()\n    public GreetingsBatchGetBuilder batchGet()\n    public GreetingsBatchCreateBuilder batchCreate()\n    public GreetingsBatchUpdateBuilder batchUpdate()\n    public GreetingsBatchPartialUpdateBuilder batchPartialUpdate()\n    public GreetingsBatchDeleteBuilder batchDelete()\n    public GreetingsDoSomeActionBuilder actionSomeAction()\n    public GreetingsFindBySearchBuilder findBySearch()\n    public GreetingsBatchFindBySomeSearchCriteriaBuilder batchFindBySomeSearchCriteria()\n}\n\n\nHere is the builder factory for Rest.li &gt;= 1.24.4:\n\npublic class GreetingsRequestBuilders extends BuilderBase {\n    public GreetingsRequestBuilders()\n    public GreetingsRequestBuilders(String primaryResourceName)\n    public GreetingsCreateRequestBuilder create()\n    public GreetingsGetRequestBuilder get()\n    public GreetingsUpdateRequestBuilder update()\n    public GreetingsPartialUpdateRequestBuilder partialUpdate()\n    public GreetingsDeleteRequestBuilder delete()\n    public GreetingsBatchGetRequestBuilder batchGet()\n    public GreetingsBatchCreateRequestBuilder batchCreate()\n    public GreetingsBatchUpdateRequestBuilder batchUpdate()\n    public GreetingsBatchPartialUpdateRequestBuilder batchPartialUpdate()\n    public GreetingsBatchDeleteRequestBuilder batchDelete()\n    public GreetingsDoSomeActionRequestBuilder actionSomeAction()\n    public GreetingsFindBySearchRequestBuilder findBySearch()\n    public GreetingsBatchFindBySomeSearchCriteriaRequestBuilder batchFindBySomeSearchCriteria()\n}\n\n\nGET Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated GET request builder for a resource\nis named &lt;Resource&gt;GetBuilder. In Rest.li &gt;= 1.24.4, the generated\nGET request builder is named &lt;Resource&gt;GetRequestBuilder. Both support\nthe full interface of the built-in GetRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\n    public &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\ne.g., for a parent pathKey named “groupId” of type Integer in the\n“Contacts” resource, the binding method in Rest.li &lt; 1.24.4 would be:\n    public ContactsGetBuilder groupIdKey(Integer key)\n\n\nIn Rest.li &gt;= 1.24.4, it would be:\n    public ContactsGetRequestBuilder groupIdKey(Integer key)\n\n\nBATCH_GET Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated BATCH_GET request builder for a\nresource is named &lt;Resource&gt;BatchGetBuilder. The generated builder\nsupports the full interface of the built-in BatchGetRequestBuilder.\n\nIn Rest.li &gt;= 1.24.4, the generated BATCH_GET request builder for a\nresource is named &lt;Resource&gt;BatchGetRequestBuilder. The generated\nbuilder extends the built-in BatchGetEntityRequestBuilder.\n\nWhen building requests with BatchGetRequestBuilder, use the\nbuildKV() method (build() is deprecated), for example:\n    new FortunesBuilders().batchGet().ids(...).buildKV()\n\n\nWhen building requests with the BatchGetEntityRequestBuilder, the\nbuild() method is used.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\n    public &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\nFor example, a parent pathKey named “groupId” of type Integer in the\n“Contacts” resource will have the binding method in Rest.li &lt; 1.24.4 be\nthis:\n    public ContactsBatchGetBuilder groupIdKey(Integer key)\n\n\nIn Rest.li &gt;= 1.24.4, it would be:\n    public ContactsBatchGetRequestBuilder groupIdKey(Integer key)\n\n\nFINDER Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated FINDER request builder for a\nresource is named &lt;Resource&gt;FindBy&lt;FinderName&gt;Builder, while in\nRest.li &gt;= 1.24.4 it is named\n&lt;Resource&gt;FindBy&lt;FinderName&gt;RequestBuilder. Both builders support the\nfull interface of the built-in FindRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\n    public &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nThe generated builder will contain a method to set each of the finder’s\nquery parameters, of the form:\n    public &lt;BuilderType&gt; &lt;paramName&gt;Param(&lt;ParamType&gt; value);\n\n\nThe value must be non-null.\n\nIf the finder specifies AssocKey parameters, the builder will contain\na method to set each of them, of the form:\n    public &lt;BuilderType&gt; &lt;assocKeyName&gt;Key(&lt;AssocKeyType&gt; value);\n\n\nBATCH FINDER Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated BATCH_FINDER request builder for a\nresource is named &lt;Resource&gt;BatchFindBy&lt;BatchFinderName&gt;Builder, while in\nRest.li &gt;= 1.24.4 it is named\n&lt;Resource&gt;BatchFindBy&lt;BatchFinderName&gt;RequestBuilder. Both builders support the\nfull interface of the built-in BatchFindRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\n    public &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nThe generated builder will contain a method to set each of the batchFinder’s\nquery parameters, of the form:\n    public &lt;BuilderType&gt; &lt;paramName&gt;Param(&lt;ParamType&gt; value);\n\n\nThe value must be non-null. For the batch query parameter, it also uses the form above\nlike the other regular parameters.\n\nIf the batchFinder specifies AssocKey parameters, the builder will contain\na method to set each of them, of the form:\n    public &lt;BuilderType&gt; &lt;assocKeyName&gt;Key(&lt;AssocKeyType&gt; value);\n\nSee more details about the BATCH_FINDER java request builder here.\n\nCREATE Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated CREATE request builder for a\nresource is named &lt;Resource&gt;CreateBuilder. The generated builder\nsupports the full interface of the built-in CreateRequestBuilder.\n\nIn Rest.li &gt;= 1.24.4, the generated CREATE request builder for a\nresource is named &lt;Resource&gt;CreateRequestBuilder. The generated\nbuilder extends the built-in CreateIdRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\n    public &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nIf @ReturnEntity annotation is specified for CREATE implementation, an\nadditional CreateAndGet request builder will be generated. Note that\nCreate request builder is still available so that adding\n@ReturnEntity is backward compatible for a Java client.\npublic class &lt;Resource&gt;RequestBuilders\n{\n...\n    public &lt;Resource&gt;CreateRequestBuilder create();\n    public &lt;Resource&gt;CreateAndGetRequestBuilder createAndGet();\n...\n}\n\nThe response will be of type IdEntityResponse&lt;K, V&gt; which has a\ngetEntity() method:\n...\n    // \"greeting\" is defined in previous context\\\n    CreateIdEntityRequest\\&lt;Long, Greeting\\&gt; createIdEntityRequest =\n    builders.createAndGet().input(greeting).build();\n    Response\\&lt;IdEntityResponse\\&lt;Long, Greeting\\&gt;\\&gt; response =\n    restClient.sendRequest(createIdEntityRequest).getResponse();\n    ...\n    IdEntityResponse\\&lt;Long, Greeting\\&gt; idEntityResponse =\n    response.getEntity();\n    // The returned entity from server\\\n    Greeting resultEntity = idEntityResponse.getEntity();\n\n\nThe projection for returned entity is supported.\n...\n    // \"greeting\" is defined in previous context\\\n    CreateIdEntityRequest\\&lt;Long, Greeting\\&gt; createIdEntityRequest =\n    builders.createAndGet().fields(Greeting.fields().tone(),\n    Greeting.fields().id()).input(greeting).build();\n\n\nBATCH_CREATE Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated BATCH_CREATE request builder for a\nresource is named &lt;Resource&gt;BatchCreateBuilder. The generated builder\nsupports the full interface of the built-in BatchCreateRequestBuilder.\n\nIn Rest.li &gt;= 1.24.4, the generated BATCH_CREATE request builder for a\nresource is named &lt;Resource&gt;BatchCreateRequestBuilder. The generated\nbuilder extends the built-in BatchCreateIdRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\n    public &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nIf @ReturnEntity annotation is specified for BATCH_CREATE\nimplementation, an additional BatchCreateAndGet request builder will\nbe generated. Note that BatchCreate request builder will still be\ngenerated so that adding @ReturnEntity annotation is backward\ncompatible for a Java client.\npublic class &lt;Resource&gt;RequestBuilders\\\n{\n    ...\n    public &lt;Resource&gt;BatchCreateRequestBuilder batchCreate();\n    public &lt;Resource&gt;BatchCreateAndGetRequestBuilder batchCreateAndGet();\n    ...\n}\n\n\nThe response will be of type BatchCreateIdEntityResponse whose\nelements are CreateIdEntityStatus object containing the returned\nentity. Here is a code example.\n// \"greetings\" is defined in previous context\nBatchCreateIdEntityRequest&lt;Long, Greeting&gt; batchCreateIdEntityRequest = builders.batchCreateAndGet().inputs(greetings).build();\nResponse\\&lt;BatchCreateIdEntityResponse&lt;Long, Greeting&gt;&gt; response = restClient.sendRequest(batchCreateIdEntityRequest).getResponse();\nBatchCreateIdEntityResponse&lt;Long, Greeting&gt; entityResponses = response.getEntity();\nfor (CreateIdEntityStatus&lt;?, ?&gt; individualResponse : entityResponses.getElements())\n{\n    Greeting entity = (Greeting)individualResponse.getEntity();// The returned individual entity from server\n}\n\n\nThe projection for returned entities is supported.\n\n...\n// \"greetings\" is defined as a list of greeting in previous context\\\nBatchCreateIdEntityRequest&lt;Long, Greeting&gt; batchCreateIdEntityRequest = builders.batchCreateAndGet().fields(Greeting.fields().tone(),\nGreeting.fields().id()).inputs(greetings).build();\n\n\nPARTIAL_UPDATE Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated PARTIAL_UPDATE request builder for\na resource is named &lt;Resource&gt;PartialUpdateBuilder. Whereas in Rest.li &gt;= 1.24.4, it is called &lt;Resource&gt;PartialUpdateRequestBuilder. Both\nbuilders support the full interface of the built-in\nPartialUpdateRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\npublic &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nSee Creating partial\nupdates\nfor details on how to create a request for a partial update.\n\nIf the PARTIAL_UPDATE method is annotated with a @ReturnEntity annotation,\nan additional PartialUpdateAndGet request builder will be generated. Note that\nthe PartialUpdate request builder is still available so that adding\n@ReturnEntity is backward compatible for Java clients.\n\npublic class &lt;Resource&gt;RequestBuilders\n{\n...\n    public &lt;Resource&gt;PartialUpdateRequestBuilder partialUpdate();\n    public &lt;Resource&gt;PartialUpdateAndGetRequestBuilder partialUpdateAndGet();\n...\n}\n\n\nThe returned entity will be directly accessible from the response using getEntity():\n\n...\n// \"greeting\" is defined in previous context\nPartialUpdateEntityRequest&lt;Greeting&gt; partialUpdateEntityRequest = builders.partialUpdateAndGet()\n    .id(1L)\n    .input(greeting)\n    .build();\nResponse&lt;Greeting&gt; response = restClient.sendRequest(partialUpdateEntityRequest).getResponse();\n...\n// The returned entity from server\nGreeting resultEntity = response.getEntity();\n\n\nUsing projections on the returned entity is supported:\n\n...\n// \"greeting\" is defined in previous context\\\nPartialUpdateEntityRequest&lt;Greeting&gt; partialUpdateEntityRequest = builders.partialUpdateAndGet()\n        .fields(Greeting.fields().tone(), Greeting.fields().id())\n        .id(1L)\n        .input(greeting)\n        .build();\n\n\nBATCH_PARTIAL_UPDATE Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated BATCH_PARTIAL_UPDATE request\nbuilder for a resource is named &lt;Resource&gt;BatchPartialUpdateBuilder.\nWhereas in Rest.li &gt;= 1.24.4, it is\n&lt;Resource&gt;BatchPartialUpdateRequestBuilder. Both support the full\ninterface of the built-in BatchPartialUpdateRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\npublic &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nUPDATE Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated UPDATE request builder for a\nresource is named &lt;Resource&gt;UpdateBuilder. Whereas in Rest.li &gt;=\n1.24.4, it is named &lt;Resource&gt;UpdateRequestBuilder. Both builders\nsupport the full interface of the built-in UpdateRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\npublic &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nBATCH_UPDATE Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated BATCH_UPDATE request builder for a\nresource is named &lt;Resource&gt;BatchUpdateBuilder. Whereas in Rest.li &gt;=\n1.24.4, it is named &lt;Resource&gt;BatchUpdateRequestBuilder. Both builders\nsupport the full interface of the built-in BatchUpdateRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\npublic &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nDELETE Request Builder\n\nThe generated DELETE request builder for a resource is named\n&lt;Resource&gt;DeleteBuilder. The generated builder supports the full\ninterface of the built-in DeleteRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\npublic &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nBATCH_DELETE Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated BATCH_DELETE request builder for a\nresource is named &lt;Resource&gt;BatchDeleteBuilder. Whereas in Rest.li &gt;=\n1.24.4, the builder is called &lt;Resource&gt;BatchDeleteRequestBuilder.\nBoth builders support the full interface of the built-in\nBatchDeleteRequestBuilder.\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\npublic &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nACTION Request Builder\n\nIn Rest.li &lt; 1.24.4, the generated ACTION request builder for a\nresource is named &lt;Resource&gt;Do&lt;ActionName&gt;Builder. Whereas in Rest.li\n\n  = 1.24.4, it is &lt;Resource&gt;Do&lt;ActionName&gt;RequestBuilder. Both\nbuilders support the full interface of the built-in\nActionRequestBuilder.\n\n\nIf the resource class is a child resource, the generated builder will\ninclude a type-safe path-key binding method for each of the resource’s\nancestors (recursively following parent resources). Each binding method\nis declared as:\npublic &lt;BuilderType&gt; &lt;pathKeyName&gt;Key(&lt;KeyType&gt; key);\n\n\nThe generated builder will contain a method to set each of the action’s\nparameters. It Rest.li &lt; 1.24.4, it is of the form:\npublic &lt;BuilderType&gt; param&lt;ParamName&gt;(&lt;ParamType&gt; value);\n\n\nIn Rest.li &gt;= 1.24.4, it is of the form:\npublic &lt;BuilderType&gt; &lt;paramName&gt;Param(&lt;ParamType&gt; value);\n\n\nThe value must be non-null.\n\nIf the request is sent to an endpoint defined at Resource entity level, an entity Key is required by setting the id\npublic &lt;BuilderType&gt; id(&lt;IdType&gt; idvalue);\n\n\n\nCalling Sub-Resources\n\nTo call a subresource of the fortunes resource, for example:\n\nGET /fortunes/1/subresource/100\n\n\nThe parent keys can be specified by calling generated setters on the\nbuilder. In this case, the fortunesIdKey() method, for example:\n\nnew SubresourceBuilders().get().fortunesIdKey(1l).id(100l).build()\n\n\nParent path keys can also be set directly builder classes using the\nsetPathKey() method on the builders classes, for example:\n\n.setPathKey(\"dest\", \"dest\").setPathKey(\"src\", \"src\")\n\n\n\n\nBuilt-in Request and RequestBuilder classes\n\nThe built-in RequestBuilder classes provide generic support for\nconstructing Rest.li requests. This layer is independent of the IDL for\nspecific resources; therefore, the interface does not enforce that only\n“valid” requests are constructed.\n\nThere is one RequestBuilder subclass for each of the Rest.li resource\nmethods. Each RequestBuilder provides a .build() method that\nconstructs a Request object that can be used to invoke the\ncorresponding resource method. Each RequestBuilder constructs the\nRequest subclass that corresponds to the Rest.li method, for example,\nBatchGetRequestBuilder.build() returns a BatchGetRequest. The\nRequest subclasses allow framework code to introspect the original\ntype and parameters for a given request.\n\nEach RequestBuilder class supports a subset of the following methods, as\nappropriate for the corresponding resource method:\n\n\n  header(String key, String value) - sets a request header\n  addCookie(HttpCookie cookie) - adds a cookie\n  id(K id) - sets the entity key for the resource\n  ids(Collection&lt;K&gt; ids) - sets a list of entity keys\n  name(String name) - sets the name for a named resource method\n  setParam(String name, Object value) - sets a query param named\nname to value\n  addParam(String name, Object value) - adds value to the query\nparam named name\n  assocKey(String key, Object value) - sets an association key\nparameter\n  pathKey(String key, Object value) - sets a path key parameter\n(entity key of a parent resource)\n  paginate(int start, int count) - sets pagination parameters\n  fields(PathSpec... fieldPaths) - sets the fields projection mask\n  input(V entity) - sets the input payload for the request\n  inputs(Map&lt;K, V&gt; entities) - sets the input payloads for batch\nrequests\n  returnEntity(boolean value) - sets the $returnEntity query parameter\n\n\nThe following table summarizes the methods supported by each\nRequestBuilder type.\n\n\n  \n    \n      Request Builder\n      header\n      id\n      ids\n      name\n      setParam\n      addParam\n      assocKey\n      pathKey\n      paginate\n      fields\n      input\n      inputs\n      returnEntity\n    \n  \n  \n    \n      Action\n      -\n      -\n       \n      -\n      -\n      -\n       \n      -\n       \n       \n       \n       \n       \n    \n    \n      Find\n      -\n       \n       \n      -\n      -\n      -\n      -\n      -\n      -\n      -\n       \n       \n       \n    \n    \n      Get\n      -\n      -*\n       \n       \n      -\n      -\n       \n      -\n       \n      -\n       \n       \n       \n    \n    \n      Create\n      -\n       \n       \n       \n      -\n      -\n       \n      -\n       \n       \n      -\n       \n      -**\n    \n    \n      Delete\n      -\n      -*\n       \n       \n      -\n      -\n       \n      -\n       \n       \n       \n       \n       \n    \n    \n      PartialUpdate\n      -\n      -\n       \n       \n      -\n      -\n       \n      -\n       \n       \n      -\n       \n      -**\n    \n    \n      Update\n      -\n      -*\n       \n       \n      -\n      -\n       \n      -\n       \n       \n      -\n       \n       \n    \n    \n      BatchGet\n      -\n       \n      -\n       \n      -\n      -\n       \n      -\n       \n      -\n       \n       \n       \n    \n    \n      BatchCreate\n      -\n       \n       \n       \n      -\n      -\n       \n      -\n       \n       \n       \n      -\n      -**\n    \n    \n      BatchDelete\n      -\n       \n      -\n       \n      -\n      -\n       \n      -\n       \n       \n       \n       \n       \n    \n    \n      BatchPartialUpdate\n      -\n       \n       \n       \n      -\n      -\n       \n      -\n       \n       \n       \n      -\n      -**\n    \n    \n      BatchUpdate\n      -\n       \n       \n       \n      -\n      -\n       \n      -\n       \n       \n       \n      -\n       \n    \n    \n      BatchFinder\n      -\n       \n       \n      -\n      -\n      -\n      -\n      -\n      -\n      -\n       \n       \n       \n    \n  \n\n\n* It is not supported, if the method is defined on a simple resource.\n\n** Supported if the resource method is annotated with @ReturnEntity. See more about this feature.\n\nRefer to the JavaDocs for specific details of RequestBuilder and Request\ninterfaces.\n\n\n\n\n\nRestspec IDL\n\nRest.li uses a custom format called REST Specification (Restspec) as its\ninterface description language (IDL). The Restspec provides a succinct\ndescription of the URI paths, HTTP methods, query parameters, and JSON\nformat. Together, these form the interface contract between the server\nand the client.\n\nRestspec files are JSON format and use the file suffix *.restspec.json.\n\nAt a high level, the restspec contains the following information:\n\n\n  name of the resource\n  path to the resource\n  schema type (value type) of the resource\n  resource pattern (collection / simple / association / actionsSet)\n  name and type of the resource key(s)\n  list of supported CRUD methods (CREATE, GET, UPDATE,\nPARTIAL_UPDATE, DELETE, and corresponding batch methods)\n  description of each FINDER, including\n    \n      name\n      parameter names, types, and optionality\n      response metadata type (if applicable)\n    \n  \n  description of each BATCH_FINDER, including\n    \n      name\n      parameter names, types, and optionality\n      batch parameter name\n      response metadata type (if applicable)\n    \n  \n  description of each ACTION, including\n    \n      name\n      parameter names, types, and optionality\n      response type\n      exception types\n    \n  \n  a description of each subresource, containing the information\ndescribed above\n\n\nAdditional details on the Restspec format may be found in the\ndesign documents.\nThe Restspec format is formally described by the data schema schema files in\n“com.linkedin.restli.restspec.* “ distributed in the restli-common module.\n\n\n\nIDL Generator Tool\n\nThe IDL generator is used to create the language-independent interface\ndescription (IDL) from Rest.li resource implementations (annotated Java\ncode).\n\nThe IDL generator is available as part of the restli-tools JAR, as the\ncom.linkedin.restli.tools.idlgen.RestLiResourceModelExporterCmdLineApp\nclass.\n\nFor details on how to use the IDL Generator, see Gradle build integration.\n\n\n\n\n\n\nRestClient\n\nRestClient encapsulates the communication with the remote resource.\nRestClient accepts a Request object as input and provides a\nResponse object as output. The Request objects should usually be\nbuilt using the generated type-safe client builders. Since the\nRestClient interface is fundamentally asynchronous, the Response\nmust be obtained through either a ResponseFuture or a Callback (both\noptions are supported).\n\nRestClient is a simple wrapper around an R2 transport client. For\nstandalone / test use cases, the transport client can be obtained\ndirectly from R2, for example, using the HttpClientFactory. If you\nwish to use D2, the Client used by the RestClient must be a D2\nclient.\n\nThe RestClient constructor also requires a URI prefix that is\nprepended to the URIs generated by the Request Builders. When using D2,\na prefix of \"d2://\" should be provided that results in URIs using the\nD2 scheme.\n\nResponseFuture\n\nThe RestClient future-based interface returns ResponseFuture, which\nimplements the standard Future interface and extends it with a\ngetResponse() method. The advantage of getResponse() is that it is\naware of Rest.li exception semantics, throwing\nRemoteInvocationException instead of ExecutionException.\n\nMaking requests using the RestClient and generated RequestBuilders\n\nThe standard pattern for making requests using the RestClient is as\nfollows:\n\n\n  Build the request using the generated request builders\n  Use the RestClient#sendRequest method to send the request and get\nback a ResponseFuture\n  Call ResponseFuture#getResponse to get the Response that the\nserver returned. Note that this call blocks until the server\nresponds or there is an error!\n\n\nHere is a more concrete example, where a client is making a GET request\nto the /greetings resource -\n\n// First we build the Request. builders is either a GreetingsBuilder or GreetingsRequestBuilder\nRequest&lt;Greeting&gt; getRequest = builders.get().id(id).build();\n\n// Send the Request and get back a ResponseFuture representing the response. This call is non-blocking.\nResponseFuture&lt;Greeting&gt; responseFuture = restClient.sendRequest(getRequest);\n\n// Like the standard Java Future semantics, calling getResponse() here IS blocking!\nResponse&lt;Greeting&gt; getResponse = responseFuture.getResponse();\n\n// Get the entity from the Response\nGreeting responseGreeting = getResponse.getEntity();\n\n\nLook at the com.linkedin.restli.client.Response interface to see what\nother methods are available for use.\n\nRequest API changes in Rest.li &gt;= 1.24.4\n\nThere are two major changes:\n\n\n  CreateIdRequestBuilder, which is the super class for all CREATE\nrequest builders, now returns a CreateIdRequest&lt;K, V&gt; when the\nbuild() method is called.\n  BatchCreateIdRequestBuilder, which is the super class for all\nBATCH_CREATE request builders, now returns a\nBatchCreateIdRequest&lt;K, V&gt; when the build() method is called.\n\n\nResponse API Changes in Rest.li &gt;= 1.24.4\n\nStarting with Rest.li 1.24.4, we have introduced a few changes to the\nResponse API.\n\nResponse from a CREATE and BATCH_CREATE Request\n\nAs mentioned in the section above, calling build() on a\nCreateIdRequestBuilder gives us a CreateIdRequest&lt;K, V&gt;.\nWhen this is sent using a RestClient we get back (after calling\nsendRequest(...).getResponse().getEntity()) an IdResponse&lt;K&gt; that\ngives us a single, strongly-typed key.\n\nSimilarly, when a RestClient is used to send out a\nBatchCreateIdRequest&lt;K, V&gt; we get back a BatchCreateIdResponse&lt;K&gt;,\nwhich contains a List of strongly-typed keys.\n\nResponse from a BATCH_GET Request\n\nWhen a BatchGetEntityRequest is sent using a RestClient we get back\n(after calling sendRequest(...).getResponse().getEntity()) a\nBatchKVResponse&lt;K,EntityResponse&lt;V&gt;&gt; where K is the key type and V\nis the value (which extends RecordTemplate) for the resource we are\ncalling.\n\nEntityResponse is a RecordTemplate with three fields:\n\n\n  entity provides an entity record if the server resource finds a\ncorresponding value for the key;\n  status provides an optional status code;\n  error provides the error details from the server resource\n(generally entity and error are mutually exclusive as null,\nbut it is ultimately up to the server resource).\n\n\nNote that since EntityResponse contains an error field, the\nMap&lt;K, V&gt; returned by BatchEntityResponse#getResults() contains both\nsuccessful as well as failed entries. BatchEntityResponse#getErrors()\nwill only return failed entries.\n\nResponse from a BATCH_UPDATE, BATCH_PARTIAL_UPDATE, and BATCH_DELETE Request\n\nThe response type of the BatchUpdate series methods are not changed.\nHowever, similar to EntityResponse, we added a new error field to\nUpdateStatus (the value type of the BatchUpdate series methods).\nFurthermore, BatchKVResponse&lt;K, UpdateStatus&gt;#getResults() will\nreturns both successful as well as failed entries. getErrors() will\nonly return failed entries.\n\nError Semantics\n\nThe following diagram illustrates the request/response flow for a\nclient/server interaction. The call may fail at any point during this\nflow, as described below.\n\n\nRest.li Request Flow\n\n\nThe following list describes the failures scenarios as observed by a\nclient calling ResponseFuture.getResponse()\n\nFailure Scenarios\n\n\n  Client Framework (outbound)\n    \n      ServiceUnavailableException - if D2 cannot locate a node for\nthe requested service URI\n      RemoteInvocationException - if R2 cannot connect to the remote\nendpoint or send the request\n    \n  \n  Network Transport (outbound)\n    \n      TimeoutException - if a network failure prevents the request\nfrom reaching the server\n    \n  \n  Server Framework (inbound)\n    \n      RestLiResponseException - if an error occurs within the\nframework, resulting in a non-200 response\n      TimeoutException - if an error prevents the server from\nsending a response\n    \n  \n  Server Application\n    \n      RestLiResponseException - if the application throws an\nexception the server framework will convert it into a non-200\nresponse\n      TimeoutException - if an application error prevents the server\nfrom sending a response in a timely manner\n    \n  \n  Server Framework (outbound)\n    \n      RestLiResponseException - if an error occurs within the\nframework, resulting in a non-200 response\n      TimeoutException - if an error prevents the server from\nsending a response\n    \n  \n  Network Transport (inbound)\n    \n      TimeoutException - if a network failure prevents the response\nfrom reaching the client\n    \n  \n  Client Framework (inbound)\n    \n      RestLiDecodingException - if the client framework cannot\ndecode the response document\n      RemoteInvocationException - if an error occurs within the\nclient framework while processing the response.\n    \n  \n\n\n\n\nRequest Options\n\nEach request sent to a Rest.li server can be configured with custom\noptions by using an instance of RestliRequestOptions.\nRestliRequestOptionsBuilder is required to construct an instance of\nRestliRequestOptions. Once constructed, an instance of\nRestliRequestOptions can then be passed to Rest.li generated type-safe\nrequest builders. Subsequently, RestClient will construct a\nRestRequest based on these custom options to send to the Rest.li\nserver. Currently we support specifying the following custom options per\nRequest:\n\nProtocolVersionOption\n\nWhen sending a Request, the caller can specify what protocol version\noption is to be used. The available ProtocolVersionOption(s) are:\n\nFORCE_USE_NEXT\n\nUse the next version of the Rest.li protocol to encode requests,\nregardless of the version running on the server. The next version of the\nRest.li protocol is the version currently under development. This option\nshould typically NOT be used for production services.\nCAUTION: this can cause requests to fail if the server does not\nunderstand the next version of the protocol.\n“Next version” is defined as\ncom.linkedin.restli.internal.common.AllProtocolVersions.NEXT_PROTOCOL_VERSION.\n\nFORCE_USE_LATEST\n\nUse the latest version of the Rest.li protocol to encode requests,\nregardless of the version running on the server.\nCAUTION: this can cause requests to fail if the server does not\nunderstand the latest version of the protocol. “Latest version” is defined as\ncom.linkedin.restli.internal.common.AllProtocolVersions.LATEST_PROTOCOL_VERSION.\n\nUSE_LATEST_IF_AVAILABLE\n\nUse the latest version of the Rest.li protocol if the server supports\nit. If the server version is less than the baseline Rest.li protocol\nversion then fail the request. If the server version is greater than the\nnext Rest.li protocol version then fail the request. If the server is\nbetween the baseline and the latest version then use the server version\nto encode the request. If the server version is greater than or equal to\nthe latest protocol version then use that to encode the request.\n\n\n  “Baseline version” is defined as\ncom.linkedin.restli.internal.common.AllProtocolVersions.BASELINE_PROTOCOL_VERSION.\n  “Latest version” is defined as\ncom.linkedin.restli.internal.common.AllProtocolVersions.LATEST_PROTOCOL_VERSION.\n  “Next version” is defined as\ncom.linkedin.restli.internal.common.AllProtocolVersions.NEXT_PROTOCOL_VERSION.\n\n\nCAUTION: Please be very careful setting the non-default\nFORCE_USE_NEXT or FORCE_USE_LATEST options as the protocol\nversion option in RestLiRequestOptions, since they may cause requests\nto fail if the server does not understand the desired protocol request.\nThis form of configuration is normally used in migration cases.\n\nCompressionOption\n\nWhen sending a Request, the caller can force compression on or off for\neach request.\n\nFORCE_ON\n\nCompress the request.\n\nFORCE_OFF\n\nDo not compress the request.\n\nIf null is specified, Rest.li ClientCompressionFilter will determine\nwhether we need to do client side compression based on request entity\nlength.\n\nContentType\n\nWhen sending a Request, the caller can also specify what content type is\nto be used. The specified value will be set to the HTTP header\n“Content-Type” for the request.\n\nJSON\n\nThis will set “Content-Type” header value as “application/json”.\n\nPSON\n\nThis will set “Content-Type” header value as “application/x-pson”\n\nPROTOBUF2\n\nThis will set “Content-Type” header value as “application/x-protobuf2”\n\nNOTE: Besides RestliRequestOption, the caller can also specify the\nContentType through the RestClient constructor by passing the\ncontentType parameter (as shown below), which will apply to all requests\nsent through that client instance.\n\npublic RestClient(Client client, String uriPrefix, ContentType contentType, List&lt;AcceptType&gt; acceptTypes)\n\n\nHowever, In cases where the caller has configured content type from \nmultiple places, RestClient will resolve request content type based \non the following precedence order:\n\n\n  Request header.\n  RestliRequestOptions.\n  RestClient configuration.\n\n\nIf null is specified for content type from these 3 sources,\nRestClient will use JSON as default.\n\nAcceptType\n\nWhen sending a Request, the caller can also specify what media types it\ncan accept. The specified value will be set to the HTTP header “Accept”\nfor the request. If more than one AcceptType is specified, we will\ngenerate an Accept header by appending each media type by a “q”\nparameter for indicating a relative quality factor. For example:\n\nAccept: application/*; q=0.2, application/json\n\n\nQuality factors allow the user or user agent to indicate the relative\ndegree of preference for that media type, using the scale from 0 to 1.\nThe default value is q=1. In our case, the quality factor generated is\nbased on the order of each accept type we specified in the list. See\nhttp://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html for details.\n\nJSON\n\nThis will accept media type of “application/json”.\n\nPSON\n\nThis will accept media type of “application/x-pson”.\n\nPROTOBUF2\n\nThis will accept media type of “application/x-protobuf2”\n\nANY\n\nThis will accept any media type.\n\nNOTE: Besides RestliRequestOption, the caller can also specify\nAcceptType through the RestClient constructor by passing the\nacceptTypes parameter (as shown below), which will apply to all requests\nsent through that client instance.\n\npublic RestClient(Client client, String uriPrefix, List&lt;AcceptType&gt; acceptTypes)\npublic RestClient(Client client, String uriPrefix, ContentType contentType, List&lt;AcceptType&gt; acceptTypes)\n\nHowever, In cases where the caller has configured accept types from multiple places,\nRestClient will resolve request accept type based on the following precedence order:\n\n\n  Request header.\n  RestliRequestOptions.\n  RestClient configuration.\n\n\nIf null is specified for the accept type from these 3 sources,\nRestClient will not set the HTTP “Accept” header. If no accept header\nfield is present, then it is assumed by the Rest.li server that the\nclient accepts all media types based on the HTTP Spec (RFC 2616).\n\nIf RestliRequestOptions is not set, or is set to null, the request builders will use\nRestliRequestOptions.DEFAULT_OPTIONS(ProtocolVersionOption.USE_LATEST_IF_AVAILABLE, null /*compression*/, null /*content type*/, null /*accept type*/)\nto generate the Request.\n\nParSeq Integrated Rest Client\n\nThe ParSeqRestClient wrapper facilitates usage with ParSeq by\nproviding methods that return a Promise or a Task. For example,\nusers can create multiple requests and use ParSeq to send them in\nparallel. This feature is independent of the asynchronous resources; in\nparticular, the server resource does not have to be asynchronous.\n\nParSeqRestClient client = new ParSeqRestClient(plain rest client);\n// send some requests in parallel\nTask&lt;Response&lt;?&gt;&gt; task1 = client.createTask(request1);\nTask&lt;Response&lt;?&gt;&gt; task2 = client.createTask(request2);\nTask&lt;Response&lt;?&gt;&gt; combineResults = ...;\n// after we get our parallel requests, combine them\nengine.run(Tasks.seq(Tasks.par(task1, task2), combineResults))\n\nUsers of createTask are required to instantiate their own ParSeq\nengine and start the task themselves.\n\n\n\n\n\nClient Code Generator Tool\n\nAs described above, the Rest.li client framework includes a\ncode-generation tool that creates type-safe Request Builder classes\nbased on resource IDL files.\n\nThe code generator is available as part of the restli-tools JAR, as\ncom.linkedin.restli.tools.clientgen.RestRequestBuilderGenerator. The\ngenerator is invoked by providing an output directory and a list of\ninput IDL files as command-line arguments.\n\nIn addition, the generator recognizes the following system properties:\n\n\n  generator.rest.generate.datatemplates - boolean property\nindicating whether the generator should generate Java RecordTemplate\nclasses for the data schemas referenced by the IDL file.\n  generator.default.package - the default package name for generated\nclasses\n  generator.resolver.path - a colon-separated list of filesystem\npaths to search when resolving references to named schemas. See\n“Data Template Generator” for more details.\n\n\nThe Rest.li client code generator is integrated as part of the pegasus\ngradle plugin. For details, see Gradle build integration.\n\n\n\nRest.li-extras\n\nRest.li can be used with the D2 layer for dynamic discovery and\nclient-side load balancing. The use of D2 is normally transparent at the\nRest.li layer. However, for applications wishing to make more\nsophisticated use of Rest.li and D2, the restli-extras module is\nprovided.\n\nScatter / Gather\n\nThe main feature supported in restli-extras is the ability to make\nparallel “scatter/gather” requests across all the nodes in a cluster.\nCurrently, scatter/gather functionality is only supported for BATCH_GET\nmethods.\n\nScatter/gather makes use of D2’s support for consistent hashing, to\nensure that a given key is routed to the same server node when possible.\nThe ScatterGatherBuilder interface can be used to partition a single\nlarge BatchGetRequest into N BatchGetRequests, one for each node\nin the cluster. The key partitioning is done according to the D2\nconsistent hashing policy, using a KeyMapper object obtained from the\nD2 Facilities interface. Batch updates and deletes are also supported.\n",
      tags: null,
      id: 33
    });
    
    

  
    index.add({
      title: "Rest.li server user guide",
      category: null,
      content: "Rest.li Server User Guide\n\nContents\n\n\n  Runtimes\n  R2 Filter Configuration\n  Defining Data Models\n  Writing Resources\n  Documenting Resources\n  Resource Annotations\n  Asynchronous Resources\n  Sub-Resources\n  Resource Methods\n    \n      Get\n      Batch Get\n      Get All\n      Finder\n      Batch Finder\n      Create\n      Batch Create\n      Update\n      Batch Update\n      Partial Update\n      Batch Partial Update\n      Delete\n      Batch Delete\n      Action\n    \n  \n  ResourceContext\n  Resource Templates\n  Free-form Resources\n  Returning Errors\n  Field Projection\n  Collection Pagination\n  Dependency Injection\n  Online Documentation\n\n\nThis document describes Rest.li support for implementing servers.\n\nRuntimes\n\nRest.li supports the following runtimes:\n\n\n  Servlet containers (for example, Jetty)\n  Netty\n\n\nR2 Filter Configuration\n\nRest.li servers can be configured with different R2 filters, according\nto your use case. How the filters are configured depends on which\ndependency injection framework (if any) you are using. For example, see Compression to understand how to configure a server for\ncompression. Another example is to add a\nSimpleLoggingFilter with Spring, which requires you to do\nthe following (full file\nhere):\n\n&lt;!-- Example of how to add filters; here we'll enable logging and snappy compression support --&gt;\n&lt;bean id=\"loggingFilter\" class=\"com.linkedin.r2.filter.logging.SimpleLoggingFilter\" /&gt;\n\n\nOther R2 filters\ncan also be configured in a similar way.\n\n\n\nDefining Data Models\n\nThe first step in building a Rest.li application is to define your data\nschema using Pegasus Data Schemas. The\nPegasus Data Schema format uses a simple Avro-like syntax to define your\ndata model in a language-independent way. Rest.li provides code\ngenerators to create Java classes that implement your data model.\n\n\n\nWriting Resources\n\nAfter you have defined your data models, the principle programming task\nwhen implementing a Rest.li server is to create resource classes. In\nRest.li, resource classes define the RESTful endpoints your server\nprovides. You create a resource class by adding a class level annotation\nand by implementing or extending a Rest.li interface or base class\ncorresponding to the annotation. The annotations help describe the\nmapping from your Java code to the REST interface protocol. When\npossible, the framework uses conventions to help minimize the\nannotations you need to write.\n\nSteps to define a resource class:\n\n\n  The class must have the default constructor. The default constructor\nwill be used by Rest.li to instantiate the resource class for each\nrequest execution.\n  The class must be annotated with one of the Resource Annotations.\n  If required by the annotation, the class must implement the\nnecessary Resource interface or extend one of the convenience base\nclasses that implements the interface.\n  To expose methods on the resource, each method must either:\n    \n      Override a standard method from the Resource interface\n      Include the necessary method-level annotation as described in\nthe Resource Methods section below\n    \n  \n  For each exposed method, each parameter must either:\n    \n      Be part of the standard signature, for overridden methods\n      Be annotated with one of the parameter-level annotations\ndescribed for the Resource Method.\n    \n  \n  All documentation is written in the resource source file using\njavadoc (or scaladoc, see below for details).\n\n\nHere is a simple example of a Resource class. It extends a convenience\nbase class, uses an annotation to define a REST end-point (“fortunes”),\nand provides a GET endpoint by overriding the standard signature of the\nget() method of the base class:\n\n/**\n * A collection of fortunes, keyed by random number.\n */\n@RestLiCollection(name = \"fortunes\", namespace = \"com.example.fortune\")\npublic class FortunesResource extends CollectionResourceTemplate&lt;Long, Fortune&gt;\n{\n  /**\n   * Gets a fortune for a random number.\n   */\n  @Override\n  public Fortune get(Long key)\n  {\n    // retrieve data and return a Fortune object ...\n  }\n}\n\n\nThis interface implements an HTTP GET:\n\n&gt; GET /fortunes/1\n...\n&lt; { \"fortune\": \"Your lucky color is purple\" }\n\n\nNote that Rest.li does not automatically use the names of your Java\nidentifiers. Class names, method names, and parameter names have no\ndirect bearing on the interface your resource exposes through\nannotations.\n\nThe above example supports the GET operation by overriding the\nCollectionResourceTemplate, and you can also choose to support other\noperations by overriding other methods. However, you can also define any\nmethod of your class as handling operations by using Resource\nAnnotations, described in detail in the next section.\n\n\n\nDocumenting Resources\n\nRest.li resources are documented in the resource source files using\njavadoc. When writing resources, developers simply add any documentation\nas javadoc to their java resource classes, methods, and method params.\nIt is recommended that developers follow the javadoc style\nguidelines\nfor all formatting so that their documentation is displayed correctly.\n\nRest.li will automatically extract this javadoc and include it in all\ngenerated “interface definitions” (.restspec.json files) and generated\nclient bindings. This approach allows REST API clients and tools to\neasily gain access to the documentation. For example, Rest.li API\nHub is an opensource web\nUI that displays REST API documentation, including all javadoc, for\nRest.li APIs.\n\n\n\nResource Annotations\n\nResource annotations are used to mark and register a class as providing\nas Rest.li resource. One of a number of annotations may be used,\ndepending on the Interface Pattern\nthe resource is intended to implement. Briefly, here are the options:\n\n\n\n\n  \n    \n      Resource Type\n      Annotation\n      Interface or Base Class\n    \n  \n  \n    \n      Collection\n      @RestLiCollection\n      For simple keys, implement CollectionResource or extend CollectionResourceTemplate. For complex key implement ComplexKeyResource, extend ComplexKeyResourceTemplate, or implement KeyValueResource for use cases requiring extensive customization\n    \n    \n      Simple\n      @RestLiSimpleResource\n      Implement SimpleResource, extend SimpleResourceTemplate or implement SingleObjectResource for use cases requiring extensive customization\n    \n    \n      Association\n      @RestLiAssociation\n      Implement AssociationResource, extend AssociationResourceTemplate, or implement KeyValueResource for use cases requiring extensive customization\n    \n    \n      Actions\n      @RestLiActions\n      N/A\n    \n  \n\n\n@RestLiCollection\n\nThe @RestLiCollection annotation is applied to classes to mark them as\nproviding a Rest.li collection resource. Collection resources model a\ncollection of entities, where each entity is referenced by a key. See\nCollection Resource Pattern for more details.\n\nThe supported annotation parameters are:\n\n\n  name - required, defines the name of the resource.\n  namespace - optional, defines the namespace for the resource.\nDefault is empty (root namespace). The namespace of the resource\nappears in the IDL, and is used as the package name for the\ngenerated client builders.\n  keyName - optional, defines the key name for the resource. Default\nis “&lt;ResourceName&gt;Id”.\n  parent - optional, defines the parent resource for this resource.\nDefault is root.\n\n\nClasses annotated with @RestLiCollection must implement the\nCollectionResource interface. The CollectionResource interface\nrequires two generic type parameters:\n\n\n  K - the key type for the resource.\n  V - the value type for the resource (also known as, the entity\ntype).\n\n\nThe key type for a collection resource must be one of:\n\n\n  String\n  Boolean\n  Integer\n  Long\n  A Pegasus Enum (any enum defined in a .pdl schema)\n  Custom Type (see below for details)\n  Complex Key (A pegasus record, any subclass of RecordTemplate\ngenerated from a .pdl schema)\n\n\nThe value type for a collection resource must be a pegasus record, any\nsubclass of RecordTemplate generated from a .pdl schema.\n\nFor convenience, collection resources may extend\nCollectionResourceTemplate rather than directly implementing the\nCollectionResource interface.\n\nExample:\n\n@RestLiCollection(name = \"fortunes\", namespace = \"com.example.fortune\", keyName = \"fortuneId\")\npublic class FortunesResource extends CollectionResourceTemplate&lt;Long, Fortune&gt;\n{\n  ...\n}\n\n\n\n\nAsynchronous Resources\n\nRest.li allows resources to return results asynchronously through a\nParSeq Task, or\nCallback. For example, a getter can be declared in either of the\nfollowing ways:\n@RestMethod.Get\npublic Task&lt;Greeting&gt; get(Long key)\n{\n  // set up some ParSeq tasks and return the final Task\n  return Tasks.seq(Tasks.par(...), ...);\n}\n\n\n@RestMethod.Get\npublic void get(Long key, @CallbackParam Callback&lt;Greeting&gt; callback)\n{\n  // use the callback asynchronously\n}\n\n\nThese method signatures can be mixed arbitrarily with the synchronous\nsignatures, including in the same resource class. For instance, simple\nmethods can be implemented synchronously and slow methods can be\nimplemented asynchronously. However, multiple implementations of the\nsame REST method with different signatures may not be provided.\n\nYou can also use the asynchronous resource templates in order to\nimplement asynchronous Rest.li resources. The templates are:\n\n\n  AssociationResourceAsyncTemplate\n  AssociationResourceTaskTemplate\n  CollectionResourceAsyncTemplate\n  CollectionResourceTaskTemplate\n  ComplexKeyResourceAsyncTemplate\n  ComplexKeyResourceTaskTemplate\n  SimpleResourceAsyncTemplate\n  SimpleResourceTaskTemplate\n\n\nThe Rest.li server will automatically start any Task that is returned\nby a Task based method by running it through a ParSeq engine. This ParSeq engine is launched while the Rest.Li server is launched (for example, NettyStandaloneLauncher), so the user does not run these task using another engine explicitly, instead, Rest.Li framework will the run the method’s returning Task using the ParSeq engine when the method is invoked. (For details check RestLiMethodInvoker.java). Also refer to this document page regarding the best practice of writing asynchronous client and service logic. Callback-based methods do not receive special treatment.\n\nSub-Resources\n\nSub-resources may be defined by setting the parent field on\n@RestLiCollection to the class of the parent resource of\nthe sub-resource.\n\nFor example, a sub-resource of the fortunes resource would have a URI\npath of the form:\n\n/fortunes/{fortuneId}/subresource\n\n\nParent resource keys can be accessed by sub-resources, as shown in the\nfollowing example:\n\n@RestLiCollection(name = \"subresource\", namespace = \"com.example.fortune\", parent = FortunesResource.class)\npublic class SubResource extends CollectionResourceTemplate&lt;Long, SubResourceEntity&gt;\n{\n  @RestMethod.Get\n  public Greeting get(Long key, @Keys PathKeys keys) {\n      Long parentId = keys.getAsLong(\"fortuneId\");\n      ...\n  }\n...\n}\n\n\nAlternatively, if not using free form methods, the path key can\nretrieved from the resource context. This approach may be deprecated in\nfuture versions in favor of @Keys.\n\npublic SubResourceEntity get(Long subresourceKey)\n{\n  Long parentId = getContext().getPathKeys().getAsLong(\"fortuneId\");\n  ...\n}\n\n\nFor details on how to make requests to sub-resources from a client, see Calling Sub-resources\n\n@RestLiCollection with Complex Key\n\nClasses implementing ComplexKeyResource can use a record type as key.\nThis allows for arbitrary complex hierarchical structures to be used to\nkey a collection resource, unlike CollectionResources, which only\nsupport primitive type keys (or typerefs to primitive types).\nComplexKeyResourceTemplate is a convenient base class to extend when\nimplementing a ComplexKeyResource.\n\nThe full interface is:\npublic interface ComplexKeyResource&lt;K extends RecordTemplate, P extends\nRecordTemplate, V extends RecordTemplate&gt; ...\n\n\nA complex key consists of a Key and Parameter part. The Key should\nuniquely identify the entities of the collection while the parameters\nmay optionally be added to allow additional information that is not used\nto lookup an entity, such as a version tag for concurrency control.\n\nSince the parameters are often not needed, an EmptyRecord may be used\nin the generic signature of a ComplexKeyResource to\nindicate that no “Parameters” are used to key the collection.\n\nExample:\n\n@RestLiCollection(name = \"widgets\", namespace = \"com.example.widgets\")\npublic class WidgetResource extends ComplexKeyResourceTemplate&lt;WidgetKey, EmptyRecord, Widget&gt;\n{\n  public Widget get(ComplexResourceKey&lt;WidgetKey, EmptyRecord&gt; ck)\n  {\n    WidgetKey key = ck.getKey();\n    int number = key.getNumber();\n    String make = key.getThing().getMake();\n    String model = key.getThing().getModel();\n    return lookupWidget(number, make, model);\n  }\n}\n\n\nTo use EmptyRecord, restli-common must be in\nthe dataModel dependencies for the api project where client\nbindings are generated, as shown in the following example:\n\napi/build.gradle:\n\ndependencies {\n  // ...\n  dataModel spec.product.pegasus.restliCommon\n}\n\n\nwhere WidgetKey is defined by the schema:\n\nnamespace com.example.widget\n\nrecord WidgetKey {\n  number: string\n\n  thing: record Thing {\n    make: string\n    model: string\n  }\n}\n\n\nExample request:\n\ncurl \"http://&lt;hostname:port&gt;/widgets/number=1&amp;thing.make=adruino&amp;thing.model=uno\"\n\n\nIf params are added, they are represented in the URL under the\n“$params” prefix like this:\n\ncurl \"http://&lt;hostname:port&gt;/widgets/number=1&amp;thing.make=adruino&amp;thing.model=uno&amp;$params.version=1\"\n\n\nThe implementation of complex key collection is identical to the regular\nRestLiCollection with the exception that it extends\nComplexKeyResourceTemplate (or directly implements\nComplexKeyResource) and takes three ﻿type parameters instead of two:\nkey type, key parameter type, and value type — each extending\n@RecordTemplate.\n\nFor details on how a complex key is represented in a request URL, see\nRest.li Protocol: Complex Types\n\n@RestLiSimpleResource\n\nThe @RestLiSimpleResource annotation is applied to classes to mark\nthem as providing a Rest.li simple resource. Simple resources model an\nentity which is a singleton in a particular scope. See the description\nof the Simple Resource Pattern for more details.\n\nThe supported annotation parameters are:\n\n\n  name - required, defines the name of the resource.\n  namespace - optional, defines the namespace for the resource.\nDefault is empty (root namespace). The namespace of the resource\nappears in the IDL, and is used as the package name for the\ngenerated client builders.\n  parent - optional, defines the parent resource for this resource.\nDefault is root.\n\n\nClasses annotated with @RestLiSimpleResource must implement the\nSimpleResource interface. The SimpleResource interface requires a\ngeneric type parameter V, which is the value type for the resource\n(also known as, the entity type). The value type for a simple resource\nmust be a pegasus record, any subclass of RecordTemplate generated\nfrom a .pdl schema.\n\nFor convenience, simple resources may extend SimpleResourceTemplate\nrather than directly implementing the SimpleResource interface.\n\nExamples:\n\n@RestLiSimpleResource(name = \"todaysPromotedProduct\", namespace = \"com.example.product\")\npublic class TodaysPromotedProductResource extends SimpleResourceTemplate&lt;Product&gt;\n{\n    ...\n}\n\n\n@RestLiAssociation\n\nThe @RestLiAssociation annotation is applied to classes to mark them\nas providing a Rest.li association resource. Association resources model\na collection of relationships between entities. Each relationship is\nreferenced by the keys of the entities it relates and may define\nattributes on the relation itself. See\nAssociation Resource Pattern\n\nFor Example:\n\n@RestLiAssociation(name = \"memberships\", namespace = \"com.example\",\n  assocKeys = {\n    @Key(name = \"memberId\", type = Long.class), \n    @Key(name = \"groupId\", type = Long.class)\n  }\n)\npublic class MembershipsAssociation extends AssociationResourceTemplate&lt;Membership&gt;\n{\n  @Override\n  public Membership get(CompoundKey key)\n  {\n    return lookup(key.getPartAsLong(\"memberId\", key.getPartAsLong(\"groupId\"));\n  }\n}\n\n\ncurl http://&lt;hostname:port&gt;/memberships/memberId=1&amp;groupId=10\n\n\nThe supported annotation parameters are:\n\n\n  name - required, defines the name of the resource.\n  namespace - optional, defines the namespace for the resource.\nDefault is empty (root namespace). The namespace of the resource\nappears in the IDL, and is used as the package name for the\ngenerated client builders.\n  parent - optional, defines the parent resource for this resource.\nDefault is root.\n  assocKeys - required, defines the list of keys for the association\nresource. Each key must declare its name and type.\n\n\nClasses annotated with @RestLiAssociation must implement the\nAssociationResource interface. The AssociationResource interface\nrequires a single generic type parameter:\n\n\n  V, which is the value type for the resource, a.k.a., the entity\ntype.\n\n\nThe value type for an association resource must be a subclass of\nRecordTemplate generated from a .pdl schema.\n\nNote that for association resources, they key type is always\nCompoundKey, with key parts as defined in the assocKeys parameter of\nthe class’ annotation.\n\nFor convenience, Association resources may extend\nAssociationResourceTemplate rather than directly implementing the\nAssociationResource interface.\n\n@RestLiActions\n\nThe @RestLiActions annotation is applied to classes to mark them as\nproviding a Rest.li action set resource. Action set resources do not\nmodel any resource pattern. They simply group together a set of custom\nactions.\n\nFor example:\n\n@RestLiActions(name = \"simpleActions\", namespace = \"com.example\")\npublic class SimpleActionsResource {\n\n  @Action(name=\"echo\")\n  public String echo(@ActionParam(\"input\") String input)\n  {\n    return input;\n  }\n}\n\n\nThe supported annotation parameters are:\n\n\n  name - required, defines the name of the resource.\n  namespace - optional, defines the namespace for the resource.\nDefault is empty (root namespace).\n\n\nAction set resources do not have a key or value type, and do not need to\nimplement any framework interfaces.\n\n\n\nResource Methods\n\nResource methods are operations a resource can perform. Rest.li defines\na standard set of resource methods, each with its own interface pattern\nand intended semantics.\n\nThe set of possible resource methods is constrained by the resource\ntype, as described in the table below:\n\n\n  \n    \n      Resource Type\n      Collection\n      Simple\n      Association\n      Action Set\n    \n  \n  \n    \n      GET\n      x\n      x\n      x\n       \n    \n  \n  \n    \n      BATCH_GET / GET_ALL\n      x\n       \n      x\n       \n    \n  \n  \n    \n      FINDER\n      x\n       \n      x\n       \n    \n  \n  \n    \n      BATCH_FINDER\n      x\n       \n      x\n       \n    \n  \n  \n    \n      CREATE / BATCH_CREATE\n      x\n       \n       \n       \n    \n  \n  \n    \n      UPDATE / PARTIAL_UPDATE\n      x\n      x\n      x\n       \n    \n  \n  \n    \n      BATCH_UPDATE \\ BATCH_PARTIAL_UPDATE\n      x\n       \n      x\n       \n    \n  \n  \n    \n      DELETE\n      x\n      x\n      x\n       \n    \n  \n  \n    \n      BATCH_DELETE\n      x\n       \n      x\n       \n    \n  \n  \n    \n      ACTION\n      x\n      x\n      x\n      x\n    \n  \n\n\nIn the section below, K is used to denote the resource’s key type, and\nV is used to denote the resource’s value type. Remember that for\nassociation resources, K is always CompoundKey.\n\n\n\nGET\n\nThe GET resource method is intended to retrieve a single entity\nrepresentation based upon its key or without a key from a simple\nresource. GET should not have any visible side effects. For example, it\nshould be safe to call whenever the client wishes.\n\nResources providing the GET resource method must override one of the\nfollowing method signatures.\n\nFor collection and association resources:\n\npublic V get(K key);\n\n\nFor simple resources:\n\npublic V get();\n\n\nFor asynchronous resources:\n\npublic Task&lt;V&gt; get(K key);\n\n\nGet methods can also be annotated if not overriding a base class method.\nGET supports a method signature with a wrapper return type.\n\nFor collection and association resources:\n\n@RestMethod.Get\npublic GetResult&lt;V&gt; getWithStatus(K key);\n\n\nFor simple resources:\n\n@RestMethod.Get\npublic GetResult&lt;V&gt; getWithStatus();\n\n\nFor asynchronous resources:\n\n@RestMethod.Get\npublic Task&lt;GetResult&lt;V&gt;&gt; getWithStatus(K key);\n\n\nAn annotated get method may also have arbitrary query params added:\n\n@RestMethod.Get\npublic GetResult&lt;V&gt; get(K key, @QueryParam(\"viewerId\") String viewerId);\n\n\nThe return type GetResult&lt;V&gt; allows users to set an arbitrary HTTP\nstatus code for the response. For more information about the\nRestMethod.Get annotation, see Free-Form\nResources.\n\n\n\nBATCH_GET\n\nThe BATCH_GET resource method retrieves multiple entity representations\ngiven their keys. BATCH_GET should not have any visible side effects.\nFor example, it should be safe to call whenever the client wishes.\nHowever, this is not something enforced by the framework, and it is up\nto the application developer that there are no side effects.\n\nResources providing the BATCH_GET resource method must override the\nfollowing method signature:\n\npublic Map&lt;K, V&gt; batchGet(Set&lt;K&gt; ids);\n\n\nAn asynchronous method must override the following method signature:\npublic Task&lt;Map&lt;K, V&gt;&gt; batchGet(Set&lt;K&gt; ids);\n\n\n@RestMethod.BatchGet may be used to indicate a batch get method\ninstead of overriding the batchGet method of a base class.\n\nResources may also return BatchResult, which allows errors to be\nreturned along with entities that were successfully retrieved.\n\nExample of a batch get:\npublic BatchResult&lt;Long, Greeting&gt; batchGet(Set&lt;Long&gt; ids)\n{\n  Map&lt;Long, Greeting&gt; batch = new HashMap&lt;Long, Greeting&gt;();\n  Map&lt;Long, RestLiServiceException&gt; errors = new HashMap&lt;Long, RestLiServiceException&gt;();\n  for (long id : ids)\n  {\n    Greeting g = _db.get(id);\n    if (g != null)\n    {\n      batch.put(id, g);\n    }\n    else\n    {\n      errors.put(id, new RestLiServiceException(HttpStatus.S_404_NOT_FOUND));\n    }\n  }\n  return new BatchResult&lt;Long, Greeting&gt;(batch, errors);\n}\n\n\nClients should make requests to a batch resource using buildKV() (not\nbuild(), it is deprecated), for example:\n\nnew FortunesBuilders().batchGet().ids(...).buildKV();\n\n\nThe batch size of requests for a BATCH_GET method can be limited and validated by configuring a max batch size.\n\n\n\nGET_ALL\n\nWhen a GET is requested on a collection or association resource with no\nkey provided (for example, /myResource), the GET_ALL resource method is\ninvoked, if present. The GET_ALL resource method retrieves all entities\nfor the collection and supports the same pagination facilities as a\nFINDER.\n\npublic List&lt;V&gt; getAll(@Context PagingContext pagingContext);\n\n\nAn asynchronous resource would implement the following method signature:\npublic Task&lt;List&lt;V&gt;&gt; getAll(@Context PagingContext pagingContext);\n\n\n@RestMethod.GetAll may be used to indicate a get all method instead of\noverriding the getAll method of a base class.\n\nTo directly control the total and metadata returned by a get all method,\ndo not override getAll. Instead, create a new method with the\n@RestMethod.GetAll annotation and return a CollectionResult rather\nthan a list, for example:\n\n@RestMethod.GetAll\npublic CollectionResult&lt;Widgets, WidgetsMetadata&gt; getAllWidgets(@Context PagingContext pagingContext)\n{\n  // ...\n  return new CollectionResult&lt;Widgets, WidgetsMetadata&gt;(pageOfWidgets, total, metadata);\n}\n\n\nWhen returning a CollectionResult from GetAll, the behavior is identical\nto a FINDER. See the FINDER documentation below for additional details\nabout CollectionResult.\n\n\n\nFINDER\n\nFINDER methods model query operations. For example, they retrieve an\nordered list of 0 or more entities based on criteria specified in the\nquery parameters. Finder results will automatically be paginated by the\nRest.li framework. Like GET methods, FINDER methods should not have side\neffects.\n\nResources may provide zero or more FINDER resource methods. Each finder\nmethod must be annotated with the @Finder annotation.\n\nPagination default to start=0 and count=10. Clients may set both of\nthese parameters to any desired value.\n\nThe @Finder annotation takes a single required parameter, which\nindicates the name of the finder method.\n\nFor example:\n\n/*\nYou can access this FINDER method via\n/resources/order?q=findOrder&amp;buyerType=1&amp;buyerId=309&amp;orderId=1208210101\n*/\n@RestLiCollection(name=\"order\",keyName=\"orderId\")\npublic class OrderResource extends CollectionResourceTemplate&lt;Integer,Order&gt;\n{\n  @Finder(\"findOrder\")\n  public List&lt;Order&gt; findOrder(@Context PagingContext context,\n                               @QueryParam(\"buyerId\") Integer buyerId,\n                               @QueryParam(\"buyerType\") Integer buyerType,\n                               @QueryParam(\"orderId\") Integer orderId)\n                              throws InternalException\n  {\n  ...\n  }\n}\n\n\nFinder methods must return either:\n\n\n  List&lt;V&gt;\n  CollectionResult&lt;V, MetaData&gt;\n  BasicCollectionResult&lt;V&gt;, a subclass of CollectionResult\n  a subclass of one the above\n\n\nEvery parameter of a finder method must be annotated with one of:\n\n\n  @Context - indicates that the parameter provides framework context\nto the method. Currently all @Context parameters must be of type\nPagingContext.\n  @QueryParam - indicates that the value of the parameter is\nobtained from a request query parameter. The value of the annotation\nindicates the name of the query parameter. Duplicate names are not\nallowed for the same finder method.\n  @AssocKey - indicates that the value of the parameter is a partial\nassociation key, obtained from the request. The value of the\nannotation indicates the name of the association key, which must\nmatch the name of an @Key provided in the assocKeys field of the\n@RestLiAssociation annotation.\n\n\nParameters marked with @QueryParam, @ActionParam, and @AssocKey\nmay also be annotated with @Optional, which indicates that the\nparameter is not required. The @Optional annotation may specify a\nString value, indicating the default value to be used if the parameter\nis not provided in the request. If the method parameter is of primitive\ntype, a default value must be specified in the @Optional annotation.\n\nValid types for query parameters are:\n\n\n  String\n  boolean / Boolean\n  int / Integer\n  long / Long\n  float / Float\n  double / Double\n  ByteString\n  A Pegasus Enum (any enum defined in a .pdl schema)\n  Custom types (see the bottom of this section)\n  Record template types (any subclass of RecordTemplate generated\nfrom a .pdl schema)\n  Arrays of one of the types above, e.g. String[], long[], …\n\n\n@Finder(\"simpleFinder\")\npublic List&lt;V&gt; simpleFind(`Context PagingContext context);\n\n@Finder(\"complexFinder\")\npublic CollectionResult&lt;V, MyMetaData&gt; complexFinder(@Context(defaultStart= 10, defaultCount = 100) PagingContext context,\n                                                     @AssocKey(\"key1\") Long key,\n                                                     @QueryParam(\"param1\") String requiredParam,\n                                                     @QueryParam(\"param2\") @Optional String optionalParam);\n\n\nA Finder method in an asynchronous resource could implement the following method signatures:\n@Finder(\"simpleAsyncFinder\")\npublic Task&lt;List&lt;V&gt;&gt; simpleFind(`Context PagingContext context);\n\n@Finder(\"complexAsyncFinder\")\npublic Task&lt;CollectionResult&lt;V, MyMetaData&gt;&gt; complexFinder(@Context(defaultStart= 10, defaultCount = 100) PagingContext context,\n                                                     @AssocKey(\"key1\") Long key,\n                                                     @QueryParam(\"param1\") String requiredParam,\n                                                     @QueryParam(\"param2\") @Optional String optionalParam);\n\n\n\n\nTyperefs (Custom Types)\n\nCustom types can be any Java type, as long as it has a coercer and a\ntyperef schema, even Java classes from libraries such as Date. To create\na query parameter that uses a custom type, you will need to write a\ncoercer and a typeref schema for the type you want to use. See the\ntyperef documentation for details.\n\nFirst for the coercer, you will need to write an implementation of\nDirectCoercer that converts between your custom type and some simpler\nunderlying type, like String or Double. By convention, the coercer\nshould be an internal class of the custom type it coerces. Additionally,\nthe custom type should register its own coercer in a static code block.\n\nIf this is not possible (for example, if you want to use a Java built-in\nclass like Date or URI as a custom type), then you can write a separate\ncoercer class and register the coercer with the private variable\ndeclaration:\n\nprivate static final Object REGISTER_COERCER = Custom.registerCoercer(new ObjectCoercer(), CustomObject.class);\n\n\nTyperef Schema\n\nThe purpose of the typeref schemas is to keep track of the underlying\ntype of the custom Type and the location of the custom type’s class,\nand, if necessary, the location of its coercer. The basic appearance of\nthe typeref schema is shown below:\n\nnamespace com.linkedin.example // namespace of the typeref\n\n@java.class = \"com.linkedin.example.CustomObject\" // location of the custom type class\n@java.coercerClass = \"com.linkedin.example.CustomObjectCoercer\" // only needed if the custom type itself cannot contain the coercer as an internal class.\ntyperef CustomObjectRef = string // underlying type that the coercer converts to/from\n\n\nThis typeref can then be referenced in other schemas:\n\nimport com.linkedin.example.CustomObjectRef\n\nrecord ExampleRecord {\n  member: CustomObjectRef\n}\n\n\nThe generated Java data templates will automatically coerce from\nCustomObjectRef to CustomObject when accessing the member field:\n\nCustomObject o = exampleRecord.getMember();\n\n\nOnce Java data templates are generated, the typeref may also be used in\nKeys, query parameters, or action parameters.\n\nKeys:\n\n@RestLiCollection(name=\"entities\", \n                  namespace = \"com.example\",\n                  keyTyperefClass = CustomObjectRef.class)\npublic class EntitiesResource extends CollectionResourceTemplate&lt;CustomObject, Urn&gt;\n\n\nCompound keys:\n\n@RestLiAssociation(name=\"entities\", \n                   namespace=\"com.example\",\n                   assocKeys={@Key(name=\"o\", type=CustomObject.class, typeref=CustomObjectRef.class)})\n\n\nQuery parameters:\n\n@QueryParam(value=\"o\", typeref=CustomObjectRef.class) CustomObject o\n\n@QueryParam(value=\"oArray\", typeref=CustomObjectRef.class) CustomObject[] oArray\n\n\n\n\nBATCH FINDER\n\nThe BATCH_FINDER resource method accepts a list of filters set. Instead of callings multiple finders with different filter values, we call 1 BATCH_FINDER method with a list of filters.\n\nResources may provide zero or more BATCH_FINDER resource methods. Each BATCH_FINDER method must be annotated with the @BatchFinder annotation.\nAnd this method must return a BatchFinderResult.\nFor example:\n\n@BatchFinder(value = \"searchGreetings\", batchParam = \"criteria\")\npublic BatchFinderResult&lt;GreetingCriteria, Greeting, EmptyRecord&gt; searchGreetings(@PagingContextParam PagingContext context,\n                                                                                  @QueryParam(\"criteria\") GreetingCriteria[] criteria,\n                                                                                  @QueryParam(\"message\") String message)\n\n\nAn asynchronous BATCH_FINDER must return a Task&lt;BatchFinderResult&gt;. For example:\n\n@BatchFinder(value = \"searchGreetings\", batchParam = \"criteria\")\npublic Task&lt;BatchFinderResult&lt;GreetingCriteria, Greeting, EmptyRecord&gt;&gt; searchGreetings(@PagingContextParam PagingContext context,\n                                                                                  @QueryParam(\"criteria\") GreetingCriteria[] criteria,\n                                                                                  @QueryParam(\"message\") String message)\n\n\nPlease note that “q” cannot be used as QueryParam name for batch finder, because that will produce ambiguation when construcitng URL.\n\nSee more details about BATCH_FINDER resource method api here: BatchFinder Resource API\n\nThe batch size of requests for a BATCH_FINDER method can be limited and validated by configuring a max batch size.\n\n\n\nCREATE\n\nCREATE methods model the creation of new entities from their\nrepresentation. In CREATE, the resource implementation is responsible\nfor assigning a new key to the created entity. CREATE methods are\nneither safe nor idempotent.\n\nResources providing the CREATE resource method must override the\nfollowing method signature:\n\npublic CreateResponse create(V entity);\n\n\nAn asynchronous method would override the following method signature:\npublic Task&lt;CreateResponse&gt; create(V entity);\n\n\nThe returned CreateResponse object indicates the HTTP status code to\nbe returned (defaults to 201 CREATED), as well as an optional ID for the\nnewly created entity. If provided, the ID will be written into the\n“X-LinkedIn-Id” header by calling toString() on the ID object.\n\n@RestMethod.Create may be used to indicate a create method instead of\noverriding the create method of a base class.\n\nReturning entity in CREATE response\n\nBy default, the newly created entity is not returned in the CREATE\nresponse because the client already has the entity when sending the\nCREATE request. However, there are use cases where the server will\nattach additional data to the new entity. Returning the entity in the\nCREATE response saves the client another GET request.\n\nStarting in Rest.li version 2.10.3, we provide the developer the option to\nreturn the newly created entity. To use this feature, add a @ReturnEntity\nannotation to the method that implements CREATE. The return type of the\nmethod must be CreateKVResponse.\n\n@ReturnEntity\npublic CreateKVResponse create(V entity);\n\n\nAn example implementation for resource is like below, note that the return type will be  CreateKVResponse :\n\n@ReturnEntity\npublic CreateKVResponse&lt;Long, Greeting&gt; create(Greeting entity)\n{\n  Long id = 1L;\n  entity.setId(id);\n  return new CreateKVResponse&lt;Long, Greeting&gt;(entity.getId(), entity);\n}\n\n\nThere may be circumstances in which you want to prevent the server from returning the entity, for example to reduce network traffic.\nHere is an example curl request that makes use of the $returnEntity query parameter to indicate that the entity should not be returned:\n\ncurl -X POST 'localhost:/greetings?$returnEntity=false' \\\n  -H 'X-RestLi-Method: CREATE' \\\n  -d '{\"message\": \"Hello, world!\", \"tone\": \"FRIENDLY\"}'\n\n\n\n\nBATCH_CREATE\n\nBATCH_CREATE methods model the creation of a group of new entities from\ntheir representations. In BATCH_CREATE, the resource implementation is\nresponsible for assigning a new key to each created entity.\nBATCH_CREATE methods are neither safe nor idempotent.\n\nResources providing the BATCH_CREATE resource method must override the\nfollowing method signature:\n\npublic BatchCreateResult&lt;K, V&gt; batchCreate(BatchCreateRequest&lt;K, V&gt; entities);\n\n\nAn asynchronous resource providing the BATCH_CREATE resource method must override the\nfollowing method signature:\npublic Task&lt;BatchCreateResult&lt;K, V&gt;&gt; batchCreate(BatchCreateRequest&lt;K, V&gt; entities);\n\n\nThe BatchCreateRequest object wraps a list of entity representations\nof type V.\n\nThe returned BatchCreateResult object wraps a list of CreateResponse\nobjects (see CREATE). The CreateResponse objects are expected to be\nreturned in the same order and position as the respective input objects.\n\nBatchCreateRequest and BatchCreateResult support the generic type\nparameter K to allow for future extension.\n\n@RestMethod.BatchCreate may be used to indicate a batch create method\ninstead of overriding the batchCreate method of a base class.\n\nExample of a batch create:\n\npublic BatchCreateResult&lt;Long, Greeting&gt; batchCreate(BatchCreateRequest&lt;Long, Greeting&gt; entities)\n{\n  List&lt;CreateResponse&gt; responses = new ArrayList&lt;CreateResponse&gt;(entities.getInput().size());\n\n  for (Greeting g : entities.getInput())\n  {\n    responses.add(create(g));\n  }\n  return new BatchCreateResult&lt;Long, Greeting&gt;(responses);\n}\n\npublic CreateResponse create(Greeting entity)\n{\n  entity.setId(_idSeq.incrementAndGet());\n  _db.put(entity.getId(), entity);\n  return new CreateResponse(entity.getId());\n}\n\n\nError details can be returned in any CreateResponse by providing a\nRestLiServiceException, for example:\n\npublic BatchCreateResult&lt;Long, Greeting&gt; batchCreate(BatchCreateRequest&lt;Long, Greeting&gt; entities) \n{\n  List&lt;CreateResponse&gt; responses = new ArrayList&lt;CreateResponse&gt;(entities.getInput().size());\n\n  ...\n  if (...)\n  {\n    RestLiServiceException exception = new RestLiServiceException(HttpStatus.S_406_NOT_ACCEPTABLE, \"...\");\n    exception.setServiceErrorCode(...);\n    exception.setErrorDetails(...);\n    responses.add(new CreateResponse(exception));\n  }\n  ...\n\n  return new BatchCreateResult&lt;Long, Greeting&gt;(responses);\n}\n\n\nReturning entities in BATCH_CREATE response\n\nSimilar to CREATE, BATCH_CREATE also could return the newly created\nentities in the response. To do that, add a @ReturnEntity annotation to\nthe method implementing BATCH_CREATE. The return type of the method\nmust be BatchCreateKVResult.\n\n@ReturnEntity\npublic BatchCreateKVResult&lt;K, V&gt; batchCreate(BatchCreateRequest&lt;K, V&gt; entities);\n\n\nAn example implementation for resource is like below, note that the\nreturn type will be BatchCreateKVResult:\n\n@ReturnEntity\npublic BatchCreateKVResult&lt;Long, Greeting&gt; batchCreate(BatchCreateRequest&lt;Long, Greeting&gt; entities)\n{\n  List&lt;CreateKVResponse&lt;Long, Greeting&gt;&gt; responses = new ArrayList&lt;CreateKVResponse&lt;Long, Greeting&gt;&gt;(entities.getInput().size());\n  for (Greeting greeting : entities.getInput())\n  {\n    responses.add(create(greeting)); // Create function should return CreateKVResponse\n  }\n  return BatchCreateKVResult&lt;Long, Greeting&gt;(responses);\n}\n\n\nThere may be circumstances in which you want to prevent the server from returning the entity, for example to reduce network traffic.\nHere is an example curl request that makes use of the $returnEntity query parameter to indicate that the entity should not be returned:\n\ncurl -X POST 'localhost:/greetings?$returnEntity=false' \\\n  -H 'X-RestLi-Method: BATCH_CREATE' \\\n  -d '{\"elements\":[{\"message\": \"Hello, world!\", \"tone\": \"FRIENDLY\"},{\"message\": \"Again!\", \"tone\": \"FRIENDLY\"}]}'\n\n\nThe batch size of requests for a BATCH_CREATE method can be limited and validated by configuring a max batch size.\n\n\n\nUPDATE\n\nUPDATE methods model updating an entity with a given key by setting its\nvalue (overwriting the entire entity). UPDATE has side effects but is\nidempotent. For example, repeating the same update operation has the\nsame effect as calling it once.\n\nResources may choose whether to allow an UPDATE of an entity that does\nnot already exist, in which case it should be created. This is different\nfrom CREATE because the client specifies the key for the entity to be\ncreated. Simple resources use UPDATE as a way to create the singleton\nentity.\n\nResources providing the UPDATE resource method must override one of the\nfollowing method signatures.\n\nFor collection and association resources:\n\npublic UpdateResponse update(K key, V entity);\n\n\nFor simple resources:\n\npublic UpdateResponse update(V entity);\n\n\nFor asynchronous resources:\npublic Task&lt;UpdateResponse&gt; update(K key, V entity);\n\n\nThe returned UpdateResponse object indicates the HTTP status code to\nbe returned.\n\n@RestMethod.Update may be used to indicate a update method instead of\noverriding the update method of a base class.\n\n\n\nBATCH_UPDATE\n\nBATCH_UPDATE methods model updating a set of entities with specified\nkeys by setting their values (overwriting each entity entirely).\nBATCH_UPDATE has side effects but is idempotent. For example, repeating\nthe same batch update operation has the same effect as calling it once.\n\nResources may choose whether to allow BATCH_UPDATE for entities that do\nnot already exist, in which case each entity should be created. This is\ndifferent from BATCH_CREATE because the client specifies the keys for\nthe entities to be created.\n\nResources providing the BATCH_UPDATE resource method must override the\nfollowing method signature:\n\npublic BatchUpdateResult&lt;K, V&gt; batchUpdate(BatchUpdateRequest&lt;K, V&gt; entities);\n\n\nAn asynchronous resource must override the following method signature:\npublic Task&lt;BatchUpdateResult&lt;K, V&gt;&gt; batchUpdate(BatchUpdateRequest&lt;K, V&gt; entities);\n\n\nBatchUpdateRequest contains a map of entity key to entity value.\n\nThe returned BatchUpdateResult object indicates the UpdateResponse\nfor each key in the BatchUpdateRequest. In the case of failures,\nRestLiServiceException objects may be added to the BatchUpdateResult\nfor the failed keys.\n\n@RestMethod.BatchUpdate may be used to indicate a batch update method\ninstead of overriding the batchUpdate method of a base class.\n\nExample of a batch update:\n\npublic BatchUpdateResult&lt;Long, Greeting&gt; batchUpdate(BatchUpdateRequest&lt;Long, Greeting&gt; entities)\n{\n  Map&lt;Long, UpdateResponse&gt; responseMap = new HashMap&lt;Long, UpdateResponse&gt;();\n  for (Map.Entry&lt;Long, Greeting&gt; entry : entities.getData().entrySet())\n  {\n    responseMap.put(entry.getKey(), update(entry.getKey(),\n    entry.getValue()));\n  }\n  return new BatchUpdateResult&lt;Long, Greeting&gt;(responseMap);\n}\n\npublic UpdateResponse update(Long key, Greeting entity)\n{\n  Greeting g = _db.get(key);\n  if (g == null)\n  {\n    return new UpdateResponse(HttpStatus.S_404_NOT_FOUND);\n  }\n\n  _db.put(key, entity);\n\n  return new UpdateResponse(HttpStatus.S_204_NO_CONTENT);\n}\n\n\nThe batch size of requests for a BATCH_UPDATE method can be limited and validated by configuring a max batch size.\n\n\n\nPARTIAL_UPDATE\n\nPARTIAL_UPDATE methods model updating part of the entity with a given\nkey. PARTIAL_UPDATE has side effects. In general, it is not guaranteed\nto be idempotent.\n\nResources providing the PARTIAL_UPDATE resource method must override\nthe following method signature:\n\npublic UpdateResponse update(K key, PatchRequest&lt;V&gt; patch);\n\n\nAn asynchronous resource must override the following method signature:\npublic Task&lt;UpdateResponse&gt; update(K key, PatchRequest&lt;V&gt; patch);\n\n\nThe returned UpdateResponse object indicates the HTTP status code to\nbe returned.\n\nRest.li provides tools to make it easy to handle partial updates to your\nresources. A typical update function should look something like this:\n\n@Override\npublic UpdateResponse update(String key, PatchRequest&lt;YourResource&gt; patch)\n{\n  YourResource resource = _db.get(key); // Retrieve the resource object\n  from somewhere\n  if (resource == null)\n  {\n    return new UpdateResponse(HttpStatus.S_404_NOT_FOUND);\n  }\n  try\n  {\n    PatchApplier.applyPatch(resource, patch); // Apply the patch.\n    // Be sure to save the resource if necessary\n  }\n  catch (DataProcessingException e)\n  {\n    return new UpdateResponse(HttpStatus.S_400_BAD_REQUEST);\n  }\n  return new UpdateResponse(HttpStatus.S_204_NO_CONTENT);\n}\n\n\nThe PatchApplier automatically updates resources defined using the\nPegasus Data format. The Rest.li client classes provide support for\nconstructing patch requests, but here is an example update request using\ncurl:\n\ncurl -X POST 'localhost:/fortunes/1' \\\n  -d '{\"patch\": {\"$set\": {\"fortune\": \"you will strike it rich!\"}}}'\n\n\n@RestMethod.PartialUpdate may be used to indicate a partial update\nmethod instead of overriding the partialUpdate method of a base class.\n\nInspecting Partial Updates to Selectively Update Fields in a Backing Store\n\nIt is possible to inspect the partial update and selectively write only\nthe changed fields to a store.\n\nFor example, to update only the street field of this address entity:\n\n{\n  \"address\": {\n    \"street\": \"10th\",\n    \"city\": \"Sunnyvale\"\n  }\n}\n\n\nThe partial update to change just the street field is:\n\n{\n  \"patch\": {\n    \"address\": {\n      \"$set\": {\n        \"street\": \"9th\"\n      }\n    }\n  }\n}\n\n\nFor the service code to selectively update just the street field (e.g.,\nUPDATE addresses SET street=:street WHERE key=:key). The partial update\ncan be inspected and the selective update if only the street field is\nchanged:\n\n@Override\npublic UpdateResponse update(String key, PatchRequest&lt;YourResource&gt; patchRequest)\n{\n  try\n  {\n    DataMap patch = patchRequest.getPatchDocument();\n    boolean selectivePartialUpdateApplied = false;\n    if(patch.containsKey(\"address\") &amp;&amp; patch.size() &gt;= 1)\n    {\n      DataMap address = patch.getDataMap(\"address\");\n      if(address.containsKey(\"$set\") &amp;&amp; address.size()  1)\n      {\n        DataMap set = address.getDataMap(\"$set\");\n        if(address.containsKey(\"street\") &amp;&amp; address.size()  1)\n        {\n          String street = address.getString(\"street\");\n          selectivePartialUpdateApplied = true;\n          // update only the street, since its the only thing this patch requests to change\n        }\n      }\n    }\n    if(selectivePartialUpdateApplied  false)\n    {\n      // no selective update available, update the whole record with\n      PatchApplier and return the result\n    }\n  }\n  catch (DataProcessingException e)\n  {\n    return new UpdateResponse(HttpStatus.S_400_BAD_REQUEST);\n  }\n\n  return new UpdateResponse(HttpStatus.S_204_NO_CONTENT);\n}\n\n\nCreating Partial Updates\n\nTo create a request to modify field(s), PatchGenerator can be used, for\nexample:\n\nFortune fortune = new Fortune().setMessage(\"Today's your lucky day.\");\nPatchRequest&lt;Fortune&gt; patch = PatchGenerator.diffEmpty(fortune);\nRequest&lt;Fortune&gt; request = new FortunesBuilders().partialUpdate().id(1L).input(patch).build();\n\n\nPatchGenerator.diff(original, revised) can also be used to create a\nminimal partial update.\n\nReturning entity in PARTIAL_UPDATE response\n\nBy default, the patched entity is not returned in the PARTIAL_UPDATE response because\nthe client already has the patch data and possibly has the rest of the entity as well.\nHowever, there are use cases where the server will attach additional data to the new entity or the user\nsimply doesn’t have the whole entity. Returning the entity in the PARTIAL_UPDATE response saves  the client\nanother GET request.\n\nStarting in Rest.li version 24.0.0, we provide the developer the option to\nreturn the patched entity. To use this feature, add a @ReturnEntity\nannotation to the method that implements PARTIAL_UPDATE. The return type of the\nmethod must be UpdateEntityResponse.\n\n@ReturnEntity\n@RestMethod.PartialUpdate\npublic UpdateEntityResponse&lt;V&gt; partialUpdate(K key, PatchRequest&lt;V&gt; patch);\n\n\nAn example resource method implementation is as follows, note that the return type will be  UpdateEntityResponse:\n\n@ReturnEntity\n@RestMethod.PartialUpdate\npublic UpdateEntityResponse&lt;Greeting&gt; update(Long key, PatchRequest&lt;Greeting&gt; patch)\n{\n  Greeting greeting = _db.get(key);\n\n  if (greeting == null)\n  {\n    throw new RestLiServiceException(HttpStatus.S_404_NOT_FOUND);\n  }\n\n  try\n  {\n    PatchApplier.applyPatch(greeting, patch);\n  }\n  catch (DataProcessingException e)\n  {\n    throw new RestLiServiceException(HttpStatus.S_400_BAD_REQUEST);\n  }\n\n  return new UpdateEntityResponse&lt;Greeting&gt;(HttpStatus.S_200_OK, greeting);\n}\n\n\nThere may be circumstances in which you want to prevent the server from returning the entity, for example to reduce network traffic.\nHere is an example curl request that makes use of the $returnEntity query parameter to indicate that the entity should not be returned:\n\ncurl -X POST 'localhost:/greetings/1?$returnEntity=false' \\\n  -d '{\"patch\": {\"$set\": {\"message\": \"Hello, world!\"}}}'\n\n\n\n\nBATCH_PARTIAL_UPDATE\n\nBATCH_PARTIAL_UPDATE methods model partial updates of multiple\nentities given their keys. BATCH_PARTIAL_UPDATE has side effects. In\ngeneral, it is not guaranteed to be idempotent.\n\nResources providing the BATCH_PARTIAL_UPDATE resource method must\noverride the following method signature:\n\npublic BatchUpdateResult&lt;K, V&gt; batchUpdate(BatchPatchRequest&lt;K, V&gt; patches);\n\n\nAn asynchronous resource providing the BATCH_PARTIAL_UPDATE resource method must\noverride the following method signature:\n\npublic Task&lt;BatchUpdateResult&lt;K, V&gt;&gt; batchUpdate(BatchPatchRequest&lt;K, V&gt; patches);\n\n\nThe BatchPatchRequest input contains a map of entity key to\nPatchRequest.\n\nThe returned BatchUpdateResult object indicates the UpdateResponse\nfor each key in the BatchPatchRequest. In the case of failures,\nRestLiServiceException objects may be added to the BatchUpdateResult\nfor the failed keys.\n\n@RestMethod.BatchPartialUpdate may be used to indicate a batch partial\nupdate method instead of overriding the batchPartialUpdate method of a\nbase class.\n\nExample of a batch partial update:\n\npublic BatchUpdateResult&lt;Long, Greeting&gt; batchUpdate(BatchPatchRequest&lt;Long, Greeting&gt; entityUpdates)\n{\n  Map&lt;Long, UpdateResponse&gt; responseMap = new HashMap&lt;Long,\n  UpdateResponse&gt;();\n  for (Map.Entry&lt;Long, PatchRequest&lt;Greeting&gt;&gt; entry :\n  entityUpdates.getData().entrySet())\n  {\n    responseMap.put(entry.getKey(), update(entry.getKey(),\n    entry.getValue()));\n  }\n  return new BatchUpdateResult&lt;Long, Greeting&gt;(responseMap);\n}\n\npublic UpdateResponse update(Long key, PatchRequest&lt;Greeting&gt; patch)\n{\n  Greeting g = _db.get(key);\n  if (g == null)\n  {\n    return new UpdateResponse(HttpStatus.S_404_NOT_FOUND);\n  }\n\n  try\n  {\n    PatchApplier.applyPatch(g, patch);\n  }\n  catch (DataProcessingException e)\n  {\n    return new UpdateResponse(HttpStatus.S_400_BAD_REQUEST);\n  }\n\n  _db.put(key, g);\n\n  return new UpdateResponse(HttpStatus.S_204_NO_CONTENT);\n}\n\n\nReturning entities in BATCH_PARTIAL_UPDATE response\n\nBy default, the patched entities are not returned in the BATCH_PARTIAL_UPDATE response because\nthe client already has the patch data and possibly has the rest of the entities as well.\nHowever, there are use cases where the server will attach additional data to the new entities or the user\nsimply doesn’t have the whole entities. Returning the entities in the BATCH_PARTIAL_UPDATE response saves the client\nanother GET request.\n\nStarting in Rest.li version 25.0.5, we provide the developer the option to\nreturn the patched entities. To use this feature, add a @ReturnEntity\nannotation to the method that implements BATCH_PARTIAL_UPDATE. The return type of the\nmethod must be BatchUpdateEntityResult.\n\n@ReturnEntity\n@RestMethod.BatchPartialUpdate\npublic BatchUpdateEntityResult&lt;K, V&gt; batchPartialUpdate(BatchPatchRequest&lt;Long, Greeting&gt; patches);\n\n\nAn example resource method implementation is as follows, note that the return type will be  BatchUpdateEntityResult:\n\n@ReturnEntity\n@RestMethod.BatchPartialUpdate\npublic BatchUpdateEntityResult&lt;Long, Greeting&gt; batchPartialUpdate(BatchPatchRequest&lt;Long, Greeting&gt; patches)\n{\n  Map&lt;Long, UpdateEntityResponse&lt;Greeting&gt;&gt; responseMap = new HashMap&lt;&gt;();\n  Map&lt;Long, RestLiServiceException&gt; errorMap = new HashMap&lt;&gt;();\n  for (Map.Entry&lt;Long, PatchRequest&lt;Greeting&gt;&gt; entry : patches.getData().entrySet())\n  {\n    try\n    {\n      UpdateEntityResponse&lt;Greeting&gt; updateEntityResponse = partialUpdate(entry.getKey(), entry.getValue());\n      responseMap.put(entry.getKey(), updateEntityResponse);\n    }\n    catch (RestLiServiceException e)\n    {\n      errorMap.put(entry.getKey(), e);\n    }\n  }\n  return new BatchUpdateEntityResult&lt;&gt;(responseMap, errorMap);\n}\n\n\nThere may be circumstances in which you want to prevent the server from returning the entities, for example to reduce network traffic.\nHere is an example curl request that makes use of the $returnEntity query parameter to indicate that the entity should not be returned:\n\ncurl -X POST 'localhost:/greetings?ids=List(1)&amp;$returnEntity=false' \\\n  -d '{\"entities\":{\"1\":{\"patch\": {\"$set\": {\"message\": \"Hello, world!\"}}}}}' \\\n  -H 'X-RestLi-Method: BATCH_PARTIAL_UPDATE' \\\n  -H 'X-RestLi-Protocol-Version: 2.0.0'\n\n\nThe batch size of requests for a BATCH_PARTIAL_UPDATE method can be limited and validated by configuring a max batch size.\n\n\n\nDELETE\n\nDELETE methods model deleting (removing) an entity with a given key on\ncollection and association resources or without a key on simple\nresources. DELETE has side effects but is idempotent.\n\nResources providing the DELETE resource method must override one of the\nfollowing method signatures.\n\nFor collection and association resources:\n\npublic UpdateResponse delete(K key);\n\n\nFor simple resources:\n\npublic UpdateResponse delete();\n\n\nFor asynchronous resources:\npublic Task&lt;UpdateResponse&gt; delete(K key);\n\n\nThe returned UpdateResponse object indicates the HTTP status code to\nbe returned.\n\n@RestMethod.Delete may be used to indicate a delete method instead of\noverriding the delete method of a base class.\n\n\n\nBATCH_DELETE\n\nBATCH_DELETE methods model deleting (removing) multiple entities given\ntheir keys. BATCH_DELETE has side effects but is idempotent.\n\nResources providing the BATCH_DELETE resource method must override the\nfollowing method signature:\n\npublic BatchUpdateResult&lt;K, V&gt; batchDelete(BatchDeleteRequest&lt;K, V&gt; ids);\n\n\nAsynchronous resources providing the BATCH_DELETE resource method must override the\nfollowing method signature:\n\npublic Task&lt;BatchUpdateResult&lt;K, V&gt;&gt; batchDelete(BatchDeleteRequest&lt;K, V&gt; ids);\n\n\nThe BatchDeleteRequest input contains the list of keys to be deleted.\nBatchDeleteRequest accepts a generic type parameter V for future\nextension.\n\nThe returned BatchUpdateResult object indicates the UpdateResponse\nfor each key in the BatchDeleteRequest. In the case of failures,\nRestLiServiceException objects may be added to the BatchUpdateResult\nfor the failed keys.\n\n@RestMethod.BatchDelete may be used to indicate a batch delete method\ninstead of overriding the batchDelete method of a base class.\n\nExample of a batch delete:\n\npublic BatchUpdateResult&lt;Long, Greeting&gt; batchDelete(BatchDeleteRequest&lt;Long, Greeting&gt; deleteRequest)\n{\n  Map&lt;Long, UpdateResponse&gt; responseMap = new HashMap&lt;Long, UpdateResponse&gt;();\n  for (Long id : deleteRequest.getKeys())\n  {\n    responseMap.put(id, delete(id));\n  }\n  return new BatchUpdateResult&lt;Long, Greeting&gt;(responseMap);\n}\n\npublic UpdateResponse delete(Long key)\n{\n  boolean removed = _db.remove(key) != null;\n\n  return new UpdateResponse(removed ? HttpStatus.S_204_NO_CONTENT : HttpStatus.S_404_NOT_FOUND);\n}\n\n\nThe batch size of requests for a BATCH_DELETE method can be limited and validated by configuring a max batch size.\n\n\n\nACTION\n\nACTION methods are very flexible and do not specify any standard\nbehavior.\n\nResources may provide zero or more ACTION resource methods. Each action\nmust be annotated with the @Action annotation.\n\nThe @Action annotation supports the following parameters:\n\n\n  name Required, the name of the action resource method.\n  resourceLevel Optional, defaults to ResourceLevel.ANY, which\nindicates that the action is defined directly on the containing\nresource and does not support an entity key as a URI parameter.\nResourceLevel.COLLECTION indicates that the action is defined on\nthe containing association or collection resource and does not\nsupport an entity key as a URI parameter. ResourceLevel.ENTITY\nindicates that the action is defined on the entity and it requires\nan entity key as a URI parameter when the containing resource is an\nassociation or collection resource. If the containing resource is a\nsimple resource ResourceLevel.ENTITY indicates that the action is\ndefined directly on the resource and does not support an entity key\nas a URI parameter.\n  returnTyperef Optional, defaults to no typeref. Indicates a\nTyperef to be used in the IDL for the action’s return parameter.\nUseful for actions that return primitive types.\n\n\nEach parameter to an action method must be annotated with\n@ActionParam, which takes the following annotation parameters:\n\n\n  value Required, string name for the action parameter. If this is\nthe only annotation, parameter, it may be specified without being\nexplicitly named, for example, @ActionParam(\"paramName\").\n  typeref Optional, Typeref to be used in the IDL for the parameter.\n\n\nParameters of action methods may also be annotated with @Optional,\nwhich indicates that the parameter is not required in the request. The\n@Optional annotation may specify a String value, which specifies the\ndefault value to be used if the parameter is not provided in the\nrequest. If the method parameter is of primitive type, a default value\nmust be specified in the @Optional annotation.\n\nValid parameter types and return types for action are:\n\n\n  String\n  boolean / Boolean\n  int / Integer\n  long / Long\n  float / Float\n  double / Double\n  ByteString\n  A Pegasus Enum (any enum defined in a .pdl schema)\n  RecordTemplate or a subclass of RecordTemplate generated from a\nrecord schema\n  FixedTemplate or a subclass of FixedTemplate generated from a\nfixed schema\n  AbstractArrayTemplate or a subclass of AbstractArrayTemplate,\nfor example, StringArray, LongArray, and so on.\n  AbstractMapTemplate or a subclass of AbstractMapTemplate, for\nexample, StringMap, LongMap, and so on.\n  Custom types\n\n\nSimilar to GetResult&lt;V&gt;, since 1.5.8, Rest.li supports an\nActionResult&lt;V&gt; wrapper return type that allows you to specify an\narbitrary HTTP status code for the response.\n\nSimple example:\n\n@Action(name=\"action\")\npublic void doAction();\n\n\nA more complex example, illustrating multiple parameters:\n(Note this action is defined on the entity level, and the entity Key is accessed from the conxt)\n@Action(name=\"sendTestAnnouncement\",resourceLevel= ResourceLevel.ENTITY)\npublic void sendTestAnnouncement(@ActionParam(\"subject\") String subject, \n                                 @ActionParam(\"message\") String message, \n                                 @ActionParam(\"emailAddress\") String emailAddress)\n{\n  Long entityKey = getContext().getPathKeys().get(_resourceName + \"Id\");\n  // ...\n}\n\n\n\nActionParam vs. QueryParam\n\n@ActionParam and @QueryParam are used in different methods.\n@ActionParam is only allowed in Action methods, while @QueryParam is\nallowed in all non-Action methods. Besides, they are also different in\nterms of how the parameter data is sent to the server. If a parameter is\nannotated with @QueryParam, the information will be sent in the\nrequest url. If a parameter is annotated with @ActionParam, the\ninformation will be sent in the request body. Therefore, one advantage\nof using @ActionParam would be that the sent parameter can be encoded.\nOne disadvantage is that the purpose of the request itself can become\nless clear if one only examines the url.\n\n\n\nReturning Nulls\n\nResource methods should never explicitly return null. If the Rest.li\nframework detects this, it will return an HTTP 500 back to the client\nwith a message indicating ‘Unexpected null encountered’. The only\nexceptions to this rule are ACTION and GET. If an ACTION resource method\nreturns null, the rest.li framework will return an HTTP 200. If a\nGET returns null, the Rest.li framework will return an HTTP 404.\n\nAlso note that the HTTP 500 will also be generated by the Rest.li\nframework if subsequent data structures inside of resource method\nresponses are null or contain null. This applies to any data structure\nthat is not a RecordTemplate. For example, all of the the following\nwould cause an HTTP 500 to be returned. Note this list is not\nexhaustive:\n\n\n  A BatchCreateResult returning a null results list.\n  A BatchCreateResult returning a valid list that as a null\nelement inside of it.\n  A CreateResponse returning a null for the HttpStatus.\n  A BatchUpdateResult returning a null key in the results map.\n  A BatchUpdateResult returning a null errors map.\n  A BatchUpdateResult returning a valid errors map, but with a\nnull key or null value inside of it.\n\n\nIt is good practice to make sure that null is never returned in any\npart of resource method responses, with the exception of RecordTemplate\nclasses, ACTION methods and GET methods.\n\n\n\nResourceContext\n\nResourceContext provides access to the context of the current request.\nResourceContext is injected into resources that implement the\nBaseResource interface, by calling setContext().\n\nFor resources extending CollectionResourceTemplate,\nAssociationResourceTemplate, or ResourceContextHolder, the current\ncontext is available by calling getContext().\n\nResourceContext provides methods to access the raw request, as well as\nparsed values from the request. ResourceContext also provides some\ncontrol over the generated response, such as the ability to set response\nheaders.\n\n\n\nResource Templates\n\nResource Templates provide convenient methods for implementing resource\nclasses by extending them. Subclasses may selectively override relevant\nmethods and for methods that are not overridden, the framework will\nrecognize that your resource does not support this method and will\nreturn a 404 if clients attempt to invoke it. Note that unsupported\nmethods will be omitted from your resources IDL (see Restspec IDL\nfor details).\n\nCollectionResourceTemplate\n\nCollectionResourceTemplate provides a convenient base class for\ncollection resources. CollectionResourceTemplate defines methods for\nall of the CRUD operations. Subclasses may also implement FINDER, BATCH_FINDER and\nACTION methods by annotating as described above. The asynchronous collection resource templates are CollectionResourceTaskTemplate and CollectionResourceAsyncTemplate.\n\npublic CreateResponse create(V entity);\npublic BatchCreateResult&lt;K, V&gt; batchCreate(BatchCreateRequest&lt;K, V&gt; entities);\npublic V get(K key);\npublic Map&lt;K, V&gt;batchGet(Set&lt;K&gt; ids);\npublic UpdateResponse update(K key, V entity);\npublic BatchUpdateResult&lt;K, V&gt; batchUpdate(BatchUpdateRequest&lt;K, V&gt; entities);\npublic UpdateResponse update(K key, PatchRequest&lt;V&gt; patch);\npublic BatchUpdateResult&lt;K, V&gt; batchUpdate(BatchPatchRequest&lt;K, V&gt; patches);\npublic UpdateResponse delete(K key);\npublic BatchUpdateResult&lt;K, V&gt; batchDelete(BatchDeleteRequest&lt;K, V&gt; ids);\n\n\nSimpleResourceTemplate\n\nSimpleResourceTemplate provides a convenient base class for simple\nresources. SimpleResourceTemplate defines methods for GET, UPDATE, and\nDELETE methods. Subclasses may also implement ACTION methods by\nannotating as described above. The asynchronous simple resource templates are SimpleResourceTaskTemplate and SimpleResourceAsyncTemplate.\n\npublic V get();\npublic UpdateResponse update(V entity);\npublic UpdateResponse delete();\n\n\nAssociationResourceTemplate\n\nAssociationResourceTemplate provides a convenient base class for\nassociation resources. AssociationResourceTemplate defines methods for\nall of the CRUD operations except CREATE. Association resources should\nimplement CREATE by providing up-sert semantics on UPDATE. Subclasses\nmay also implement FINDER, BATCH_FINDER and ACTION methods by annotating as described\nabove. The asynchronous association resource templates are AssociationResourceTaskTemplate and AssociationResourceAsyncTemplate.\n\npublic CreateResponse create(V entity);\npublic BatchCreateResult&lt;CompoundKey, V&gt;batchCreate(BatchCreateRequest&lt;CompoundKey, V&gt; entities);\npublic V get(CompoundKey key);\npublic Map&lt;CompoundKey, V&gt; batchGet(Set&lt;CompoundKey&gt; ids);\npublic UpdateResponse update(CompoundKey key, V entity);\npublic BatchUpdateResult&lt;CompoundKey, V&gt;batchUpdate(BatchUpdateRequest&lt;CompoundKey, V&gt; entities);\npublic UpdateResponse update(CompoundKey key, PatchRequest&lt;V&gt; patch);\npublic BatchUpdateResult&lt;CompoundKey, V&gt; batchUpdate(BatchPatchRequest&lt;CompoundKey, V&gt; patches);\npublic UpdateResponse delete(CompoundKey key);\npublic BatchUpdateResult&lt;CompoundKey, V&gt; batchDelete(BatchDeleteRequest&lt;CompoundKey, V&gt; ids);\n\n\n\n\nFree-Form Resources\n\nResource Templates provide a convenient way to implement the recommended\nsignatures for the basic CRUD operations (CREATE, GET, UPDATE,\nPARTIAL_UPDATE, DELETE, and respective batch operations). When\npossible, we recommend using the resource templates to ensure that your\ninterface remains simple and uniform.\n\nHowever, it is sometimes necessary to add custom parameters to CRUD\noperations. In these cases, the fixed signatures of resource templates\nare too constraining. The solution is to create a free-form resource by\nimplementing the corresponding marker interface for your resource and\nannotating CRUD methods with @RestMethod.* annotations.The\nKeyValueResource interface is the marker interface for collection and\nassociation resources where the SingleObjectResource interface is the\nmarker interface for simple resources.\n\npublic class FreeFormCollectionResource implements KeyValueResource&lt;K, V&gt;\n{\n  @RestMethod.Create\n  public CreateResponse myCreate(V entity);\n\n  @RestMethod.BatchCreate\n  public BatchCreateResult&lt;K, V&gt; myBatchCreate(BatchCreateRequest&lt;K, V&gt; entities);\n\n  @RestMethod.Get\n  public V myGet(K key);\n\n  @RestMethod.GetAll\n  public CollectionResult&lt;V, M&gt; myGetAll(@Context PagingContext pagingContex);\n\n  @RestMethod.BatchGet\n  public Map&lt;K, V&gt; myBatchGet(Set&lt;K&gt; ids);\n\n  @RestMethod.Update\n  public UpdateResponse myUpdate(K key, V entity);\n\n  @RestMethod.BatchUpdate\n  public BatchUpdateResult&lt;K, V&gt; myBatchUpdate(BatchUpdateRequest&lt;K, V&gt; entities);\n\n  @RestMethod.PartialUpdate\n  public UpdateResponse myUpdate(K key, PatchRequest&lt;V&gt; patch);\n\n  @RestMethod.BatchPartialUpdate\n  public BatchUpdateResult&lt;K, V&gt; myBatchUpdate(BatchPatchRequest&lt;K, V&gt; patches);\n\n  @RestMethod.Delete\n  public UpdateResponse myDelete(K key);\n\n  @RestMethod.BatchDelete\n  public BatchUpdateResult&lt;K, V&gt; myBatchDelete(BatchDeleteRequest&lt;K, V&gt; ids);\n}\n\n\npublic class FreeFormSimpleResource implements SingleObjectResource&lt;V&gt;\n{\n  @RestMethod.Get\n  public V myGet();\n\n  @RestMethod.Update\n  public UpdateResponse myUpdate(V entity);\n\n  @RestMethod.Delete\n  public UpdateResponse myDelete();\n}\n\n\nThe advantage of explicitly annotating each resource method is that you\ncan add custom query parameters (see description of @QueryParam for\nFINDER resource method) and take advantage of wrapper return types.\nCustom query parameters must be defined after the fixed parameters\nshown above.\n\n@RestMethod.Get\npublic V myGet(K key, @QueryParam(\"myParam\") String myParam);\n\n@RestMethod.Get\npublic GetResult&lt;V&gt; getWithStatus(K key);\n\n\nNote that each resource may only provide one implementation of each CRUD\nmethod (for example, it is invalid to annotate two different methods with\n@RestMethod.Get).\n\nThings to Remember about Free-Form Resources\n\n\n  Free-form resources allow you to add query parameters to CRUD\nmethods.\n  Resource Templates should be used whenever possible.\n  Free-form resources must implement one of the KeyValueResource and\nSingleObjectResource marker interfaces.\n  Methods in free-form resources must be annotated with appropriate\n@RestMethod.* annotations.\n  Methods in free-form resources must use the same return type and\ninitial signature as the corresponding Resource Template method.\n  Methods in free-form resources may add additional parameters\nafter the fixed parameters.\n  Free-form resources may not define multiple implementations of the\nsame resource method.\n\n\n\n\nReturning Errors\n\nThere are several mechanisms available for resources to report errors to\nbe returned to the caller. Regardless of which mechanism is used,\nresources should be aware of the resulting HTTP status code and ensure\nthat meaningful status codes are used. Remember that 4xx codes should\nbe used to report client errors (errors that the client may be able to\nresolve), and 5xx codes should be used to report server errors.\n\nReturn null for GET\n\nIf a resource method returns null for GET, the framework will\nautomatically generate a 404 response to be sent to the client.\n\nNote that returning null for resource methods is generally forbidden\nwith the exception of GET and ACTION. Returning a null for a GET\nreturns a 404 and returning a null for an ACTION returns 200.\n\nReturning a null for any other type of resource method will cause the\nrest.li framework to return an HTTP 500 to be sent back to the client\nwith a message indicating ‘Unexpected null encountered’. This is\ndescribed in detail above at Returning\nNulls\n\nReturn Any HTTP Status Code in a CreateResponse/UpdateResponse\n\nCreateResponse and UpdateResponse allow an Http Status\nCode to be\nprovided. Status codes in the 4xx and 5xx ranges may be used to\nreport errors.\n\nThrow RestLiServiceException to Return a 4xx/5xx HTTP Status Code\n\nThe framework defines a special exception class,\nRestLiServiceException, which contains an Http Status\nCode field, as\nwell as other fields that are returned to the client in the body of the\nHTTP response. Resources may throw RestLiServiceException or a\nsubclass to prompt the framework to return an HTTP error response.\n\nThrow Another Exception\n\nAll exceptions originating in application code are caught by the\nframework and used to generate an HTTP response. If the exception does\nnot extend RestLiServiceException, an HTTP 500 response will be\nsent.\n\nReturn Errors as Part of a BatchResult\n\nBATCH_GET methods may return errors for individual items as part of a\nBatchResult object. Each error is represented as a RestLiServiceException\nobject. In this case, the overall status will still be an HTTP 200.\n\npublic BatchResult&lt;K, V&gt; batchGet((Set&lt;K&gt; ids)\n{\n  Map&lt;K, V&gt; results = ...\n  Map&lt;K, RestLiServiceException&gt; errors = ...\n  ...\n  return new BatchResult(results, errors);\n}\n\n\nIf you want to return an error response with an overall status of 4xx or 5xx,\nthen you can do this by throwing a RestLiServiceException in the resource method.\nIn this case, the response will be an error response and won’t contain any batch\ninformation.\n\npublic BatchResult&lt;K, V&gt; batchGet((Set&lt;K&gt; ids)\n{\n  throw new RestLiServiceException(HttpStatus.S_400_BAD_REQUEST);\n}\n\n\nThe same logic applies to BATCH_UPDATE, BATCH_PARTIAL_UPDATE, and BATCH_DELETE.\n\nFor BATCH_FINDER method, it may return errors for individual criteria as part of a BatchFinderResult object.\nEach error is represented as a RestLiServiceException object when it cannot find a corresponding\nresponse for that search criteria Or the developer can put the customized RestLiServiceException into the BatchFinderResult . \nIn this case, the overall status will still be an HTTP 200.\n\nHandling Errors on the Client\n\nWhen making requests using RestClient, a ResponseFuture is always\nreturned, as shown in this example:\n\nResponseFuture&lt;Greeting&gt; future = restClient.sendRequest(new GreetingsBuilders.get().id(1L));\n\n\nThis future might contain an error response. When calling\nResponseFuture.getResponse(), the default behavior is for a\nRestLiResponseException to be thrown if the response contains an error\nresponse. Error responses are all 400 and 500 series HTTP status code,\nas shown in this example:\n\ntry\n{\n  Greeting greeting = restClient.sendRequest(new GreetingsBuilders.get().id(1L)).getResponseEntity();\n  // handle successful response\n}\ncatch (RestLiResponseException e)\n{\n  if(e.getStatus() == 400) {\n    // handle 400\n  } else {\n    // ... handle other status codes or rethrow\n  }\n}\n\n\nAlternatively, ErrorHandlingBehavior.TREAT_SERVER_ERROR_AS_SUCCESS can\nbe set when making a request. If set, .getResponse() will not\nthrow RestLiResponseException even if the response contains a 400 or\n500 series HTTP status code, as shown in this example:\n\nResponse&lt;Greeting&gt; response = restClient.sendRequest(new GreetingsBuilders.get().id(1L),\n                                                     ErrorHandlingBehavior.TREAT_SERVER_ERROR_AS_SUCCESS).getResponse();\nif(response.getStatus() == 200)\n{\n  // handle successful response\n}\nelse if (response.getStatus() == 404)\n{\n  // handle 404\n}\nelse\n{\n  // ... handle other status codes or rethrow\n}\n\n\nHowever because error responses do not contain an entity, calling\nResponseFuture.getResponseEntity() or Response.getEntity() will\nalways throw a RestLiResponseException for 400 or 500 series HTTP\nstatus code, regardless of ErrorHandlingBehavior.\n\nConfiguring How Errors are Represented in an HTTP Response\n\nBy default, Rest.li returns an extensive HTTP error response that\nincludes:\n\n\n  HTTP Status Code (manditory)\n  X-LinkedIn-Error-Response header (this will be renamed to\nX-RestLi-Error-Response shortly)\n  A response body containing:\n    \n      A full stack trace\n      A service error code (optional)\n      Application specific error details (optional)\n    \n  \n\n\nThe error response format configured to return only a subset of these\nparts using RestLiConfig, as shown in this example:\n\nrestLiConfig.setErrorResponseFormat(ErrorResponseFormat.MESSAGE_AND_DETAILS);\n\n\nWhen Rest.li server application code throws an exception, if the\nexception is of type RestLiServiceException, then the error message\nprovided by the RestLiServiceException is used for the error message in\nthe HTTP response. However if any other Java exception is thrown,\nRest.li automatically provides a default error message of “Error in\napplication code” in the error response. This default error message may\nbe customized via RestLiConfig as well, as shown in this example:\n\nrestLiConfig.setInternalErrorMessage(\"Internal error, please try again later.\");\n\n\nSee this page for more information on\nerror responses in Rest.li.\n\n\n\nField Projection\n\nRest.li provides built-in support for field projections, for example the\nstructural filtering of responses. The support includes Java Projection Bindings\nand a JSON Projection wire protocol. The\nprojection is applied separately to each entity object in the response\n(i.e., to the value-type of the CollectionResource or\nAssociationResource). If the invoked method is a FINDER that returns a\nList, the projection is applied to each element of the list\nindividually. Likewise, if the invoked method is a BATCH_GET that\nreturns a Map&lt;K, V&gt;, the projection is applied to each value in the\nmap individually. Project can also be applied to CREATE and\nBATCH_CREATE when the newly returned entity or entities are returned.\n\nFor resource methods that return CollectionResult, the Rest.li framework\nalso provides the ability to project the Metadata and as well as the\nPaging that is sent back to the client. More info on Collection\nPagination is provided below.\n\nThe Rest.li server framework recognizes the “fields”, “metadataFields”,\nor “pagingFields” query parameters in the request. If available, the\nRest.li framework then parses each of these as individual MaskTrees.\nThe resulting MaskTrees are available through the ResourceContext (see\nabove) or directly to the resource methods.\n\nProjection can also be toggled between AUTOMATIC and MANUAL. The\nlatter precludes the Rest.li framework from performing any projection\nwhile the former forces the Rest.li framework to perform the projection.\n\nAdditional details are described in How to Use Projections in Java\n\n\n\nCollection Pagination\n\nRest.li provides helper methods to implement collection pagination, but\nit requires each resource to implement core pagination logic itself.\nRest.li pagination uses positional indices to specify page boundaries.\n\nThe Rest.li server framework automatically recognizes the \"start\" and\n\"count\" parameters for pagination, parses the values of these\nparameters, and makes them available through a PagingContext object.\nFINDER methods may request the PagingContext by declaring a method\nparameter annotated with @Context (see above).\n\nFINDER methods are expected to honor the PagingContext requirements,\nfor example, to return only the subset of results with logical indices\n&gt;= start and &lt; start+count.\n\nThe Rest.li server framework also includes support for returning\nCollectionMetadata as part of the response. CollectionMetadata includes\npagination info such as:\n\n\n  The requested start\n  The requested count\n  The total number of results (before pagination)\n  Links to the previous and next pages of results\n\n\nFINDER methods that can provide the total number of matching results\nshould do so by returning an appropriate CollectionResult or\nBasicCollectionResult object.\n\nIn order for the Rest.li framework to automatically construct Link objects,\ncertain conditions must be met. For both previous and next links, the count\nin the request must be greater than 0. For links to the previous page,\nstart must be greater than 0. For links to the next page, the sum\nof start and count must be less than the total number of results.\nIt’s possible for the total property to be unspecified, but in this case\nthe PageIncrement property of the CollectionResult must be RELATIVE\nand the amount of results returned by the resource method must match the\ncount desired in the request. The reasoning here is that the only way\nfor Rest.li to know that you’re reaching the end of a collection of\nresults is if the amount of results returned differs from the amount\nrequested.\n\nHere is an example request illustrating the use of start &amp; count pagination parameters,\nand resulting links in CollectionMetadata:\n\n$ curl \"http://localhost:1338/greetings?q=search&amp;start=4&amp;count=2\"\n{\n  \"elements\": [ ... ],\n  \"paging\": {\n    \"count\": 2,\n    \"links\": [\n      \"href\": \"/greetings?count=10&amp;start=10&amp;q=search\",\n      \"rel\": \"next\",\n    \"type\": \"application/json\"\n    ],\n    \"start\": 4\n  }\n}\n\n\nNote that “start” and “count” returned in CollectionMetadata is REQUEST\nstart and REQUEST count (that is​, the paging parameter passed from\nincoming REQUEST, not metadata for the returned response). If start and\ncount is not passed in Finder or GetAll request, it will return default\n0 for start and 10 for count.The rationale behind this is to make it\neasier for a client to subsequently construct requests for additional\npages without having to track the start and count themselves.\nFurthermore, there is no point to return a count for number of items\nreturned, since client can easily get that by calling size() for the\nelements array returned.\n\n\n\nDependency Injection\n\nThe Rest.li server framework controls the lifecycle of instances of\nResource classes, instantiating a new Resource object for each request.\nIt is therefore frequently necessary/desirable for resources to use a\ndependency-injection mechanism to obtain the objects they depend upon,\nfor example, database connections or other resources.\n\nRest.li includes direct support for the following dependency injection frameworks:\n\n\n  Spring using the rest.li/spring bridge\n  Guice using the rest.li/guicebridge\n\n\nOther dependency injection frameworks can be used as well. Rest.li\nprovides an extensible dependency-injection mechanism, through the\nResourceFactory interface.\n\nThe most broadly used dependency injection mechanism is based on mapping\nJSR-330 annotations to the Spring ApplicationContext, and it is provided\nby the InjectResourceFactory from restli-contrib-spring. This is the\nrecommended approach.\n\nResource classes may annotate fields with @Inject or @Named. If only\n@Inject is specified, the field will be bound to a bean from the\nSpring ApplicationContext based on the type of the field. If @Named is\nused, the field will be bound to a bean with the same name. All beans\nmust be in the root Spring context.\n\nOnline Documentation\n\nRest.li has an on-line documentation generator that dynamically\ngenerates resource IDL and PDL schemas hosted in the server. The\ndocumentation is available in both HTML and JSON formats, and there are\nthree ways to access the documentation:\n\n\n  HTML. The relative path to HTML documentation is restli/docs/. For\nexample, the documentation URI for resource\nhttp://&lt;host&gt;:&lt;port&gt;/&lt;context-path&gt;/&lt;resource&gt; is\nGET http://&lt;host&gt;:&lt;port&gt;/&lt;context-path&gt;/restli/docs/rest/&lt;resource&gt;\n(GET is the HTTP GET\nmethod,\nwhich is the default for the web browser). The root URL, such as\nhttp://&lt;host&gt;:&lt;port&gt;/&lt;context-path&gt;/restli/docs, displays the list\nof all accessible resources and data schemas in the server. Use it\nas a starting point for HTML documentation. Remember to remove the\n&lt;context-path&gt; part if there is no context path.\n  JSON. There are 2 alternative ways to access the raw JSON data:\n    \n      Use the format=json query parameter on any of the HTML pages\nabove. For example,\nGET http://&lt;host&gt;:&lt;port&gt;/&lt;context-path&gt;/restli/docs/rest/&lt;resource&gt;?format=json\nfor resource documentation and\nGET http://&lt;host&gt;:&lt;port&gt;/&lt;context-path&gt;/restli/docs/data/&lt;full_name_of_data_schema&gt;?format=json\nfor schema documentation. Homepage\nGET http://&lt;host&gt;:&lt;port&gt;/&lt;context-path&gt;/restli/docs/?format=json\nis also available, which aggregates all resources and data\nschemas.\n      Use the HTTP OPTIONS\nmethod.\nSimply replace the HTTP GET method with the OPTIONS method when\naccessing a resource without using the format query parameter.\nThis approach only works for resources, and there is no need for\nthe special restli/docs/ path. For example,\nOPTIONS http://&lt;host&gt;:&lt;port&gt;/&lt;context-path&gt;/&lt;resource&gt;.\n    \n  \n\n\nThe JSON format is structured as following:\n\n{\n  \"models\": {\n    \"&lt;full_name_of_data_schema_1&gt;\": { pdsc_of_data_schema_1 },\n    \"&lt;full_name_of_data_schema_2&gt;\": { pdsc_of_data_schema_2 }\n  },\n  \"resources\": {\n    \"&lt;resource_1&gt;\": { idl_of_resource_1 },\n    \"&lt;resource_2&gt;\": { idl_of_resource_2 }\n  }\n}\n\n\nWhen accessing the JSON format of data schema, the resources key\nexists but the value is always empty.\n\nInitialize Online Documentation Generator\n\n\n  documentationRequestHandler: instance of\nRestLiDocumentationRequestHandler class, default to null. Specify\nwhich implementation of documentation generator is used in the\nserver. If null, the on-line documentation feature is disabled.\n  serverNodeUri: URI prefix of the server without trailing slash,\ndefault to empty string (“”). The URI prefix is mainly used in the\nHTML documents by DefaultDocumentationRequestHandler to properly\ngenerate links. Usually, this should be an absolute path.\n\n",
      tags: null,
      id: 34
    });
    
    

  
    index.add({
      title: "Rest.li architecture user guide",
      category: null,
      content: "Rest.li Architecture User Guide\n\nContents\n\n\n  Introduction\n  Asynchronous APIs\n  Server Data Flow\n  Client Data Flow\n  Development Flow\n\n\nThis document describes how to use Rest.li to build RESTful clients and\nservers. The first section introduces key architectural elements and\nprovides an overview of the development process. The remainder of the\ndocument serves as a detailed reference to Rest.li features. It is not\nnecessary to read this entire document before using Rest.li. Once you\nunderstand the basic principles, you can refer to specific sections in\nthis guide when you have questions. If you just want to get started\nexploring a simple sample implementation, see the Tutorial to Create a Server and Client.\n\nIntroduction\n\nRest.li is a Java framework that allows you to easily create clients and\nservers that use a REST style of communication. Rest.li is based on an\ninversion-of-control model. The framework handles most of the data flow\nand client/server interaction transparently and calls code you supply at\nthe appropriate time.\n\nRest.li allows you to build and access RESTful servers and clients,\nwithout worrying too much about the details of HTTP or JSON. You simply\ndefine a data model (using a schema definition language) and\nresources (Java classes that supply or act on the appropriate data in\nresponse to HTTP requests), and Rest.li takes care of everything else.\nIn this section, we’ll describe the flow of control and data between a\nRest.li server and client. We’ll also look briefly at the development\nprocess, so you understand what tasks you need to do to develop Rest.li\nclients and servers, including what Rest.li does for you automatically.\n\nThe Rest.li server framework consists of libraries that provide\nannotations and helper classes for describing your resources, as well as\nan inversion-of-control dispatcher that handles incoming requests and\nautomatically invokes the appropriate methods in your resources.\n\nThe following diagram provides a high-level view of the interaction and\ndata flow between a Rest.li client and server. The yellow arrows\nindicate the flow of requests out of the client and into the server,\nwhile dark blue arrows represent the server’s response. You as a\ndeveloper implement the Resource classes in the server. Rest.li provides\nthe platform code and infrastructure for dispatching and handling\nrequests. It also generates the Record Templates and RequestBuilder\nclasses:\n\n\nData and Control Flow Between a Rest.li Server and\nClient\n\n\n\n\nAsynchronous APIs\n\nRest.li is built on simple asynchronous APIs. These APIs allow both\nservers to run in non-blocking event based frameworks and allow client\ncode to be written to make non-blocking calls. This approach has a\ncouple major benefits. On the server, it means that our servers can be\nleaner and scale to high request throughput because we don’t need large,\nper request, thread pools. On the client, it makes it easy to stitch\ntogether multiple requests to servers in sophisticated flows where\nindependent calls can be made in parallel.\n\nRest.li’s client implementation is Netty-based and is designed to work\nseamlessly with ParSeq to\nconstruct complex asynchronous request flows.\n\nThere are several server implementations:\n\n\n  Servlet\n— Battle tested and ready for production use. Containers\nsupporting Servlet 3.0\nAPI\nare required to benefit from asynchronous, non-blocking request\nprocessing. Jetty 8.x supports Servlet 3.0 and has been used in\nlarge production environments.\n  Netty\n— Experimental\n  Embedded Jetty — Primarily for integration testing as it’s trivial\nto spin up as part of a test suite\n\n\nSee Asynchronous Resources for more details on how to handle requests using non-blocking request processing.\n\nThe remainder of this guide will assume use of the servlet server\nimplementation.\n\nServer Data Flow\n\nStarting with the server (on the right in the diagram above), the\nfollowing steps occur when a request is submitted to a Rest.li server:\n\n\n  The R2 transport layer receives a request (HTTP + JSON) and sends it\non to Rest.li. (R2 is a separate library that provides HTTP transport\nservices. It is independent of Rest.li but is included with the Rest.li\ncode base. It’s designed to work well with Rest.li.)\n  Rest.li’s routing logic inspects the request’s URI path and\ndetermines which target resource (a Java class) the server has defined\nto handle that request.\n  Rest.li parses the request to extract any parameters.\n  Rest.li creates a new instance of the resource class designated to\nhandle the request.\n  Rest.li invokes the appropriate methods of the resource object,\npassing in any necessary Java parameters.\n  The resource object instantiates and returns a response, in the form\nof a RecordTemplate object.\n  Rest.li serializes the response object and passes it back to the\nrequesting client through the R2 transport layer.\n\n\nWe’ll look at what you, as a developer, need to do to support this data\nflow shortly. Although, you probably noticed that Rest.li does most of\nthe work. The primary task of the developer is to define the data model\nto be supported by your server and implement the resource classes that\ncan produce that data. Rest.li handles the details of routing requests,\ninstantiating resource classes, and invoking methods on objects at the\nright time.\n\nWhen writing resource classes, it is important to understand that\nRest.li constructs a new instance of the appropriate resource class to\nhandle each request. This means that resource objects cannot store state\nacross multiple requests. Any long-lived resources should be managed\nseparately. See Dependency Injection.\n\n\n\nClient Data Flow\n\nRest.li also provides support for writing clients. Clients issue\nrequests by instantiating a RequestBuilder object that supports methods\nthat allow details of the request to be specified. The RequestBuilder\nobject generates a Request object that can be passed to Rest.li and sent\nto the server using the R2 transport layer. When the server responds (as\ndetailed above), the client receives the request using the R2 transport,\nand Rest.li produces a RecordTemplate object (matching the object\ninstantiated by the server) and provides the object to the client.\n\nBoth client and server work with the same Java representations of the\nserver’s data model. Note that you do not need to use a Rest.li based\nclient to communicate with a Rest.li server. However, Rest.li supports\ntype-safe data exchange using Java interfaces when using Rest.li for\nboth client and server.\n\n\n\nDevelopment Flow\n\nNext, let’s briefly look at the basic development flow required to\nimplement a client and server to support the data flow described in the\nprevious section. Your tasks as a developer are basically to define your\ndata model using a simple modeling language and to implement Java\nclasses that act on or produce that data. Rest.li supports these tasks\nwith a combination of base classes and code generation.\n\nThe following diagram illustrates the major steps in building servers\nand clients based on the Rest.li framework. The numbers in the diagram\ncorrespond to the sequence in which tasks are done. Blue boxes represent\nclasses you will write, while green boxes represent components that are\ncreated by Rest.li’s code generators. Black arrows indicate a code\ngeneration process; red dashed lines indicate the use of classes that\nallow a server and clients to exchange data.\n\n\nRest.li Development\nFlow\n\n\nLet’s look at each step:\n\n\n  Step 1. The first step in building a Rest.li application is to\ndefine your data schema using Pegasus Data Schemas.\n  In Step 2, a Rest.li code generator creates Java classes that\nrepresent the data model defined in Step 1. These RecordTemplate\nclasses serve as the Java representation of the data in both the\nserver and client.\n  Step 3 is to implement the server Resource classes and define\nthe operations they support. Rest.li provides a set of annotations\nand base classes that allow you to map Resource classes to REST\nendpoints and to specify methods of your Resource classes to respond\nto REST operations, such as GET or PUT. Your Resource classes are\nexpected to return data using instances of the RecordTemplate\nclasses generated in Step 2.\n  In Step 4, Rest.li generates an interface description (IDL) file\nthat provides a simple, textual, machine-readable specification of\nthe server resources implemented in Step 3. The IDL is considered\nthe source of truth for the interface contract between the server\nand its clients. The IDL itself is a language-agnostic JSON format.\nRest.li uses this IDL along with the original data schema files to\nsupport automatically generating human-readable documentation, which\ncan be requested from a server. See IDL Compatibility\nfor build details and how run the IDL check in “backwards” and\n“ignore” modes.\n  Step 5 is to create your server application, which involves\nleveraging a few Rest.li classes to instantiate the Rest.li server,\nset up the transport layer, and supply Rest.li with the location\n(class path) of your Resource classes.\n  In Step 6, Rest.li generates classes known as RequestBuilders\nthat correspond to the server resource classes. These\nRequestBuilders are used by clients to create requests to the\nserver. Together with the RecordTemplate and Resource classes,\nRequestBuilders provide convenient and type-safe mechanisms for\nworking with the data models supported by the server.\n  Finally, Step 7 is to implement one or more clients. Clients\nissue requests by instantiating the RequestBuilder classes generated\nin Step 6. These RequestBuilders produce Requests that are passed to\nRest.li to issue requests to a server.\n\n\nThe Tutorial\nprovides a step-by-step walk through of this development process and\ndemonstrates the nuts and bolts, including build scripts and other\ninfrastructure required to execute these steps.\n",
      tags: null,
      id: 35
    });
    
    

  
    index.add({
      title: "Configuring Service Errors in Java",
      category: null,
      content: "Configuring Service Errors in Java\n\nThis page describes how to configure service errors for a resource in Java.\n\nSee Service Errors for an in-depth reference of service errors in Rest.li.\n\nContents\n\n\n  Background\n  Step 1: Define your Service Errors\n  Step 2: Write Your Service Error Definition\n  Step 3: Apply the Service Errors to your Resource\n  Step 4: Returning Service Errors\n  Step 5: Enabling Service Error Validation\n\n\nBackground\n\nIt’s important to communicate to the consumers of a service what types of errors may be returned by a particular API.\nFortunately, Rest.li allows application developers to configure service errors for their resources. This makes APIs\nclearer by documenting in the IDL what sorts of errors should be expected, allowing clients to effectively handle and\nprocess returned errors. Also, enabling service error validation allows services to guarantee which errors a client\nshould expect to encounter.\n\nStep 1: Define your Service Errors\n\nFirst, you need to determine which service error codes your service may return, and what information will be contained\nin error responses corresponding with these codes. You’ll need to define four things:\n\n\n  The application-specific string code associated with some service error (e.g. QUOTA_EXCEEDED)\n  The HTTP status code it’ll return (e.g. 429)\n  The string message it’ll return\n  The schema defining the format of the error details\n\n\nFor example, say we want to standardize the way errors are returned as a result of violating the service’s rate-limiting\nconstraint. We would need to pick a string error code such as QUOTA_EXCEEDED, an HTTP status such as 429, a string\nmessage describing the failure, and some schema describing the custom error details sent back to the client.\n\nIf you need help determining which HTTP status code is right for your service error, please\nsee this table of codes.\n\nStep 2: Write Your Service Error Definition\n\nWrite a Java enum implementing the\nServiceError\ninterface, which will contain a set of service error definitions for your service. The enum implementation may expose\nmethods for obtaining the HTTP status, the error code, the error message, and the error detail type. Here is an example\nimplementation which defines one service error and exposes all four accessor methods:\n\npublic enum SampleServiceError implements ServiceError\n{\n  QUOTA_EXCEEDED(HttpStatus.S_429_TOO_MANY_REQUESTS, \"You've exceeded your daily request quota.\", QuotaDetails.class),\n  INVALID_PERMISSIONS(HttpStatus.S_403_FORBIDDEN, \"User does not have valid permissions\", null),\n  ILLEGAL_PARAM(HttpStatus.S_400_BAD_REQUEST, \"Request parameter cannot be used\", null);\n  // Add more service errors here...\n\n  SampleServiceError(HttpStatus status, String message, Class&lt;? extends RecordTemplate&gt; errorDetailType)\n  {\n    _httpStatus = status;\n    _message = message;\n    _errorDetailType = errorDetailType;\n  }\n\n  private final HttpStatus _status;\n  private final String _message;\n  private final Class&lt;? extends RecordTemplate&gt; _errorDetailType;\n\n  /*\n   * Suggested: Define string constants for each of the error codes for easy access.\n   * String literals or constants must be used as annotation attributes due to the\n   * restrictions imposed by Java.\n   *\n   * These codes should match the enum value names.\n   */\n  public class Codes\n  {\n    QUOTA_EXCEEDED = \"QUOTA_EXCEEDED\";\n    INVALID_PERMISSIONS = \"INVALID_PERMISSIONS\";\n    ILLEGAL_PARAM = \"ILLEGAL_PARAM\";\n  }\n\n  @Override\n  public HttpStatus httpStatus()\n  {\n    return _status;\n  }\n\n  @Override\n  public String code()\n  {\n    // Note how the application-specific code defers to the enum value name\n    return name();\n  }\n\n  @Override\n  public String message()\n  {\n    return _message;\n  }\n\n  @Override\n  public Class&lt;? extends RecordTemplate&gt; errorDetailType()\n  {\n    return _errorDetailType;\n  }\n}\n\n\nStep 3: Apply the Service Errors to your Resource\n\nNow that you have defined your service errors, you can use them to specify which resources\nor particular resource methods may return such errors. You can do this by using a few\nJava annotations described below.\n\nKeep in mind that adding or removing service error codes impacts the backward compatibility of a resource.\nSee here for more information.\n\n@ServiceErrorDef\n\nIn order to apply your service error definitions to a particular resource, you first need to use the @ServiceErrorDef\nannotation at the resource level to reference your enum. This class-level annotation is used to reference your service\nerror enum definition. This is used by Rest.li during build-time when generating IDLs and also during runtime when\nvalidating service errors returned by your resource. Without this annotation, Rest.li will have no way of referencing\nthe information in your service error definition.\n\n@ServiceErrorDef(SampleServiceError.class)\nclass MyResource extends CollectionResourceTemplate&lt;Long, MyRecord&gt;\n{\n  // Resource methods here...\n}\n\n\n@ServiceErrors\n\nNext, you can use the @ServiceErrors annotation to indicate which service error codes may be returned by a particular\nresource or method. It can be used as a class-level annotation (to refer to a resource) or as a method-level annotation\n(to refer to a resource method). This is used during build-time when generating IDLs and also during runtime when\nvalidating service errors returned by your resource. In the following example, all methods in the resource may return a\nQUOTA_EXCEEDED error, but only getAll can return an INVALID_PERMISSIONS error:\n\n@ServiceErrorDef(SampleServiceError.class)\n@ServiceErrors(SampleServiceError.Codes.QUOTA_EXCEEDED)\nclass MyResource extends CollectionResourceTemplate&lt;Long, MyRecord&gt;\n{\n  @Override\n  public MyRecord get(Long id)\n  {\n    // Logic here...\n  }\n\n  @Override\n  @ServiceErrors(SampleServiceError.Codes.INVALID_PERMISSIONS)\n  public List&lt;MyRecord&gt; getAll()\n  {\n    // Logic here...\n  }\n}\n\n\n@ParamError\n\nAdditionally, you can use the @ParamError annotation to indicate that a service error may be returned by a particular\nmethod, but also that it references a specific parameter of such method. This is used solely as a means of documentation\nin the IDL; no validation at all is done at runtime using this information. In this example, the search method may\nreturn an ILLEGAL_PARAM error related to either the foo parameter or the bar parameter:\n\n@ServiceErrorDef(SampleServiceError.class)\nclass MyResource extends CollectionResourceTemplate&lt;Long, MyRecord&gt;\n{\n  @Finder(\"search\")\n  @ParamError(code = SampleServiceError.Codes.ILLEGAL_PARAM, parameterNames = { \"foo\", \"bar\" })\n  public List&lt;MyRecord&gt; search(@QueryParam(\"foo\") String foo, @QueryParam(\"bar\") String bar)\n  {\n    // Logic here...\n  }\n}\n\n\n@SuccessResponse\n\nIn addition to service errors, success response codes can also be specified for a resource method. Note that success\ncodes (unlike service errors) are not validated by the Rest.li framework, meaning that the framework will allow\nunrecognized success codes to be returned. This feature is meant only for enhancing IDL documentation, please see\nthis page for how this are documented. In this example, the action\nmethod may return an HTTP 200 or 204 response:\n\n@ServiceErrorDef(SampleServiceError.class)\nclass MyResource extends CollectionResourceTemplate&lt;Long, MyRecord&gt;\n{\n  @Action(name = \"action\")\n  @SuccessResponse(statuses = { HttpStatus.S_200_OK, HttpStatus.S_204_NO_CONTENT })\n  public void action()\n  {\n    // Logic here...\n  }\n}\n\n\nStep 4: Returning Service Errors\n\nTo return one of your defined service errors, you can simply do the following in your\nresource implementation:\n\nthrow new RestLiServiceException(SampleServiceError.QUOTA_EXCEEDED);\n\n\nThis automatically constructs an ErrorResponse using information from your service error definition:\n\n{\n  \"code\": \"QUOTA_EXCEEDED\",\n  \"httpStatus\": 429,\n  \"message\": \"You've exceeded your daily request quota.\"\n}\n\n\nThis looks good, except it’s missing some extra information such as error details and\na request ID. Fortunately, you can use builder syntax to easily add this data:\n\nRestLiServiceException exception = new RestLiServiceException(SampleServiceError.QUOTA_EXCEEDED);\n\nthrow exception.setRequestId(requestId).setErrorDetails(quotaInfo);\n\n\nYou can also use purely builder syntax to construct your exception:\n\nthrow new RestLiServiceException(HttpStatus.S_400_BAD_REQUEST).setCode(code);\n\n\nStep 5: Enabling Service Error Validation\n\nReturned service errors can be validated in order to ensure that only specified\nservice errors are returned. In other words, if a method returns an error that\nisn’t specified via an annotation, a 500 exception will be thrown.\n\nThis validation can be enabled by adding an ErrorResponseValidationFilter to\nthe server filter chain:\n\nrestLiConfig.addFilter(new ErrorResponseValidationFilter());\n\n\nNote that only exceptions containing a code property will be validated.\n",
      tags: null,
      id: 36
    });
    
    

  
    index.add({
      title: "How Data is Represented in Memory",
      category: null,
      content: "How Data is Represented in Memory\n\nContents\n\n\n  The Data Layer\n  The Data Schema Layer\n  The Data Template Layer\n\n\nThere are three architectural layers that define how data is stored\nin-memory and provide the API’s used to access this data.\n\n\n  The first layer is the Data layer. This is the storage layer and is\ntotally generic, for example, not schema aware.\n  The second layer is the Data Schema layer. This layer provides the\nin-memory representation of the data schema.\n  The third layer is the Data Template layer. This layer provides Java\ntype-safe access to data stored by the Data layer.\n\n\nThe Data Layer\n\nAt the conceptual level, the Data layer provides generic in-memory\nrepresentations of JSON objects and arrays. A DataMap and a DataList\nprovide the in-memory representation of a JSON object and a JSON array\nrespectively. These DataMaps and DataLists are the primary in-memory\ndata structures that store and manage data belonging to instances of\ncomplex schema types. This layer allows data to be serialized and\nde-serialized into in-memory representations without requiring the\nschema to be known. In fact, the Data layer is not aware of schemas and\ndo not require a schema to access the underlying data.\n\nThe main motivations behind the Data layer are:\n\n\n  To allow generic access to the underlying data for building generic\nassembly and query engines. These engines need a generic data\nrepresentation to data access. Furthermore, they may need to\nconstruct new instances from dynamically executed expressions, such\nas joins and projections. The schema of these instances depend on\nthe expression executed, and could not be known in advance.\n  To facilitate schema evolution. The Data layer enables “use what you\nknow and pass on what you don’t”. It allows new fields to be added\nand passed through intermediate nodes in the service graph without\nrequiring these nodes to also have their schemas updated to include\nthese new fields.\n  To permit some Java Virtual Machine service calls to be optimized by\navoiding serialization.\n\n\nConstraints\n\nThe Data layer implements the following constraints:\n\n\n  It permits only allowed types to be stored as values.\n  All non-container values (not DataMap and not DataList) are\nimmutable.\n  Null is not a value. The Data.NULL constant is used to represent\nnull deserialized from or to be serialized to JSON. Avoiding null\nJava values reduces complexity by reducing the number of states a\nfield may have. Without null values, a field can have two states,\n“absent” or “has valid value”. If null values are permitted, a\nfield can have three states, “absent”, “has null value”, and “has\nvalid value”.\n  The object graph is always acyclic. The object graph is the graph of\nobjects connected by DataMaps and DataLists.\n  The key type for a DataMap is always java.lang.String.\n\n\nAdditional Features\n\nThe Data layer provides the following additional features (above and\nbeyond what the Java library provides.)\n\n\n  A DataMap and DataList may be made read-only. Once it is\nread-only, mutations will no longer be allowed and will throw\njava.lang.UnsupportedOperationException. There is no way to revert\na read-only instance to read-write.\n  Access instrumentation. See com.linkedin.data.Instrumentable for\ndetails.\n  Implements deep copy that should return a object graph that is\nisomorphic with the source, i.e. the copy will retain the directed\nacyclic graph structure of the source.\n\n\nAllowed Value Types\n\n\n  java.lang.Integer\n  java.lang.Long\n  java.lang.Float\n  java.lang.Double\n  java.lang.Boolean\n  java.lang.String\n  com.linkedin.data.ByteString\n  com.linkedin.data.DataMap\n  com.linkedin.data.DataList\n\n\nNote Enum types are not allowed because enum types are not generic and\nportable. Enum values are stored as a string.\n\nDataComplex\n\nBoth DataMap and DataList implement the\ncom.linkedin.data.DataComplex interface. This interface declares the\nmethods that supports the additional features common to a DataMap and\na DataList. These methods are:\n\n\n  \n    \n      Method\n      Declared by\n      Description\n    \n  \n  \n    \n      DataComplex clone()\n      DataComplex\n      A shallow copy of the instance. The read-only state is not copied, the clone will be mutable. The instrumentation state is also not copied. Although java.lang.CloneNotSupportedException is declared in the throws clause, the method should not throw this exception.\n    \n    \n      DataComplex copy() \n      DataComplex \n      A deep copy of the object graph rooted at the instance. The copy will be isomorphic to the original. The read-only state is not deep copied, and the new DataComplex copies will be mutable. The instrumentation state is also not copied. Although java.lang.CloneNotSupportedException is declared in the throws clause, the method should not throw this exception.\n    \n    \n      void setReadOnly() \n      CowCommon \n      Make the instance read-only. It does not affect the read-only state of contained DataComplex values. \n    \n    \n      boolean isReadOnly() \n      CowCommon \n      Whether the instance is in read-only state. \n    \n    \n      void makeReadOnly() \n      DataComplex \n      Make the object graph rooted at this instance read-only. \n    \n    \n      void isMadeReadOnly() \n      DataComplex \n      Whether the object graph rooted at this instance has been made read-only. \n    \n    \n      Collection&lt;Object&gt; values() \n      DataComplex \n      Returns the values stored in the DataComplex instance, i.e. returns the values of a DataMap or the elements of a DataList. \n    \n    \n      void startInstrumentatingAccess() \n      Instrumentable \n      Starts instrumenting access. \n    \n    \n      void stopInstrumentingAccess() \n      Instrumentable \n      Stops instrumenting access. \n    \n    \n      void clearInstrumentedData() \n      Instrumentable \n      Clears instrumentation data collected. \n    \n    \n      void collectInstrumentedData(...) \n      Instrumentable \n      Collect data gathered when instrumentation was enabled. \n    \n  \n\n\nNote: Details on CowCommon, CowMap, and CowList have been omitted\nor covered under DataComplex. Cow provides copy-on-write\nfunctionality. The semantics of CowMap and CowList is similar to\nHashMap and ArrayList.\n\nDataMap\n\nThe com.linkedin.data.DataMap class has the following characteristics:\n\n\n  DataMap implements java.util.Map&lt;String, Object&gt;.\n  Its entrySet(), keySet(), and values() methods return\nunmodifiable set and collection views.\n  Its clone() and copy() methods returns a DataMap.\n\n\nDataList\n\nThe com.linkedin.data.DataList class has the following\ncharacteristics.\n\n\n  DataList implements java.util.List&lt;Object&gt;.\n  Its clone() and copy() method return a DataList.\n\n\nThe Data Schema Layer\n\nThe Data Schema layer provides the in-memory representation of the data\nschema. The Data Schema Layer provides the following main features:\n\n\n  Parse a JSON encoded schema into in-memory representation using\nclasses in this layer\n  Validate a Data object against a schema\n\n\nTheir common base class for Data Schema classes is\ncom.linkedin.data.schema.DataSchema. It defines the following methods:\n\n\n  \n    \n      Method\n      Description\n    \n  \n  \n    \n      Type getType() \n      Provide the type of the schema, can be BOOLEAN, INT, LONG, FLOAT, DOUBLE, BYTES, STRING, FIXED, ENUM, NULL, ARRAY, RECORD, MAP, UNION. \n    \n    \n      boolean hasError() \n      Whether the schema definition contains at least one error. \n    \n    \n      boolean isPrimitive() \n      Whether the schema type is a primitive schema type. \n    \n    \n      boolean isComplex() \n      Whether the schema type is a complex schema type, i.e. not primitive type. \n    \n    \n      Map&lt;String,Object&gt; getProperties() \n      Return the properties of the schema. These properties are the keys and values from the JSON fields in complex schema definitions that are not processed and interpreted by the schema parser. For primitive types, this method always return an immutable empty map. \n    \n    \n      String getUnionMemberKey() \n      If this type is used as a member of a union without an alias, this will be the key that uniquely identifies/selects this type within the union. This value of this key is as defined by the Avro 1.4.1 specification for JSON serialization. \n    \n    \n      String toString() \n      A more human consumable formatting of the schema in JSON encoding. Space will added between fields, items, names, values, … etc. \n    \n    \n      Type getDereferencedType \n      If the type is a typeref, it will follow the typeref reference chain and return the type referenced at the end of the typeref chain. \n    \n    \n      DataSchema getDereferencedSchema \n      If the type is a typeref, it will follow the typeref reference chain and return the DataSchema referenced at the end of the typeref chain. \n    \n  \n\n\nThe following table shows the mapping of schema types to Data Schema\nclasses.\n\n\n  \n    \n      Schema Type \n      Data Schema class \n      Relevant Specific Attributes \n    \n  \n  \n    \n      int \n      IntegerDataSchema \n       \n    \n    \n      long \n      LongDataSchema \n       \n    \n    \n      float \n      FloatDataSchema \n       \n    \n    \n      double \n      DoubleDataSchema \n       \n    \n    \n      boolean \n      BooleanDataSchema \n       \n    \n    \n      string \n      StringDataSchema \n       \n    \n    \n      bytes \n      BytesDataSchema \n       \n    \n    \n      enum \n      EnumDataSchema \n      List&lt;String&gt; getSymbols() int index(String symbol) boolean contains(String symbol) \n    \n    \n      array \n      ArrayDataSchema \n      DataSchema getItems() \n    \n    \n      map \n      MapDataSchema \n      DataSchema getValues()\n    \n    \n      fixed \n      FixedDataSchema \n      int getSize() \n    \n    \n      record, error \n      RecordDataSchema \n      RecordType recordType() (record or error) boolean isErrorRecord() List&lt;Field&gt; getFields() int index(String fieldName) boolean contains(String fieldName) Field getField(String fieldName) \n    \n    \n      union \n      UnionDataSchema \n      List&lt;Member&gt; getMembers() boolean contains(String memberKey) DataSchema getTypeByMemberKey(String memberKey) boolean areMembersAliased() \n    \n    \n      null \n      NullDataSchema \n       \n    \n  \n\n\nData to Schema Validation\n\nThe ValidateDataAgainstSchema class provides methods for validating\nData layer instances with a Data Schema. The ValidationOption class is\nused to specify how validation should be performed and how to fix-up the\ninput Data layer objects to conform to the schema. There are two\nindependently configuration options:\n\n\n  RequiredMode option indicates how required fields should be\nhandled during validation.\n  CoercionMode option indicates how to coerce Data layer objects to\nthe Java type corresponding to their schema type.\n\n\nExample Usage:\n\nValidationResult validationResult =\nValidateDataAgainstSchema.validate(dataTemplate, dataTemplate.schema(),\nnew ValidationOptions());  \nif (!validationResult.isValid())  \n{  \n  // do something  \n}  \n\n\nRequiredMode\n\nThe available RequiredModes are:\n\n\n  IGNORE\nRequired fields may be absent. Do not indicate a validation error if\na required field is absent.\n  MUST_BE_PRESENT\nIf a required field is absent, then validation fails. Validation\nwill fail even if the required field has been declared with a\ndefault value.\n  CAN_BE_ABSENT_IF_HAS_DEFAULT\nIf a required field is absent and the field has not been declared\nwith a default value, then validation fails. Validation will not\nattempt to modify the field to provide it with the default value.\n  FIXUP_ABSENT_WITH_DEFAULT\nIf a required field is absent and it cannot be fixed-up with a\ndefault value, then validation fails.\nThis mode will attempt to modify an absent field to provide it with\nthe field’s default value.\nIf the field does not have a default value, validation fails.\nIf the field has a default value, validation will attempt to set the\nfield’s value to the default value.\nThis attempt may fail if fixup is not enabled or the DataMap\ncontaining the field cannot be modified because it is read-only.\nThe provided default value will be read-only.\n\n\nCoercionMode\n\nSince JSON does not have or encode enough information on the actual\ntypes of primitives, and schema types like bytes and fixed are not\nrepresented by native types in JSON, the initial de-serialized in-memory\nrepresentation of instances of these types may not be the actual type\nspecified in the schema. For example, when de-serializing the number 52,\nit will be de-serialized into an Integer even though the schema type\nmay be a Long. This is because a schema is not required to serialize\nor de-serialize.\n\nWhen the data is accessed via schema aware language binding like the\nJava binding, the conversion/coercion can occur at the language binding\nlayer. In cases when the language binding is not used, it may be\ndesirable to fix-up a Data layer object by coercing it the Java type\ncorresponding to the object’s schema. For example, the appropriate Java\ntype the above example would be a Long. Another fix-up would be to\nfixup Avro-specified string encoding of binary data (bytes or fixed)\ninto a ByteString. In another case, it may be desirable to coerce the\nstring representation of a value to the Java type corresponding to the\nobject’s schema. For example, coerce “65” to 65, the integer, if the\nschema type is “int”.\n\nWhether an how coercion is performed is specified by CoercionMode. The\navailable CoercionModes are:\n\n\n  OFF\nNo coercion is performed.\n  NORMAL\nNumeric types are coerced to the schema’s corresponding Java numeric\ntype. Avro-encoded binary strings are coerced to ByteString if the\nschema type is bytes or fixed.\n  STRING_TO_PRIMITIVE\nIncludes all the coercions performed by NORMAL. In addition, also\ncoerces string representations of numbers to the schema’s\ncorresponding numeric type, and string representation of booleans\n(“true” or “false” case-insenstive) to Boolean.\n\n\n\n\nNORMAL Coercion Mode\n\nThe following table provides additional details on the NORMAL\nvalidation and coercion mode.\n\n\n  \n    \n      Schema Type \n      Post-coercion Java Type \n      Pre-coercion Input Java Types \n      Validation Performed \n      Coercion Method\n    \n  \n  \n    \n      int \n      java.lang.Integer \n      java.lang.Number (1) \n      Value must be a Number. \n      Number.intValue() \n    \n    \n      long \n      java.lang.Long \n      java.lang.Number (1)\n      Value must be a Number. \n      Number.longValue() \n    \n    \n      float \n      java.lang.Float \n      java.lang.Number (1)\n      Value must be a Number.\n      Number.floatValue() \n    \n    \n      double \n      java.lang.Double \n      java.lang.Number (1)\n      Value must be a Number.\n      Number.doubleValue() \n    \n    \n      boolean \n      java.lang.Boolean \n      java.lang.Boolean (2) \n      Value must be a Boolean. \n       \n    \n    \n      string \n      java.lang.String \n      java.lang.String (2) \n      Value must be a String. \n       \n    \n    \n      bytes \n      com.linkedin.data.ByteString \n      com.linkedin.data.ByteString, java.lang.String (3) \n      If the value is a String, the String must be a valid encoding of binary data as specified by the Avro specification for encoding bytes into a JSON string. \n      ByteString.copyFromAvroString() \n    \n    \n      enum \n      java.lang.String \n      java.lang.String \n      The value must be a symbol defined by the enum schema. \n       \n    \n    \n      array \n      com.linkedin.data.DataList \n      com.linkedin.data.DataList (2) \n      Each element in the DataList must be a valid Java type for the schema’s item type. For example, if the schema is an array of longs, then every element in the DataList must be a Number. \n       \n    \n    \n      map \n      com.linkedin.data.DataMap \n      com.linkedin.data.DataMap (2) \n      Each value in the DataMap must be a valid Java type for the schema’s value type. For example, if the schema is a map of longs, then every value in the DataMap must be a Number.\n       \n    \n    \n      fixed \n      com.linkedin.data.ByteString \n      com.linked.data.ByteString (2), java.lang.String (3) \n      If the value is a String, the String must be a valid encoding of binary data as specified by the Avro specification for encoding bytes into a JSON string and the correct size for the fixed schema type. If the value is a ByteString, the ByteString must be the correct size for the fixed schema type. \n      ByteString.copyFromAvroString()\n    \n    \n      record \n      com.linkedin.data.DataMap \n      com.linkedin.data.DataMap (2) \n      Each key in the DataMap will be used lookup a field in the record schema. The value associated with this key must be a valid Java type for the field’s type. If the required validation option is enabled, then all required fields must also be present. \n       \n    \n    \n      union \n      com.linkedin.data.DataMap \n      java.lang.String, com.linkedin.data.DataMap (2) \n      If the value is a String, the value must be Data.NULL. If the value is a DataMap, then the DataMap must have exactly one entry. The key of the entry must identify a member of the union schema, and the value must be a valid type for the identified union member’s type. \n       \n    \n  \n\n\n(1) Even though Number type is allowed and used for fixing up to the\ndesired type, the Data layer only allows Integer, Long, Float, and\nDouble values to be held in a DataMap or DataList.\n(2) No fix-up is performed.\n(3) the String must be a valid encoding of binary data as specified by\nthe Avro specification for encoding bytes into a JSON string.\n\nSTRING_TO_PRIMITIVE Coercion Mode\n\nThis mode includes allowed input types and associated validation and\ncoercion’s of NORMAL. In addition, it allows the following additional\ninput types and performs the following coercions on these additional\nallowed input types.\n\n\n  \n    \n      Schema Type \n      Post-coercion Java Type \n      Pre-coercion Input Java Types \n      Validation Performed \n      _. Coercion Method \n    \n  \n  \n    \n      int \n      java.lang.Integer \n      java.lang.String \n      If value is a String, it must be acceptable to BigDecimal(String val), else it has to be a Number (see “NORMAL”) \n      (new BigDecimal(value)).intValue() \n    \n    \n      long \n      java.lang.Long \n      java.lang.String\n      If value is a String, it must be acceptable to BigDecimal(String val), else it has to be a Number (see “NORMAL”)\n      (new BigDecimal(value)).longValue()\n    \n    \n      float \n      java.lang.Float \n      java.lang.String\n      If value is a String, it must be acceptable to BigDecimal(String val), else it has to be a Number (see “NORMAL”) \n      (new BigDecimal(value)).floatValue()\n    \n    \n      double \n      java.lang.Double \n      java.lang.String\n      If value is a String, it must be acceptable to BigDecimal(String val), else it has to be a Number (see “NORMAL”) \n      (new BigDecimal(value)).doubleValue()\n    \n    \n      boolean \n      java.lang.Boolean \n      java.lang.String\n      if value is a String, its value must be either \"true\" or \"false\" ignoring case, else it has to be a Boolean (see “NORMAL”) \n      &lt;pre&gt;if (\"true\".equalsIgnoreCase(value)) return Boolean.TRUE;else if (\"false\".equalsIgnoreCase(value))  return Boolean.FALSE;else   // invalid string representation  // of boolean &lt;/pre&gt;\n    \n  \n\n\nValidationResult\n\nThe result of validation is returned through an instance of the\nValidationResult class. This class has the following methods:\n\n\n  \n    \n      Method\n      Description \n    \n  \n  \n    \n      boolean hasFix() \n      Whether any fix-ups (i.e., modification or replacement of input Data layer objects) have been proposed. Fixes may be proposed but not applied because fixes cannot be applied to read-only complex objects. \n    \n    \n      boolean hasFixupReadOnlyError() \n      Whether any fix-ups could not be applied because of read-only complex objects. \n    \n    \n      Object getFixed() \n      Return a fixed object. In-place fixes may or may not be possible because some objects are immutable. For example, if the schema type is “fixed” and String object is provided as the Data object, the fixed-up object that would be returned will be a ByteString. Since String and ByteString are both immutable and have different types, the fixed object will be a different object, i.e. the fix-up cannot be done in-place. For complex objects, the fix-ups can be applied in place. This is because the new values can replace the old values in a DataMap or DataList. \n    \n    \n      boolean isValid() \n      Whether the fixed object returns by getFixed() contains any errors. If it returns false, then the fixed object and its dependents are fixed up according to the provided schema. \n    \n    \n      String getMessage() \n      Provides details on validation and fix-up failures. Returns empty string if isValid() is true and fix-ups/validation have occurred without problems. \n    \n  \n\n\nNote: Schema validation and coercion are currently explicit operations.\nThey are not implicitly performed when data are de-serialized as part of\nremote invocations.\n\nThe Data Template Layer\n\nThe Data Template layer provides Java type-safe access to the underlying\ndata stored in the Data layer. It has explicit knowledge of the schema\nof the data stored. The code generator generates classes for complex\nschema types that derive from base classes in this layer. The common\nbase of these generated is com.linkedin.data.DataTemplate. Typically,\na DataTemplate instance is an overlay or wrapper for a DataMap or\nDataList instance. It allows type-safe access to the underlying data\nin the DataMap or DataList. (The exception is the FixedTemplate\nwhich is a subclass of DataTemplate for fixed schema types.)\n\nThe Data Template layer provides the following abstract base classes\nthat are used to construct Java bindings for different complex schema\ntypes.\n\n\n  \n    \n      Class\n      Underlying Data \n      Description \n    \n  \n  \n    \n      AbstractArrayTemplate \n      DataList \n      Base class for array types. \n    \n    \n      DirectArrayTemplate \n      DataList \n      Base class for array types containing unwrapped item types, extends AbstractArrayTemplate. \n    \n    \n      WrappingArrayTemplate \n      DataList \n      Base class for array types containing wrapped item types, extends AbstractArrayTemplate. \n    \n    \n      AbstractMapTemplate \n      DataMap \n      Base class for map types. \n    \n    \n      DirectMapTemplate \n      DataMap \n      Base class for map types containing unwrapped value types, extends AbstractMapTemplate. \n    \n    \n      WrappingMapTemplate \n      DataMap \n      Base class for map types containing wrapped value types, extends AbstractMapTemplate. \n    \n    \n      FixedTemplate \n      ByteString \n      Base class for fixed types. \n    \n    \n      RecordTemplate \n      DataMap \n      Base class for record types. \n    \n    \n      ExceptionTemplate \n      DataMap \n      Base class for record types that declared as errors. \n    \n    \n      UnionTemplate \n      DataMap \n      Base class for union types. \n    \n  \n\n\nThe unwrapped schema types are:\n\n\n  int\n  long\n  float\n  double\n  boolean\n  string\n  bytes\n  enum\n\n\nThe wrapped schema types are types whose Java type-safe bindings are not\nthe same as their data type in the Data layer. These types require a\nDataTemplate wrapper to provide type-safe access to the underlying\ndata managed by the Data layer. The wrapped types are:\n\n\n  array\n  map\n  fixed\n  record and error\n  union\n\n\nEnum is an unwrapped type even though its Java type-safe binding is\nnot the same as its storage type in the Data layer. This is because enum\nconversions are done through coercing to and from java.lang.String s\nimplemented by the Data Template layer. This is similar to coercing\nbetween different numeric types also implemented by the Data Template\nlayer.\n\nThe following table shows the relationships among types defined in the\ndata schema, types stored and managed by the Data layer, and the types\nof the Java binding in the Data Template\nlayer.\n\n\n  \n    \n      Schema Type \n      Data Layer \n      Data Template Layer \n    \n  \n  \n    \n      int \n      java.lang.Integer \n      Coerced to java.lang.Integer or int (2) \n    \n    \n      long \n      java.lang.Integer or java.lang.Long (1) \n      Coerced to java.lang.Long or long (2) \n    \n    \n      float \n      java.lang.Integer, java.lang.Long, java.lang.Float or java.lang.Double (1) \n      Coerced to java.lang.Float or float (2) \n    \n    \n      double \n      java.lang.Integer, java.lang.Long, java.lang.Float or java.lang.Double (1) \n      Coerced to java.lang.Double or double (2) \n    \n    \n      boolean \n      java.lang.Boolean \n      Coerced to java.lang.Boolean or boolean (2) \n    \n    \n      string \n      java.lang.String \n      java.lang.String \n    \n    \n      bytes \n      java.lang.String or com.linkedin.data.ByteString (3) \n      com.linkedin.data.ByteString \n    \n    \n      enum \n      java.lang.String \n      Generated enum class. \n    \n    \n      array \n      com.linkedin.data.DataList \n      Generated or built-in array class. \n    \n    \n      map \n      com.linkedin.data.DataMap \n      Generated or built-in map class. \n    \n    \n      fixed \n      java.lang.String or com.linkedin.data.ByteString \n      Generated class that derives from FixedTemplate \n    \n    \n      record \n      com.linkedin.data.DataMap \n      Generated class that derives from RecordTemplate \n    \n    \n      error \n      com.linkedin.data.DataMap \n      Generated class that derives from ExceptionTemplate \n    \n    \n      union \n      com.linkedin.data.DataMap \n      Generated class that derives from UnionTemplate \n    \n  \n\n\n(1) When a JSON object is deserialized, the actual schema type is not\nknown. Typically, the smallest sized type that can represent the\ndeserialized value will be used to store the value in-memory.\n(2) Depending on the method, un-boxed types will be preferred to boxed\ntypes if applicable and the input or output arguments can never be\nnull.\n(3) When a JSON object is deserialized, the actual schema type is not\nknown for bytes and fixed. Values of bytes and fixed types are stored as\nstrings as serialized representation is a string. However, ByteString\nis an equally valid Java type for these schema types.\n",
      tags: null,
      id: 37
    });
    
    

  
    index.add({
      title: "How Data is Serialized for Transport",
      category: null,
      content: "How Data is Serialized for Transport\n\nThe data is serialized for transport using JSON encoding in following\nthe Avro 1.4.1 specification. This JSON encoding is also the same as the\nJSON expression used to describe default values.\n\nOne notable difference from the Avro spec is that optional fields with\nno value are represented by its omission in the serialized data. To\nphrase it differently, optional fields are never explicitly set to\nnull in the serialized body. As such, null is never a valid value\nto appear in the serialized data. The only exception to this rule is if\nthe schema for the data is a union that has a null member.\n\nThe following table summarizes the JSON encoding.\n\n\n\n\nSchema Type \nJSON Type \nJSON Encoding Examples \n\n\n\n\nint \nnumber \n123 \n\n\nlong \nnumber \n123456789000 \n\n\nfloat \nnumber \n3.5 \n\n\ndouble \nnumber \n3.5555555 \n\n\nboolean \ntrue or false \ntrue \n\n\nstring \nstring \n“hello” \n\n\nbytes \nstring (bytes encoded as least significant 8-bits of 16-bit character) \n“\\u00ba\\u00db\\u00ad” \n\n\nenum \nstring \n“APPLE” \n\n\nfixed \nstring (bytes encoded as least significant 8-bits of 16-bit character)\n“\\u0001\\u0002\\u0003\\u0004” (fixed of size 4) \n\n\narray \narray \n[ 1, 2, 3 ] \n\n\nmap \nobject \n{ “a” : 95, “b” : 90, “c” : 85 } \n\n\nrecord (error) \nobject (each field is encoded using a name/value pair in the object) \n{ “intField” : 1, “stringField” : “abc”, “fruitsField” : “APPLE” }\n\n\nunion \nnull if value is null. \nobject if member value is not null with only one name/value pair in the object. The name will be the member discriminator (NOTE Member discriminator will be the member’s alias if one is specified, else it is the member’s fully qualified type name.) and value is the JSON encoded value. \nnull \n{ “int” : 1 } \n{ “float” : 3.5 } \n{ “string” : “abc” } \n{ “array” : { “s1”, “s2”, “s3” } } \n{ “map” : { “key1” : 10, “key2” : 20, “key3” : 30 } } \n{ “com.linkedin.generator.examples.Fruits” : “APPLE” }\n\n\n\n\nIf a union schema has a typeref member, then the key for that member is\nthe dereferenced type. E.g. for union\n\n  {\n    \"name\" : \"unionField\",\n    \"type\" : [\n      \"int\",\n      { \"type\" : \"typeref\", \"name\" : \"a.b.c.d.Foo\", \"ref\"  : \"string\" }\n    ]\n  }\n\n\nthe JSON encoding for the typeref member should look like\n\n{ “string” : “Correct key” }\n\n\nNOT\n\n{ “a.b.c.d.Foo” : “Wrong key” }\n\n\nSimilarly, for a union with aliased members the key for the members will\nbe its corresponding alias. For example,\n\n{\n  \"name\" : \"unionField\",\n  \"type\" : [\n    { \"type\" : \"int\", \"alias\" : \"count\" },\n    { \"type\" : { \"type\" : \"typeref\", \"name\" : \"a.b.c.d.Foo\", \"ref\"  : \"string\" }, \"alias\" : \"foo\" }\n  ]\n}\n\n\nthe JSON encoding for the typeref member should look like\n\n{ “foo” : “Correct key” }\n\nHow to serialize data to JSON\n\nDataMapUtils provides convenience methods to serialize and deserialize\nbetween data and JSON using JacksonDataCodec.\n\nTo serialize from a DataMap to JSON:\n\nDataMap dataMap = new DataMap();\ndataMap.put(\"message\", \"Hi!\");\nbyte[] jsonBytes = DataMapUtils.mapToBytes(dataMap);\nString json = new String(jsonBytes, \"UTF-8\");\n\n\n\nTo serialize from a RecordTemplate instance to JSON:\n\nGreeting greeting = new Greeting().setMessage(\"Hi!\"); // Where Greeting is class extending RecordTemplate\nbyte[] jsonBytes = DataMapUtils.dataTemplateToBytes(greeting, true);\nString json = new String(jsonBytes, \"UTF-8\");\n\n\nHow to Deserialize JSON to Data\n\nTo deserialize from JSON to a DataMap:\n\nInputStream in = IOUtils.toInputStream(\"{'message':'Hi!'}\");\nDataMap dataMap = DataMapUtils.readMap(in);\n\n\nTo deserialize from JSON to a RecordTemplate:\n\nInputStream in = IOUtils.toInputStream(\"{'message':'Hi!'}\");\nGreeting deserialized = DataMapUtils.read(in, Greeting.class); // Where Greeting is class extending RecordTemplate\n\n\nHow to Serialize Data to PSON\n\nPSON is a binary format that can represent any JSON data but is more\ncompact, requires less computation to serialize and deserialize, and can\ntransmit byte strings directly.\n\nPSON serialization/deserialization works similar to JSON (as described\nabove) but uses these two methods:\n\nDataMapUtils.readMapPson()\nDataMapUtils.mapToPsonBytes()\n\n",
      tags: null,
      id: 38
    });
    
    

  

  
    index.add({
      title: "Rest.li - A framework for building RESTful architectures at scale",
      category: null,
      content: "A framework for building RESTful architectures at scale\n",
      tags: null,
      id: 39
    });
    
    

  
    index.add({
      title: "Installation",
      category: null,
      content: "Todo\n",
      tags: null,
      id: 40
    });
    
    

  
    index.add({
      title: "Java Binding",
      category: null,
      content: "Java Binding\n\nContents\n\n\n  Package and Class Names\n  Primitive Types\n  Enum Type\n  Fixed Type\n  Array Type\n  Map Type\n  Record Type\n  Error Type\n  Union Type\n  Custom Java Class Binding for Primitive Types\n  Fields class\n  Clone Method\n  Escaping for Reserved Words\n  Exceptions\n  Running the Code Generator\n\n\nThis section describes the details of the Java classes (dataModels)\ngenerated by the code generator.\n\nPackage and Class Names\n\nThe Java binding determines the package and class names of the generated\nand/or built-in classes using the following rules.\n\n\n\n\nSchema Type \nJava Package and Class Name\n\n\n\n\n\nmaps and arrays of primitive types\n\n\nPackage name is com.linkedin.data.template. \nClass name is computed by appending \"Map\" or \"Array\" to the corresponding boxed type's class name. \nFor multi-dimensional maps and arrays, a \"Map\" or \"Array\" is appended for each dimension starting with the inner most dimension first. \n\nExample Schema\n{ \"type\" : \"map\", \"values\" : \"boolean\" }\n{ \"type\" : \"array\", \"items\" : { \"type\" : \"map\" : \"values\" : \"string\" } }\n\nJava package and class\npackage com.linkedin.data.template;\npublic class BooleanMap extends DirectArrayTemplate&amp;ltBoolean&amp;gt ...\npublic class StringMapArray extends DirectArrayTemplate&amp;ltStringMap&amp;gt ...\n\n\n\n\n\nenum, fixed, record types  (named schema types)\n\n\nPackage name is the package of the named schema type if it is specified, otherwise package name will use the namespace of the named schema type by default. \nClass name is the name of the named schema type. \n\nExample Schema \n\n{ \"type\" : \"record\", \"name\" : \"a.b.c.d.Foo\", \"fields\" : ... }\n{ \"type\" : \"enum\", \"name\" : \"Bar\", \"namespace\" : \"x.y.z\", \"package\": \"x.y.z.test\", symbols\" : ... }\n\nJava package and class\n\npackage a.b.c.d;\npublic class Foo extends RecordTemplate ...\n\npackage x.y.z.test;\npublic enum class Bar ...\n\n\n\n\n\nmaps and arrays of enum, fixed, record \n(maps and arrays of named schema types) \n\n\nPackage name is the package name of the named schema type, which follows the rule documented in this table for named schema. \nClass name is computed by appending \"Map\" or \"Array\" to name of generated class for the named schema type. \nFor multi-dimensional maps and arrays, a \"Map\" or \"Array\" is appended for each dimension starting with the inner most dimension first. \n\nExample Schema \n\n{ \"type\" : \"map\", \"values\" : \"a.b.c.d.Foo\" }\n{ \"type\" : \"map\", \"values\" : { \"type\" : \"array\", \"items\" : \"a.b.c.d.Foo\" } }\n\n{ \"type\" : \"array\", \"items\" : \"x.y.z.Bar\" }\n{ \"type\" : \"array\", \"items\" : { \"type\" : \"map\", \"values\" : \"x.y.z.Bar\" } }\n\n\nJava package and class \n\npackage a.b.c.d;\n \npublic class FooMap extends WrappingMapTemplate&lt;Foo&gt; ...\npublic class FooArrayMap extends WrappingMapTemplate&lt;Foo&amp;gt ...\n \npackage x.y.z.test;\npublic class BarArray extends DirectArrayTemplate&lt;Bar&amp;gt ...\npublic class BarMapArray extends DirectArrayTemplate&lt;Bar&amp;gt ...\n\n\n\n\n\nunions \n\n\nThe name of the union class is determined in two ways. \n\n1. Union without typeref \nIf there is no typeref for the union, the code generator makes up the class name from the name of the closest enclosing field that declared the union type. \nPackage name is the package name of the closest outer record type, which follows the rule documented in this table for that closest outer record type. \nThe generated union class will be declared in the generated class of the closest outer record type. \nClass name will be name of the field in the closest outer record that declared the union with the first character capitalized. \n\nExample Schema \n{\n  \"type\" : \"record\",\n  \"name\" : \"a.b.c.d.Foo\",\n  \"fields\" : [ { \"name\" : \"bar\", \"type\" : [ \"int\", \"string\" ] } ]\n}\n\nJava package and class \npackage a.b.c.d;\npublic class Foo extends RecordTemplate {\n  public class Bar extends UnionTemplate ...\n}\n\n2. Union with typeref \nIf there is a typeref for the union, the code generator will use the name of typeref for the generated union class. \nPackage name is the package of the typeref if it is specified, otherwise package name is the namespace of the typeref by default. \nClass name is the name of the typeref.\n\nExample Schema \n\n{\n  \"type\" : \"typeref\",\n  \"name\" : \"a.b.c.d.Bar\",\n  \"package\" : \"a.b.c.d.test\",\n  \"ref\"  : [ \"int\", \"string\" ] \n}\nJava package and class\n\npackage a.b.c.d.test;\npublic class Bar extends UnionTemplate implements HasTyperefInfo {\n  ...\n  public TyperefInfo typerefInfo() \n  {\n    ... \n  }\n}\n\nWhen the typeref provides the name of the generated union class.   The generated class will also implement the HasTyperefInfo interface.  This interface declares the typerefInfo() method that will be implemented by the generated class.\nTo avoid generating duplicate classes for the duplicate declarations of unions, it is a good practice to declare unions with a typeref when the same union type is used more than once.\n\n\n\n\nmaps and arrays of unions \n\n\nPackage name is the package name of the union, which follows the rule documented in this table for unions.  \nThe generated class will be declared in the same outer class (for unions without typeref) or same package (for unions with typeref) as the generated class for the union. \nClass name is computed by appending \"Map\" or \"Array\" to the name of the generated class for the union. \nFor multi-dimensional maps and arrays, a \"Map\" or \"Array\" is appended for each dimension starting the inner most dimension first. \n\n1. Union without typeref\n\nExample Schema \n\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"a.b.c.d.Foo\",\n  \"fields\" : [\n    { \"name\" : \"members\", \"type\" : { \"type\" : \"array\", \"items\" : [ \"int\", \"string\" ] } }\n    { \"name\" : \"locations\", \"type\" : { \"type\" : \"map\", \"values\" : [ \"int\", \"string\" ]  } }\n  ]\n}\n\n\n\nJava package and class \n\npackage a.b.c.d;\npublic class Foo extends RecordTemplate {\n  public class Members extends UnionTemplate ...\n  public class MembersArray extends WrappingArrayTemplate&lt;Members&gt; ...\n  public class Locations extends UnionTemplate ...\n  public class LocationsMap extends WrappingMapTemplate&lt;Locations&t; ...\n \n  public MembersArray getMembers() ...\n  public LocationsMap getLocations() ...\n}\n\n\n\n2. Union with typeref\nExample Schema \n\n{\n  \"type\" : \"typeref\",\n  \"name\" : \"a.b.c.d.Bar\",\n  \"package\": \"a.b.c.d.test\",\n  \"ref\"  : [ \"int\", \"string\" ] \n}\n \n{\n  \"type\" : \"record\",\n  \"name\" : \"a.b.c.d.Foo\",\n  \"package\" : \"a.b.c.d.test\",\n  \"fields\" : [\n    { \"name\" : \"members\", \"type\" : { \"type\" : \"array\", \"items\" : \"Bar\" } }\n    { \"name\" : \"locations\", \"type\" : { \"type\" : \"map\", \"values\" : \"Bar\" } }\n  ]\n}\nJava package and class \n\npackage a.b.c.d.test;\n \npublic class Bar extends UnionTemplate ...\npublic class BarArray extends WrappingArrayTemplate&lt;Bar&gt; ...\npublic class BarMap extends WrappingMapTemplate&lt;Bar&gt; ...\n \npublic class Foo extends RecordTemplate\n{\n  public BarArray getMembers() ...\n  public BarMap getLocations() ...\n}\n \n\n\n\n\n\nPrimitive Types\n\nThe Java binding for primitive schema types are as follows:\n\n\n  \n    \n      Schema Type\n      Java Type\n    \n  \n  \n    \n      int \n      java.lang.Integer or int (1) \n    \n    \n      long \n      java.lang.Long or long (1) \n    \n    \n      float \n      java.lang.Float or float (1) \n    \n    \n      double \n      java.lang.Double or double (1) \n    \n    \n      boolean \n      java.lang.Boolean or boolean (1) \n    \n    \n      string \n      java.lang.String \n    \n    \n      bytes \n      com.linkedin.data.ByteString \n    \n  \n\n\n(1) Depending on the method, un-boxed types will be preferred to boxed\ntypes if applicable when input or output arguments can never be null.\n\nIn addition to the standard bindings, custom Java class bindings may be\ndefined for these types to specify a user-defined class as substitute\nfor the standard Java class bindings. For additional details, see\nCustom Java Class Binding for Primitive\nTypes.\n\nEnum Type\n\nThe code generator generates a Java enum class. There will be a\ncorresponding symbol in the Java enum class for each symbol in the enum\nschema. In addition, the code generator will add a $UNKNOWN symbol to\nthe generated enum class. $UNKNOWN will be returned if the value\nstored in the Data layer cannot be mapped to a symbol present in the\nJava enum class. For example, this may occur if an enum symbol has been\nadded to a new version of the enum schema and is transmitted to client\nthat has not been updated with the new enum schema.\n\nEnums also supports a symbolDocs attribute to provide documentation for\neach enum symbol. E.g.\n\n...\n\"symbols\" : [ \"APPLE\", \"BANANA\", ... ],\n\"symbolDocs\" : { \"APPLE\":\"A red, yellow or green fruit.\", \"BANANA\":\"A yellow fruit.\", ... } \n...\n\n\n\npackage com.linkedin.pegasus.generator.examples;\n\n...\n/**\n* A fruit\n*\n*/\npublic enum Fruits {\n\n  /**\n   * A red, yellow or green fruit.\n   * \n   */\n  APPLE,\n\n  /**\n   * A yellow fruit.\n   * \n   */\n  BANANA,\n\n  /**\n   * An orange fruit.\n   * \n   */\n  ORANGE,\n\n  /**\n   * A yellow fruit.\n   * \n   */\n  PINEAPPLE,\n  $UNKNOWN;\n}\n\n\nNote: Due to the addition of doclint in JDK8, anything under the\nsymbolDocs attribute must be W3C HTML 4.01 compliant. This is because\nthe contents of this string will appear as Javadocs in the generated\nJava ‘data template’ classes later. Please take this into consideration\nwhen writing your documentation.\n\nFixed Type\n\nThe code generator generates a class that extends\ncom.linkedin.data.template.FixedTemplate. This class provides the\nfollowing\nmethods:\n\n\n  \n    \n      Method \n      Implemented by \n      Description \n    \n  \n  \n    \n      Constructor(String arg) \n      Generated class \n      Construct with an instance whose value is provided by the input string representing the bytes in the fixed. \n    \n    \n      Constructor(Object obj) \n      Generated class \n      Construct with an instance whose value is provided by the input string representing the bytes in the fixed or a ByteString. \n    \n    \n      ByteString bytes() \n      Base class \n      Returns the bytes of the fixed type. \n    \n    \n      FixedDataSchema schema() \n      Generated class \n      Returns the DataSchema of the instance. The size of the fixed type can be obtained from this schema. \n    \n    \n      Object data() \n      Base class \n      Returns the underlying data of the instance. This is the same as bytes(). \n    \n    \n      String toString() \n      Base class \n      Returns the string representation of the bytes in the instance. \n    \n  \n\n\nA fixed instance is immutable once constructed.\n\nArray Type\n\nThe code generator generates a class that extends\ncom.linkedin.data.template.DirectArrayTemplate&lt;E&gt; or\ncom.linkedin.data.template.WrappingArrayTemplate&lt;E extends\nDataTemplate&lt;?&gt;&gt;. The latter is used for item types whose Java binding\nrequire wrapping. The former is used for items types whose Java binding\nthat do not require wrapping. The E generic type variable is the Java\nclass of the array’s item type. By creating a concrete subclass per map\ntype, this binding avoids lost of item type information due to Java\ngenerics type erasure.\n\nThe primary characteristics of both base classes are as follows:\n\n\n  They implement java.util.List&lt;E&gt;.\n  Their methods perform runtime checks on updates and inserts to\nensure that the value types of arguments are either the exact type\nspecified by the type variable or types that can be coerced to the\nspecified type.\n\n\nThe methods with more specific behavior are described\nbelow.\n\n\n  \n    \n      Method\n      Implemented by \n      Description \n    \n  \n  \n    \n      Constructor()\n      Generated class \n      Constructs an empty array. \n    \n    \n      Constructor(int initialCapacity)\n      Generated class\n      Constructs an empty array with the specified initial capacity. \n    \n    \n      Constructor(Collection c)\n      Generated class\n      Constructs an array by inserting each element of the provided collection into the constructed array. \n    \n    \n      Constructor(DataList list)\n      Generated class\n      Constructs an array that wraps the provided DataList. \n    \n    \n      ArrayDataSchema schema()\n      Generated class\n      Returns the DataSchema of the instance. The schema of the items in the array can be obtained from this schema.\n    \n    \n      int hashCode() \n      Base class \n      Returns the hashCode() of the underlying DataList wrapped by this instance. \n    \n    \n      boolean equals(Object object) \n      Base class \n      If object is an instance of AbstractArrayTemplate, invoke equals on the underlying DataList of this instance and the object’s underlying DataList. Otherwise, invoke super.equals(object) which is AbstractMap ’s equals method. \n    \n    \n      String toString() \n      Base class \n      Returns the result of calling toString() on the underlying DataList wrapped by this instance. \n    \n    \n      java.util.List methods\n      Base class \n      See java.util.List. \n    \n  \n\n\nMap Type\n\nThe code generator generates a class that extends\ncom.linkedin.data.template.DirectMapTemplate&lt;E&gt; or\ncom.linkedin.data.template.WrappingMapTemplate&lt;E extends\nDataTemplate&lt;?&gt;&gt;. The latter is used for item types whose Java binding\nrequire wrapping. The former is used for items types whose Java binding\nthat do not require wrapping. The E generic type variable is the Java\nclass of the map’s value type. By creating a concrete subclass per map\ntype, this binding avoids lost of value type information due to Java\ngenerics type erasure.\n\nThe primary characteristics of both base classes are as follows:\n\n\n  They implement java.util.Map&lt;String, E&gt;.\n  Their methods perform runtime checks on updates and inserts to\nensure that the value types of arguments are either the exact type\nspecified by the type variable or are types that can be coerced to\nthe specified type.\n\n\nThe methods with somewhat specialized behavior are described\nbelow.\n\n\n  \n    \n      Method\n      Implemented by \n      Description \n    \n  \n  \n    \n      Constructor()\n      Generated class \n      Constructs an empty map. \n    \n    \n      Constructor(int initialCapacity)\n      Generated class\n      Constructs an empty map with the specified initial capacity. \n    \n    \n      Constructor(int initialCapacity, float loadFactor)\n      Generated class\n      Constructs an empty map with the specified initial capacity and load factor. \n    \n    \n      Constructor(Map&lt;String, E&gt; c)\n      Generated class\n      Constructs a map by inserting each entry of the provided map into new instance. \n    \n    \n      Constructor(DataMap map)\n      Generated class\n      Constructs an array that wraps the provided DataMap. \n    \n    \n      MapDataSchema schema()\n      Generated class\n      Returns the DataSchema of the instance. The schema of the values in the map can be obtained from this schema.\n    \n    \n      int hashCode() \n      Base class \n      Returns the hashCode() of the underlying DataMap wrapped by this instance. \n    \n    \n      boolean equals(Object object) \n      Base class \n      If object is an instance of AbstractMapTemplate, invoke equals on the underlying DataMap of this instance and the object’s underlying DataMap. Otherwise, invoke super.equals(object) which is AbstractMap ’s equals method. \n    \n    \n      String toString() \n      Base class \n      Returns the result of calling toString() on the underlying DataMap wrapped by this instance. \n    \n    \n      java.util.Map methods\n      Base class \n      See java.util.Map. \n    \n  \n\n\nRecord Type\n\nThe code generator generates a class that extends\ncom.linkedin.data.template.RecordTemplate. This class provides the\nfollowing\nmethods:\n\n\n  \n    \n      Method \n      Implemented by \n      Description \n    \n  \n  \n    \n      Constructor() \n      Generated class \n      Construct instance that wraps an empty DataMap. Even mandatory fields are not present. \n    \n    \n      Constructor(DataMap map) \n      Generated class\n      Construct instance that wraps the provided DataMap. Method invocations on the RecordTemplate translates to accesses to the underlying DataMap. \n    \n    \n      RecordDataSchema schema() \n      Generated class\n      Returns the DataSchema of this instance. The fields of the record can be obtained from this schema. \n    \n    \n      static Fields fields() \n      Generated class\n      Returns a generated Fields class that provides identifiers for fields of this record and certain nested types. See Fields section below. \n    \n    \n      DataMap data() \n      Base class\n      Returns the underlying DataMap wrapped by this instance. \n    \n    \n      String toString() \n      Base class\n      Equivalent to data().toString(). \n    \n  \n\n\nThe code generator generates the following methods in the generated\nclass for each field. FieldName is the name of field with the first\ncharacter\ncapitalized.\n\n\n  \n    \n      Method\n      Description \n    \n  \n  \n    \n      boolean hasFieldName() \n      Returns whether the field is present in the underlying DataMap. \n    \n    \n      void removeFieldName() \n      Removes the field from the underlying DataMap. \n    \n    \n      T getFieldName(GetMode mode) \n      Returns the value of the field. The mode parameter allows the client to specify the desired behavior if the field is not present. T is the Java type of the field.\n    \n    \n      T getFieldName() \n      Returns the value of the field. This is equivalent to getFieldName(GetMode.STRICT). T is the Java type of the field. \n    \n    \n      R setFieldName(T value, SetMode mode)\n      Sets the specified value into the field. The mode parameter allows the client to specify the desired behavior if the provided value is null. Returns this. R is the generated Java class. T is the Java type of the field. \n    \n    \n      R setFieldName(T value) \n      Sets the specified value into the field. This is equivalent to setFieldName(value, SetMode.DISALLOW_NULL). Returns this. R is the generated Java class. T is the native type rather than the corresponding boxed type where applicable, e.g. int instead of Integer. \n    \n  \n\n\n\n\nGetMode\n\nWhen getting a field from a record, the caller must specify the behavior\nof the function in case the requested field does not exist in the\nrecord.\n\nThe available GetModes are:\n\n\n  NULL\nIf the field is present, then return the value of the field. If the\nfield is not present, then return null (even if there is a default\nvalue).\n  DEFAULT\nIf the field is present, then return the value of the field. If the\nfield is not present and there is a default value, then return the\ndefault value. If the field is not present and there is no default\nvalue, then return null.\n  STRICT\nIf the field is present, then return the value of the field.\nIf the field is not present and the field has a default value, then\nreturn the default value.\nIf the field is not present and the field is not optional, then\nthrow\ncom.linkedin.data.template.RequiredFieldNotPresentException.\nIf the field is not present and the field is optional, then return\nnull.\n\n\nSetMode\n\nWhen setting a field in a record, the caller must specify the behavior\nof the function in case the field is attempted to be set to null.\n\nThe available SetModes are:\n\n\n  IGNORE_NULL\nIf the provided value is null, then do nothing i.e. the value of the\nfield is not changed. The field may or may be present.\n  REMOVE_IF_NULL\nIf the provided value is null, then remove the field. This occurs\nregardless of whether the field is optional.\n  REMOVE_OPTIONAL_IF_NULL\nIf the provided value is null and the field is an optional field,\nthen remove the field. If the provided value is null and the field\nis a mandatory field, then throw\njava.lang.IllegalArgumentException.\n  DISALLOW_NULL\nThe provided value cannot be null. If the provided value is null,\nthen throw java.lang.NullPointerException.\n\n\n\n\npackage com.linkedin.pegasus.generator.examples;\n\n...\n\npublic class Foo extends RecordTemplate\n{\n    public Foo() ...\n    public Foo(DataMap data) ...\n    ...\n\n    // intField - field of int type\n    public boolean hasIntField() ...\n    public void removeIntField() ...\n    public Integer getIntField(GetMode mode) ...\n    public Integer getIntField() { return getIntField(GetMode.STRICT); }\n    public Foo setIntField(int value) { ... ; return this; }\n    public Foo setIntField(Integer value, SetMode mode { ... ; return this; }\n    ...\n\n    // bytesField - field of bytes, Java binding for bytes is ByteString\n    public boolean hasBytesField() ...\n    public void removeBytesField() ...\n    public ByteString getBytesField(GetMode mode) { return getBytesField(GetMode.STRICT); }\n    public ByteString getBytesField() ...\n    public Foo setBytesField(ByteString value) { ... ; return this; }\n    public Foo setBytesField(ByteString value, SetMode mode) { ... ; return this; }\n    ...\n\n    // fruitsField - field of enum\n    public boolean hasFruitsField() ...\n    public void removeFruitsField() ...\n    public Fruits getFruitsField(GetMode mode) ...\n    public Fruits getFruitsField() { return getFruitsField(GetMode.STRICT); }\n    public Foo setFruitsField(Fruits value) { ... ; return this; }\n    public Foo setFruitsField(Fruits value, SetMode mode) { ... ; return this; }\n    ...\n\n    // intArrayField - field of { \"type\" : \"array\", \"items\" : \"int\" }\n    public boolean hasIntArrayField() ...\n    public void removeIntArrayField() ...\n    public IntegerArray getIntArrayField(GetMode mode) ...\n    public IntegerArray getIntArrayField() { return getIntArrayField(GetMode.STRICT); }\n    public Foo setIntArrayField(IntegerArray value) { ... ; return this; }\n    public Foo setIntArrayField(IntegerArray value, SetMode mode) { ... ; return this; }\n\n    // stringMapField - field of { \"type\" : \"map\", \"values\" : \"string\" }\n    public boolean hasStringMapField() ...\n    public void removeStringMapField() ...\n    public StringMap getStringMapField(GetMode mode) ...\n    public StringMap getStringMapField() { return getIntStringMapField(GetMode.STRICT); }\n    public Foo setStringMapField(StringMap value) { ... ; return this; }\n    public Foo setStringMapField(StringMap value, SetMode mode) { ... ; return this; }\n    ...\n\n    // unionField - field of union\n    public boolean hasUnionField() ...\n    public void removeUnionField() ...\n    public Foo.UnionField getUnionField(GetMode mode) ...\n    public Foo.UnionField getUnionField() { return getUnionField(GetMode.STRICT); }\n    public Foo setUnionField(Foo.UnionField value) { ... ; return this; }\n    public Foo setUnionField(Foo.UnionField value, SetMode mode) { ... ; return this; }\n\n    // get fields\n    public static Foo.Fields fields() {\n        ...;\n    }\n\n    public static class Fields\n        extends PathSpec\n    {\n        ...\n        public PathSpec intField() { ... }\n        public PathSpec longField() { ... }\n        public PathSpec bytesField() { ... }\n        public PathSpec fruitsField() { ... }\n        public PathSpec intArrayField() { ... }\n        public PathSpec stringMapField() { ... }\n        public Foo.UnionField.Fields unionField() { ... }\n    }\n}\n\n\nError Type\n\nError types are specialized record types. The code generator generates a\nclass that extends com.linkedin.data.template.ExceptionTemplate. The\ngenerated class has the same methods as a generated class for a record\nwith the same fields. Unlike RecordTemplate instances,\nExceptionTemplate instances can be thrown and caught.\nExceptionTemplate extends java.lang.Exception.\n\nUnion Type\n\nThe code generator generates a class that extends\ncom.linkedin.data.template.UnionTemplate. This class provides the\nfollowing methods:\n\n\n  \n    \n      Method \n      Implemented by \n      Description \n    \n  \n  \n    \n      Constructor() \n      Generated class \n      Construct a union with null as it value. An instance with null as its value cannot be assigned another value. \n    \n    \n      Constructor(DataMap map) \n      Generated class\n      If the argument is null or Data.NULL, then construct a union with a null value. If the argument is not null, then construct a union whose value is provided by the DataMap. Method invocations on the UnionTemplate translates to accesses to the underlying DataMap. An instance with null as its value cannot be assigned another value. An instance that has a non-null value cannot be later assigned a null value. Note: This limitation is because the underlying data types that back the union for null verus non-null values are different. For non-null values, the underlying data type is a DataMap. For null values, the underlying data type is a string. \n    \n    \n      UnionDataSchema schema() \n      Generated class\n      Returns the DataSchema of the instance. The members of the union can be obtained from this schema. \n    \n    \n      DataScheme memberType() \n      Base class \n      Returns DataSchemaConstants.NULL_TYPE if the union has a null value, else return the schema for the value. If the schema cannot be determined, then throw TemplateOutputCastException. The schema cannot be determined if the content of the underlying DataMap cannot be resolved to a known member type of the union schema. See serialization format for details. This exception is thrown if the DataMap has more than one entry and the key of the only entry does not identify one of the member types of the union. \n    \n    \n      boolean memberIs(String key) \n      Base class \n      Returns whether the union member key of the current value is equal the specified key. The type of the current value is identified by the specified key if the underlying DataMap has a single entry and the entry’s key equals the specified key. \n    \n    \n      boolean isNull() \n      Base class \n      Returns whether the value of the union is null. \n    \n    \n      Object data() \n      Base class\n      Returns Data.NULL if the union has a null value, else return the underlying DataMap fronted by the instance. \n    \n    \n      String toString() \n      Base class\n      Equivalent to data().toString(). \n    \n  \n\n\nThe code generator generates the following methods in the generated\nclass for each member type of the union. In the following table,\nMemberKey is either the member’s alias (if specified) or the member’s\nnon-fully qualified type name with the first character\ncapitalized.\n\n\n  \n    \n      Method\n      Description \n    \n  \n  \n    \n      U createWithMemberKey(T value) \n      Create a union instance with the specified value for the member identified by MemberKey. \n    \n    \n      boolean isMemberKey() \n      Returns whether the value of the union is of the member identified by the MemberKey. \n    \n    \n      T getMemberKey() \n      Returns the value of the union if it is for the member identified by MemberKey. T is the Java type of the value and if the current value is not of this type, then throw TemplateOutputCastException. \n    \n    \n      void setMemberKey(T value) \n      Sets the specified value into the union. \n    \n  \n\n\nHere is an example generated class for a union who’s members are not\naliased.\n\npackage com.linkedin.pegasus.examples;\n\n...\npublic class Foo extends RecordTemplate\n{\n  ...\n  public final static class UnionField extends UnionTemplate\n  {\n    public UnionField() ...\n    public UnionField(Object data) ...\n\n    // int value\n    public boolean isInt() ...\n    public Integer getInt() ...\n    public void setInt(Integer value) ...\n\n    // string value\n    public boolean isString() ...\n    public String getString() ...\n    public void setString(String value) ...\n\n    // com.linkedin.pegasus.generator.examples.Fruits enum value\n    public boolean isFruits() ...\n    public Fruits getFruits() ...\n    public void setFruits(Fruits value) ...\n\n    // com.linkedin.pegasus.generator.examples.Foo record value\n    public boolean isFoo() ...\n    public Foo getFoo() ...\n    public void setFoo(Foo value) ...\n\n    // array value ({ \"type\" : \"array\", \"items\" : \"string\" })\n    public boolean isArray() ...\n    public StringArray getArray() ...\n    public void setArray(StringArray value) ...\n\n    // map value ({ \"type\" : \"map\", \"values\" : \"long\" })\n    public boolean isMap() ...\n    public LongMap getMap() ...\n    public void setMap(LongMap value) ...\n  }\n\n  public static class Fields extends PathSpec\n  {\n    ...\n    public Foo.Fields Foo() { ... }\n  }\n}\n\n\nFor a union who’s members are aliased, the generated methods will use\nthe alias instead of the member’s type name like illustrated below.\n\npackage com.linkedin.pegasus.examples;\n\n...\npublic class Foo extends RecordTemplate\n{\n  ...\n  public final static class UnionField extends UnionTemplate\n  {\n    public UnionField() ...\n    public UnionField(Object data) ...\n\n    // int with alias ({ \"type\" : \"int\", \"alias\" : \"count\" })\n    public UnionField createWithCount(Integer value) ...\n    public boolean isCount() ...\n    public Integer getCount() ...\n    public void setCount(Integer value) ...\n\n    // string with alias ({ \"type\" : \"string\", \"alias\" : \"message\" })\n    public UnionField createWithMessage(String value) ...\n    public boolean isMessage() ...\n    public String getMessage() ...\n    public void setMessage(String value) ...\n\n    // another string with alias ({ \"type\" : \"string\", \"alias\" : \"greeting\" })\n    public UnionField createWithGreeting(String value) ...\n    public boolean isGreeting() ...\n    public String getGreeting() ...\n    public void setGreeting(String value) ...\n  }\n\n  public static class Fields extends PathSpec\n  {\n    ...\n    public Foo.Fields Foo() { ... }\n  }\n}\n\n\nCustom Java Class Binding for Primitive Types\n\nA typeref can also be used to define a custom Java class binding for a\nprimitive type. The primary intended use is to provide a more developer\nfriendly experience by having the framework perform conversions from\nprimitive type to a more friendly Java class that can implement methods\nfor manipulating the underlying primitive data. Custom Java class\nbinding also provides additional type-safety by allowing typerefs of the\nsame primitive type to be bound to different custom Java classes. This\nenables compile time type-checking to disambiguate typeref’s, e.g. a Urn\ntyperef to a string can be bound to a different Java class than a\nFileName typeref to a string.\n\nWhen a typeref has a custom Java binding, the generated Java data\ntemplates that reference this type will accept and return parameters of\nthe custom Java class instead of standard Java class for the primitive\ntype. The value stored in the underlying DataMap or DataList will always\nbe of the corresponding primitive Java type (not the custom Java type.)\n\nA custom Java class binding is declared by:\n\n\n  defining a typeref of the primitive type,\n  adding a “java” attribute whose value is a map to the typeref\ndeclaration,\n  adding a “class” attribute to the “java” map whose value is a string\nthat identifies the name of custom Java class.\n\n\nA custom class must meet the following requirements:\n\n\n  Instances of the custom class must be immutable.\n  A coercer must be defined that can coerce the primitive Java class\nof the type to the custom Java class of the type, in both the input\nand output directions. The coercer implements the DirectCoercer\ninterface.\n  An instance of the coercer must be registered with the data template\nframework.\n\n\n\n\n{\n  \"type\" : \"typeref\",\n  \"name\" : \"CustomPoint\",\n  \"ref\"  : \"string\",\n  \"java\" : {\n    \"class\" : \"CustomPoint\"\n  }\n}\n\n\n//\n// The custom class\n// It has to be immutable.\n//\npublic class CustomPoint\n{\n  private int _x;\n  private int _y;\n\n  public CustomPoint(String s)\n  {\n    String parts[] = s.split(\",\");\n    _x = Integer.parseInt(parts\"0\":0);\n    _y = Integer.parseInt(parts\"1\":1);\n  }\n\n  public CustomPoint(int x, int y)\n  {\n    _x = x;\n    _y = y;\n  }\n\n  public int getX()\n  {\n    return _x;\n  }\n\n  public int getY()\n  {\n    return _y;\n  }\n\n  // Implement equals, hashCode, toString, ...\n\n  //\n  // The custom class's DirectCoercer.\n  //\n  public static class CustomPointCoercer implements DirectCoercer&lt;CustomPoint&gt;\n  {\n    @Override\n    public Object coerceInput(CustomPoint object)\n      throws ClassCastException\n    {\n      return object.toString();\n    }\n\n    @Override\n    public CustomPoint coerceOutput(Object object)\n      throws TemplateOutputCastException\n    {\n      if (object instanceof String == false)\n      {\n        throw new TemplateOutputCastException(\"Output \" + object + \n                                              \" is not a string, and cannot be coerced to \" + \n                                              CustomPoint.class.getName());\n      }\n      return new CustomPoint((String) object);\n    }\n  }\n\n  //\n  // Automatically register Java custom class and its coercer.\n  //\n  static\n  {\n    Custom.registerCoercer(CustomPoint.class, new CustomPointCoercer());\n  }\n}\n\n\nFields class\n\nThe code generator also generates a Fields class within the generated\nclass for certain complex types. The primary use case for the Fields\nclass is to provide a type-safe way to refer or identify a field within\na record or a member of a union. The Fields class of a record is\naccessed through the generated fields() method of the generated class\nfor a record. Only record types have the generated fields() method.\n\nIf there are nested complex types, a path to a particular nested field\nmay be obtained by chaining method invocations on Fields classes along\nthe path to the field, e.g. Foo.fields().barField().bazField(). The\npath may be used to specify the nested fields to return from a resource\nrequest, i.e. deep projection.\n\nThe following table summarizes which complex types will have a generated\nFields class and the content of the Fields\nclass.\n\n\n  \n    \n      Complex type \n      Whether type will have a generated Fields class \n      Content of generated Fields class \n    \n  \n  \n    \n      record \n      A Fields class always generated. \n      A method returning a PathSpec will be generated for each field of the record. \n    \n    \n      union \n      A Fields class always generated. \n      A method returning a PathSpec will be generated for each member of the union. \n    \n    \n      array \n      A Fields class will be generated if the array directly or indirectly contains a nested record or union. \n      An items() method returning a PathSpec will be generated for the array. \n    \n    \n      map \n      A Fields class will be generated if the map directly or indirectly contains a nested record or union. \n      A values() method returning a PathSpec will be generated for the map.\n    \n  \n\n\nClone Method\n\nFor the classes that wrap DataMap or DataList, their clone method\nwill clone the underlying DataMap or DataList and then create and\nreturn a new DataTemplate of the same class to wrap the clone. This\nclone operation performs a shallow copy.\n\nEscaping for Reserved Words\n\nWhen symbols such as schema names or enum symbol names are the same as\none of the reserved words in Java, the code generator will escape these\nsymbols by appending an underscore (“_”) to the name to obtain the Java\nname of the symbol.\n\nExceptions\n\nThe Data layer can throw two exceptions:\n\n\n  java.lang.ClassCastException - This exception is thrown if the\ninput argument to a method is not the expected type, cannot be cast\nor coerced to the expected type.\n  com.linkedin.data.TemplateOutputCastException - This exception if\nthe underlying data cannot be wrapped, cast or coerced to the type\nto type of the output argument.\n\n\n\n\nRunning the Code Generator\n\nThe code generator that generates the Java bindings is the\ncom.linkedin.pegasus.generator.PegasusDataTemplateGenerator class.\n\nThe arguments to the main method of this class are targetDirectoryPath\n[sourceFile or schemaName]+”.\n\n\n  targetDirectoryPath provides the root of the output directory for\nJava source files generated by the code generator. The output\ndirectory structure will follow the Java convention, with Java\nsource files residing in sub-directories corresponding to the\npackage name of the classes in the Java source files.\n  sourceFile provides the name of a file. Files containing schemas\nshould have an .pdsc extension. Although a file name provided as\nan argument to the code generator need not end with .pdsc, only\n.pdsc files will be read by the schema resolver when trying to\nresolve a name to schema. Java type: String.\n  schemaName provides the fully qualified name of a schema. The\nschema resolver computes a relative path name from this argument and\nenumerate through resolver paths to locate a file with this relative\nname. If a file is found, the code generator will parse the file\nlooking for a schema with the specified name. Java type: String[].\n\n\nThe resolver path is provided by the “generator.resolver.path” property.\nIts format is the same as the format for Java classpath. Each path is\nseparated by a colon (“:”). Only file system directory paths may be\nspecified (i.e. the resolver does not comprehend .jar files in the\nresolver path.). You can set this in java by System.setProperty().\n\nThe dependencies of the code generator are:\n\n\n  com.sun.codemodel:codemodel:2.2\n  org.codehaus.jackson-core-asl:jackson-core-asl:1.4.0\n  com.linkedin.pegasus:cow\n  com.linkedin.pegasus:r2\n  com.linkedin.pegasus:generator\n\n\nRunning the Code Generator from Command Line\n\nRunning the Code Generator with Gradle\n\nA dataTemplate.gradle script is available in the build_script/\ndirectory of pegasus. To use it, add the script to your project, then\nadd this to your build.gradle file:\n\napply from: \"${buildScriptDirPath}/dataTemplate.gradle\"\n\n\nand put the .pdl files in a directory structure of the form:\n‘src/\\&lt;sourceset\\&gt;/pegasus’, where typically it would be\nsrc/main/pegasus. The plugin is set to trigger on this sort of\ndirectory structure and have the files laid out like a java source tree,\nie if the namespace of my foo schema is “com.linkedin.foo.rest.api”, the\nfile would be located at\nsrc/main/pegasus/com/linkedin/foo/rest/api/Foo.pdl. See\nrestli-example-api/build.gradle in the Pegasus codebase for an\nexample. This script will generate the required Java classes before the\ncompileJava task, so that other classes can refer to it.\n\nNote this will only generate the data templates, but further steps will\nbe needed to generate the rest.li IDL and the clientModel.\n",
      tags: null,
      id: 41
    });
    
    

  

  
    index.add({
      title: "Configure Max Batch Size in Java",
      category: null,
      content: "Configure Max Batch Size in Java\nThis page describes how to set up max batch size limitation for resource batch methods in Java.\n\nContents\n\n  Define Max Batch Size\n  Batch Size Validation\n  Backward Compatibility Rules\n\n\nFor Rest.li resources, we provide infrastructure support that enables users to define the @MaxBatchSize annotation on the batch methods and provides the opt-in batch size validation based on the defined @MaxBatchSize.\n\nDefine Max Batch Size\n\nUsers can use @MaxBatchSize annotation to define the max batch size on Rest.li batch methods.\n\n@MaxBatchSize(value = 100, validate = true)\npublic BatchResult&lt;Long, Photo&gt; batchGet(Set&lt;Long&gt; ids)\n{\n // Logic here...\n}\n\n\nThe @MaxBatchSize annotation contains two elements: value and validate.\nvalue is an integer specifying the max batch size.\nvalidate is a boolean flag for validating the request batch size, which is an optional field. If the validate is not provided, the value of validate is false by default, which means there is no request batch size validation.\n\nList of methods which can use @MaxBatchSize annotation: BATCH_GET, BATCH_UPDATE, BATCH_PARTIAL_UPDATE, BATCH_DELETE, BATCH_CREATE and BATCH_FINDER.\n\nNote: for BATCH_FINDER, the batch size means the number of criteria, not the collection size for each criteria.\n\nDocumenting max batch size information in the IDL.\nOnce the @MaxBatchSize annotation is defined, the batch size information will be exposed in the IDL(restspec.json), for example:\n\n{ \"name\" : \"someResource\",\n   ...\n  \"collection\" : {\n\t\"methods\" : [ {\n      \"method\" : \"batch_get\",\n      \"maxBatchSize\" : {\n        \"value\" : 100,\n        \"validate\" : true\n      }\n    }, {\n      \"method\" : \"batch_update\",\n      \"maxBatchSize\" : {\n        \"value\" : 100,\n        \"validate\" : false\n      }\n    } ],\n    ...\n  }\n  ...\n}\n\n\nBatch Size validation\nIf validate in the @MaxBatchSize is specified as true, on the Rest.li server, when it processes the request, it will check the request that has @MaxBatchSize annotation, and compare the batch size of each request with the defined max batch size of such method. If the actual batch size is larger than the defined max batch size, it will fail the request with http status 400.\n\nExample of enabling batch size validation:\n@RestMethod.BatchGet\n@MaxBatchSize(value = 100, validate = true)\npublic Map&lt;Long, Greeting&gt; batchGet(Set&lt;Long&gt; ids)\n{\n // Logic here...\n}\n\n\n\nBatch size validation is an opt-in feafture. If validate in the @MaxBatchSize is specified as false or validate field is not provided, the @MaxBatchSize is used to provide batch size information, the Rest.li server will not take any action based on the value of max batch size.\n\nExamples of disabling batch size validation:\n@RestMethod.BatchDelete\n@MaxBatchSize(value = 50, validate = false)\npublic BatchUpdateResult&lt;Long, Greeting&gt; batchDelete(BatchDeleteRequest&lt;Long, Greeting&gt; deleteRequest)\n{\n // Logic here...\n}\n\n\n\n@RestMethod.BatchPartialUpdate\n@MaxBatchSize(value = 100)\npublic BatchUpdateResult&lt;Long, Greeting&gt; batchUpdate(BatchPatchRequest&lt;Long, Greeting&gt; entityUpdates)\n{\n // Logic here...\n}\n\n\nBackward Compatibility Rules\nMaking changes to the max batch size information in a resource’s IDL has an impact on the\ncompatibility checker.\n\nThe following changes are considered backward compatible:\n\n\n  Adding a @MaxBatchSize annotation with validation disabled.\n  Removing an existing @MaxBatchSize annotation.\n  Increasing the value of max batch size.\n  Decreasing the value of max batch size when validation is disabled.\n  Updating validate value from true to false.\n\n\nThe following changes are considered backward incompatible:\n\n\n  Adding a @MaxBatchSize annotation with validation enabled.\n  Updating validate value from false to true.\n  Decreasing the value of max batch size when validation is enabled.\n\n",
      tags: null,
      id: 42
    });
    
    

  
    index.add({
      title: "Modeling Resources with Rest.li",
      category: null,
      content: "Modeling Resources with Rest.li\n\nContents\n\n\n  Rest.li’s Uniform Interface\n  Rest.li Modeling Tips\n  Common Modeling Challenges\n  Options for Modeling Entity Relationships\n  General Tips\n  When All Else Fails: Non-Uniform Interfaces\n\n\nThis document will describe how to model a resource using Rest.li. Before reading this page, you may want to get a feel for how to write and build code with Rest.li by reading the Tutorial to Create a Server and Client.\n\nRest.li’s Uniform Interface\n\nRest.li is intended to foster the design of Uniform Interfaces. The idea\nof Uniform Interfaces is core to REST because it establishes standard\npatterns and behaviors that allow clients to explore and make sense of\ninterfaces that they have never seen before.\n\nRest.li’s approach to uniform interface is based on REST principles and\ncomprises the following:\n\n\n  Resource Identifiers\n  Self-Descriptive Messages\n  Resource Methods\n  Resource Types\n\n\nResource Identifiers\n\nSome resource types have an identifier, a name by which it can be\naccessed. Rest.li resource identifiers are URIs that follow specific\npatterns.\n\nSelf-Descriptive Messages\n\nRest.li messages are built with a JSON-encoded body, which provides a\nself-describing structure for the data in the message. The Rest.li\nframework provides convenient access to messages using Pegasus\nRecordTemplates.\n\nSince Rest.li is built on HTTP, messages may also contain metadata in\nHTTP headers.\n\nResource Methods\n\nResources are the nouns in the Rest.li world, and Resource Methods are\nthe verbs. Rest.li provides a standard set of Resource Methods that\ndescribe what clients may do with the resources. These verbs are\ndifferent than the HTTP verbs (GET, POST, PUT, DELETE, and so on) but\nmap onto them (CREATE is a POST, UPDATE is a PUT, FIND is a GET).\n\n\n  CREATE is used to create an entity. Create requires that the\nresource identifier is assigned by the server. (HTTP verb: POST)\n  GET is used to read an entity. Get may require the client to provide\nthe resource identifier, depending on the resource type. (HTTP verb:\nGET)\n  UPDATE is used to modify an entity. Update may require the client to\nprovide the resource identifier, depending on the resource type.\n(HTTP verb: PUT)\n  DELETE is used to remove an entity. Delete may require the client to\nprovide the resource identifier, depending on the resource type.\n(HTTP verb: DELETE)\n  FIND is used to search for entities. Find allows the client to\nprovide query parameters for the search. Find methods return a list\nof entities. (HTTP verb: GET)\n\n\nCREATE, GET, UPDATE, DELETE and FIND also have a corresponding batch method.\nFor example, BATCH_GET is used to read multiple entities and requires\nthe client to provide a list of resource identifiers. It returns a map\nfrom identifiers to entities.\n\nResource Types\n\nEach Rest.li resource endpoint is one of the following types:\nCollection, Simple, Association. Additionally, each resource endpoint\nmay be a sub-resource of any other resource. These resources can be implemented with synchronous or asynchronous templates. Asynchronous is recommended if the code implementation is non-blocking.\n\nCollection\n\nCollection is the most frequently used resource type. A collection\nmodels a key/value map of entities. It may be helpful to think of a\ndatabase table with a primary key. Collections have a key type, used for\nidentifying entities in the collection, and a value type used to\nrepresent the entity itself.\n\nCollections can support all of the Rest.li resource methods described\nabove.\n\nSynchronous collections are declared by creating a class that extends\nCollectionResourceTemplate (or ComplexKeyResourceTemplate for complex\nkeys, see below for details).\n\nAsynchronous collections are declared by creating a class that extends CollectionResourceAsyncTemplate or CollectionResourceTaskTemplate.\n\nFor example:\n\n@RestLiCollection(name=“items”, …)  \npublic class ItemsResource extends CollectionResourceTaskTemplate\\&lt;Long,\nItem\\&gt;  \n\n\nDefines an asynchronous resource with a URI of the form:\n\n/items/{longKey}  \n\n\nFor keys with complex hierarchical data structures, use\nComplexKeyResourceTaskTemplate.\n\nFor Example:\n\n@RestLiCollection(name=“widgets”, …)  \npublic class WidgetResource implements extends\nComplexKeyResourceTaskTemplate\\&lt;WidgetKey, EmptyRecord, Widget\\&gt;  \n\n\nDefines a resource with a URI of the\nform:\n\n/widgets/number={number}\\&amp;thing.make={thing.make}\\&amp;thing.model={thing.model}  \n\n\nSimple\n\nA simple resource models a singleton entity in a particular scope.\nSimple resources support the Rest.li resource methods GET, UPDATE and\nDELETE.\n\nSynchronous simple resources are declared by creating a class that extends\nSimpleResourceTemplate.\n\nAsynchronous simple resources are declared by creating a class that extends SimpleResourceAsyncTemplate or SimpleResourceTaskTemplate.\n\nFor example:\n\n@RestLiSimple(name=“selectedItem”, …)  \npublic class SelectedItemResource extends SimpleResourceTaskTemplate&lt;Item&gt;  \n\n\nDefines a resource with a URI of the form:\n\n/selectedItem  \n\n\nAssociation\n\nAssociations are structured like specialized collections but are used\nfor a different modeling purpose. Associations model relationships\nbetween entities. Associations are like mapping tables in a database.\nAssociations have a compound key consisting of multiple partial keys.\nEach partial key references one of the associated entities. Like\ncollections, associations have a value type. However, in the case of\nassociations, the value type is used to model attributes on the\nrelationship between entities.\n\nAssociations support all of the Rest.li resource methods except for\nCreate. Create requires the server to assign the resource identifier,\nwhich is incompatible with the constraint that an association’s partial\nkeys are “foreign” keys from the referenced entities. Instead of using\nCreate, new association relationships are made by using Update, which\nallows the client to provide the resource identifier.\n\nAsynchronous association resources are declared by creating a class that extends AssociationResourceAsyncTemplate or AssociationResourceTaskTemplate.\n\nFor example:\n\n@RestLiAssociation(name=\"myRelations\", assocKeys={`Key(name=“key1”, type=long.class), @Key(name=“key2”, type=long.class)}, …)  \npublic class MyRelationResource extends AssociationResourceTaskTemplate&lt;Relation&gt; { … }  \n\n\nDefines a child resource with a URI of the form:\n\n/myRelations/key1={longKey1}\\&amp;key2={longKey2}  \n\nAsynchronous resources\nAsynchronous Rest.li resources should be used when the downstream implementations are non-blocking. This means that your implementation should not be blocking while waiting for any downstream service. If the downstream service has an asynchronous implementation, please use that.\n\nThere are asynchronous resource templates for Collection, Simple, Association, and ComplexKey resources.\n\nTask templates should be used when the resource implementation will leverage ParSeq Tasks.\n\nAsync templates should be used when the resource implementation will leverage Callbacks.\n\nPlease see Async Server Implementations for more information on implementation.\n\nChild Resources (Sub-resources)\n\nChild resources are resources that are referenced through a parent\nresource. In Rest.li, the resource identifier for a child resource is an\nextension of the resource identifier for the parent resource. This means\nthat the child resource has access to all of the information used to\naccess the parent resource, including entity keys. A common pattern for\nusing child resources is when accessing an element of a collection\nrequires the key of another collection (the parent resource).\n\nFor example:\n\n@RestLiCollection(parent=MyParentResource.class, name=“mySubResources”,…)  \npublic class MySubResource extends CollectionResourceTemplate\\&lt;String,MySub\\&gt; { … }  \n\n\nDefines a child resource with a URI of the form:\n\n/myParentResources/{parentResourceKey}/mySubResources/{stringKey}  \n\n\nResources Customizations\n\nRest.li resources may be customized to handle a variety of use cases.\nThe most common means of customization is to only implement the methods\nappropriate for a particular use case. Rest.li allows you to implement\nas many or as few of the resource methods as you choose. The framework\nwill understand which methods you have implemented, and will advertise\nonly those methods to clients.\n\nHere are some examples.\n\nRead-Only Collections\n\nA read-only collection can be used to expose entities that you do not\nwant your clients to be able to modify. This might be useful if you need\nto expose a view which is derived from other data, or if you need to\nexpose entities whose lifecycle is managed internally by your domain.\n\nRead-Only collections can easily be implemented using Rest.li\nCollections, by only including read methods such as GET, BATCH_GET and\nFIND. By omitting any write operations (CREATE, UPDATE, DELETE) the\ncollection becomes read-only.\n\nNatural Keys\n\nAnother variation on collections is a “natural key” collection, where\nthe identifier for each entity is one of the domain attributes of the\nentity itself. (The alternative is a synthetic key collection, where the\nidentifier is assigned arbitrarily, such as from a sequence number).\n\nNatural key collections are implemented in Rest.li using Collections, by\nomitting the implementation for CREATE, and instead using UPDATE to add\nentities into the Collection. This is because CREATE is used when the\nserver assigns the key for the entity. Although the name may be\ncounter-intuitive, using UPDATE for this case is correct, because it is\nthe resource method with the best defined semantics (PUT this entity\nrepresentation at this location).\n\nComplex Keys\n\nBy extending ComplexKeyResourceTemplate instead of\nCollectionResourceTemplate, a collection may use any complex type (any\ndefined pegasus record type) as a key.\n\nFactories\n\nA “Factory” Resource is used when the representation of an entity is\nassigned by the server using some input information provided by the\nclient. For example, an authentication service might create a Session\nentity given a username and password, but the Session entity is created\ninternally by the service.\n\nTo create a Factory resource in rest.li, use a Collection which only\nimplements CREATE (and optionally DELETE). The value type of the\nCollection should be a Pegasus schema representing the input to the\nfactory method. The key type should be the key for the resulting entity.\n\nNote that a Factory would normally be paired with a Collection (perhaps\na Read-Only Collection) which provides access to the entities\nthemselves.\n\nRest.li Modeling Tips\n\n\n  Start with nouns (“things”) in your domain. These will be your\nresources and entity representations.\n  Think about the keys for each entity — what is the minimal\ninformation you need in order to GET (read) an entity?\n  For any entity that can be accessed with a single key, try creating\na root-level Collection resource. Implement as many of the Rest.li\nmethods as you reasonably can.\n  For a singleton entity, try creating a root-level Simple resource\nand implement the GET, UPDATE and DELETE methods where applicable.\n  For entities that require multiple keys to access,\n    \n      If all but one of the keys belongs to other resources, try\nmodeling as a child resource of another entity.\n      If all of the keys belong to other resources, consider whether\nyour entity is really a relationship between the other entities.\nIf so, try creating an association.\n      If all of the keys are unique to this entity, you may have a\ncollection with a complex key.\n    \n  \n\n\nCommon Modeling Challenges\n\nCommon Key, Different Entity Type\n\nRest.li collections have a single value type that is used for all of the\nresource methods supported by the collection. There are cases where you\nwant to use different entity representations with the same key, for\nexample, the input to CREATE is a different type than that returned by\nGET (See Factories section above). In such cases, you have two options:\n\n\n  Use a union in the Pegasus schema for your value type.\n  Create a different resource for each value type\n\n\nCommon Entity Type, Different Keys\n\nSometimes the same entity can be accessed using different sets of keys.\n\nFor example, Products belong to Companies. They are normally accessed\nefficiently by using both the CompanyId and ProductId, so the Product\ncollection is a child resource of the Company collection. But if there\nis a legacy access path which uses only ProductId, this method cannot be\nmodeled on the Product resource because the client does not have a key\nfor the parent Company.\n\nIn such cases, you have a few options:\n\n\n  Create a separate resource for each access path\n  Use FIND methods for some or all of the access paths\n\n\nOptions for Modeling Entity Relationships\n\nA frequently encountered question is how relationships between entities\nshould be modeled. Suppose you have two objects, A and B - should you\nmodel their relationship by:\n\n\n  Creating an association between A and B?\n  Making B a subresource of A?\n  Making B a field of A?\n\n\nHere are some rules of thumb to help answer this question:\n\nAssociations are preferred when:\n\n\n  A and B have independent lifecycles. For example, they can both\nexist without needing to be related\n  A and B have a many-to-many / bidirectional relationship (you can\nlookup in either direction)\n  You need to store attributes on the relationship itself\n  You need to relate more than two objects: A, B, and C\n  There is more than one type of relationship between A and B\n\n\nFields are preferred (B is a field of A) when:\n\n\n  B exists only as a part (fragment) of A. A is not complete without\nB.\n  B does not have its own independent key\n  B models a value, not an entity. For example, identity is not\nimportant; only the values of attributes matter for equality.\n  There are a small number of B objects related to any given A object,\nsuch that they can all be accessed together.\n  B is usually accessed and modified as part of A\n\n\nSubresources are preferred (B is a subresource of A) when:\n\n\n  B has its own key, which is unique within the scope of A.\n  A’s key is required to access B\n  B is frequently accessed or modified independent of A\n  B is queried as a collection, e.g., using filters, sorts, or\npagination\n  B models an entity that is dependent on A, but which is not a part\nof A\n  Each B relates to exactly one A\n\n\nGeneral Tips\n\n\n  Remember that it’s ok to have multiple separate Resources with the\nsame Pegasus schema as value type, as long as each resource\nrepresents a distinct pattern of interaction. Likewise, it’s okay to\nhave separate Resources which access the same underlying\nimplementation.\n\n\n\n\n\n  Resource identifiers are the key (pun intended) to parent/child\nresource relationships. A resource should be made a child resource\nif and only if it depends on the parent resource’s key. If a\nresource can be accessed without providing the parent’s key, it\nshould not be a child.\n\n\n\n\n\n  Use separate resources for different representations of the same\nunderlying implementation. If you want to provide more than one\nrepresentation (Pegasus schema) of the same information, you can\ncreate a resource for each representation. However, if one\nrepresentation is just a subset of the other, you should use\nRest.li’s field projection support.\n\n\nWhen All Else Fails: Non-Uniform Interfaces\n\nUniform Interfaces give us a powerful way to expose resources to the\nbroadest possible set of clients. However, if you have a requirement\nwhich can’t reasonably be modeled using the uniform interface\nconstructs, Rest.li does provide a loophole to help you. Actions are a\nspecial type of resource method which allow arbitrary RecordTemplate\ntypes as input and output parameters, and which have no constraints on\nsemantics. It should be possible to model any operation as an Action,\nallowing you to fit your special case within Rest.li. However, because\nActions do not conform to the Uniform Interface, actions cannot easily\nbe used by higher-level frameworks like query languages, etc., which may\nbe built on top of Rest.li. You should therefore avoid Actions whenever\nthere is another reasonable option.\n",
      tags: null,
      id: 43
    });
    
    

  
    index.add({
      title: "Rest.Li PathSpecs",
      category: null,
      content: "Rest.li PathSpecs\n\n  Rest.li PathSpecs\n    \n      What is PathSpec\n      Applications\n        \n          Specifying Projections\n          Request Validation\n        \n      \n      PathSpec Syntax in its string form\n        \n          Primitive type fields\n          Record type fields\n          Map and Array type fields\n            \n              Map type\n              Array type\n            \n          \n          Union and UnionArray, Alias and Alias in Unions\n            \n              Union\n              UnionArray\n              Alias\n              Alias in Unions\n            \n          \n          TypeRef and Fixed\n            \n              TypeRef\n              Fixed\n            \n          \n        \n      \n      PathSpec Syntax in its java binded class form\n      More resources and examples\n    \n  \n\n\nWhat is PathSpec\nIn Rest.li framework, PathSpec represents a path to a component within a complex data object. It generates uniform references in hierarchical datamap components. It is an abstract path concept and has its specification. It currently has two concrete forms in Rest.li, but PathSpec itself should not be language-specific.\n\n\n  \n    The PathSpec path can be represented as a string, current example usages of its string form can be found in Validation annotation, where the PathSpec string is used in annotation and the Annotation Reader would interpret it.\n  \n  \n    The PathSpec path also have a binded java class: PathSpec.java. This java class can be passed as arguments in Rest.li Framework. Its Java class form is now mainly used for Projection in Rest.li framework. The PathSpec java class has convenient method to return the correct pathspec string by calling toString(), which returns the string form.\n  \n\n\nFor example:\ngiven a data schema\nrecord User {\n  firstName: string\n  birthday: optional Date\n  isActive: boolean = true\n  address: record Address {\n    state: string\n    zipcode: string\n  }\n}\n\nboth /address/zipcode and new PathSpec(\"addressa\", \"zipcodeb\") are pathspecs referring to the inner zipcode field.\nApplications\nSpecifying Projections\nPathSpec’s java class binding can be used for projection. Users can get PathSpec object that represents the fields in data object. For example, it could be obtained from generated RecordTemplate subclasses using the .fields() method. \nFor example:\nPathSpec pathSpec = Foo.fields().bar();\n\n\nMore concrete examples can be found here in the wiki.\n\nThis capability is provided by Rest.li auto-generated code, as data object representation for data in Pegasus schema should extend RecordTemplate.java, and will define a inner class called “fields”, which extended PathSpec.java. By passing PathSpec object to the reqeust builder, the PathSpec is then used by MaskCreator to create a MaskTree. Thus, PathSpec can be used to control the projection behavior.\n\nRequest Validation\nPathSpec’s string form is used for Request Validation in Rest.li Resource. A string path can be added in annotation such as “CreateOnly” and “ReadOnly”. For example:\n@CreateOnly({\"/id\", \"/EXIF\"})\npublic class PhotoResource extends CollectionResourceTemplate&lt;Long, Photo&gt;\n{\n    // ...\n}\n\n\nThis string is then parsed by RestLiAnnotationReader, and DataSchemaUtil will do corresponding validation against this path within the DataMap.\n\nYou can also invoke RestliDataValidator and pass PathSpec string to it. TestRestLiValidation.java’s testCustomValidatorMap() is such an example\n    ...\n\n    Map&lt;String, List&lt;String&gt;&gt; annotations = new HashMap&lt;&gt;();\n    annotations.put(\"createOnly\", Arrays.asList(\"stringB\", \n        \"intB\", \"UnionFieldWithInlineRecord/com.linkedin.restli.examples.greetings.api.myRecord/foo2\", \"MapWithTyperefs/*/id\"));\n    annotations.put(\"readOnly\", Arrays.asList(\"stringA\", \n        \"intA\", \"UnionFieldWithInlineRecord/com.linkedin.restli.examples.greetings.api.myRecord/foo1\", \"ArrayWithInlineRecord/*/bar1\", \"validationDemoNext/stringB\", \"validationDemoNext/UnionFieldWithInlineRecord\"));\n    ...\n\n    validator = new RestLiDataValidator(annotations, ValidationDemo.class, ResourceMethod.CREATE, validatorClassMap);\n\n\n\nPathSpec Syntax in its string form\n\nEach PathSpec has a corresponding string form. PathSpec has been defined as a path to a component within a complex data object path. This provides a way to traverse the Pegasus data object. The abstract data object, in most case, is a form of DataMap internally in Rest.li framework, but PathSpec should be meaningful for the same data object in other forms, for example, it could provide path reference to a json representing the data object.\n\nThe PathSpec string format is represented by separators('/') and segments in between.\n/demoRecord/innerRecordField/nestedInnerRecordField\n\n\nThe path segment can use the “attribute” syntax to carry meaningful attributes. These attributes added to the string form using ‘?’ and ‘&amp;’ separators. Users can add any attributes but for some types, there are reserved attributes. For example, for array type, one can specify start and count attributes and these two attributes are used in specifying the projections for array.\n/arrayOfIntFieldE?start=0&amp;count=10\n\n\nFor collection types, such as maps and arrays, the path segment could also be replaced by the wildcard. A wildcard means that this segment path string can be replaced by any applicable segment string, for example.\n/mapOfRecordField/*/innerRecordField\n\nAbove examples points to the innerRecordField field of the map value in a map schema. map is a collection schema type, here * wildcarded its keys.\n\nPegasus schema has defined various kinds of types, the full specification about the supported types can be found from the document Rest.li Data Schema And Templates. There are mainly following supported types in Pegasus and following sections list the example PathSpecs.\n\nPrimitive type fields\nPrimitive types includes type such as bytes, string, boolean, double, float, long, int. In the reocrd form, they came with a name to the field in record, so the reference to the primitive types, in most cases are just a PathSpec string which specify the field name of this type.\n\nFor example for a Pegasus pdl schema file as such\nnamespace com.linkedin.pegasus.examples\n\n/**\n * example Pegasus schema of a record containing primitive types\n */\nrecord RecordTest {\n  intField: int\n  intOptionalField: optional int\n  intDefaultField: int = 17\n  intDefaultOptionalField: optional int = 42\n  longField: long\n  floatField: float\n  doubleField: double\n  booleanField: boolean\n  stringField: string\n  bytesField: bytes\n}\n\nThe example PathSpec for above fields in this record example would be\n/initField\n/intOptionalField\n/intDefaultField\n/intDefaultOPtionalField\n/longField\n/floatField\n/doubleField\n/booleanField\n/stringField\n/bytesField\n\n\nRecord type fields\nIf a field in a record is of another record type, in this case  you have “nested field”, then again the reference for the nested path component is the record’s field name,\n\nFor example the above PDL schema example now hava a record field,\n\nnamespace com.linkedin.pegasus.examples\n\n/**\n * example Pegasus schema of a record containing record field\n */\nrecord RecordTest {\n  recordField: RecordBar\n}\n\nAnd that record expands to\nnamespace com.linkedin.pegasus.examples\n\nrecord RecordBar {\n  location: string\n}\n\n\nThen the PathSpec reference to the nested field “location” is\n/recordField/location\n\nMap and Array type fields\nMap and Array are two collection type used in Pagasus.\n\nMap type\nThe map has a key followed by the value referenced. Recall PathSpec’s semantic is a reference to a component within a complex data object. The map entries were referenced by their keys.\n\nFor example, here is a example PathSpec that can be used to traverse the map.\n\n/mapField/&lt;A-Key-In-the-Map&gt;\n\nNote that the above PathSpec is referring to the component key-ed by the key in the map. If the value is of a primitive type, the above PathSpec can be implicitly used to refer to the primitive type value.\n\nIf you want to specifically refer to all the key fields, the syntax is to use the keyword “$key”\n/mapField/$key\n\n\nif the value held in the map is a record type and you want to refer to that record, the PathSpec could be\n\n/mapField/&lt;A-Key-In-the-Map&gt;/&lt;someRecordField&gt;\n\n\nAnother common use case is that you want to use wildcard (represented by symbol “*” ) to “select all keys”, if the interested path is to a field for all entries in this map, then it would be\n\n/mapField/*/&lt;recordField&gt;\n\n\nIt is worth noting that in Pegasus, the map keys are always of string types.\n\nHere is an concrete example of a record, containing a map field, and that map field’s map has value of record type. The example also defines a “recordInlineMap” field for similar demonstration purpose\nnamespace com.linkedin.pegasus.examples\n\n/**\n * a record containing a map field, which holds values of record type\n */\nrecord RecordWithMapFields {\n\n  recordMap: map[string, RecordBar]\n\n  recordInlineMap: map[string, record RecordInMap {\n    f: int\n  }]\n}\n\nThe PathSpec to refer to the record field, and the inline map field, are\n/recordMap/*/location\n/recordInlineMap/*/f\n\n\nArray type\nArray type is somewhat similar to Map type in the sense that the length might not be definite. For example it could be defined as such\nnamespace com.linkedin.pegasus.examples\n\n/**\n * a record containing an array field, which holds values of record type\n */\nrecord RecordWithArrayFields {\n\n  recordArray: array[RecordBar]\n\n  recordInlineArray: array[record RecordInArray {\n    f: int\n  }]\n}\n\nThen similarly if you are interested in some fields only, you can use wildCard to select all elements so you can refer to a field\n/recordArray/*/location\n/recordInlineArray/*/f\n\n\nOne big difference between Array and Map is that in Array, we support reference to the range. It is achieved by using the ‘start’ and ‘count’ attributes.\n\nSay you want to select some elements in within a range in the array, you can do so by using the “start” and “count” attribute,\n/intArray?start=10&amp;count=5\n/recordInlineArray?count=2\n\n\nIt is worth noting that single element indexing in Array’s PathSpec is currently not defined yet. For example indexing the first element of an array is currently not defined. Alternatively this can be achieved by using the range PathSpec.\n/recordArray/0 # This sytanx is not defined yet\n/recordArray?start=0&amp;count=1 # This syntax has been defined\n\n\nUnion and UnionArray, Alias and Alias in Unions\n\nUnion\nThe use case for the PathSpec for union would be a path to one of the types within the union.\n\nHere is an example of a record containa a union field, and that union is with a null type\nnamespace com.linkedin.pegasus.examples\n\n/**\n * a record that has a field contains a union with null type\n */\nrecord UnionExample {\n\n  unionWithNull: union[\n    int,\n    string,\n    bytes,\n    RecordBar,\n    array[string],\n    map[string, long],\n    null\n  ]\n}\n\n\nThe PathSpecs for types within the uinos are\n/unionWithNull/null\n/unionWithNull/int\n/unionWithNull/string\n\n\nUnionArray\nUnion can be used as Array’s item type. A common use case is a reference to all the same  type within an union.\nnamespace com.linkedin.pegasus.examples\n\n/**\n * a record that has a field contains a union array\n */\nrecord UnionArrayExample {\n\n  unionArray: array[union[\n    null,\n    int,\n    string,\n    map[string, string],\n    array[int],\n    RecordBar,\n    FixedMD5\n  ]]\n}\n\n\n/unionArray/*/null\n/unionArray/*/int\n/unionArray/*/string\n/unionArray/*/RecordBar\n\n\nIn this example, the union also contains array and map, and the reference to them would be\n/unionArray/*/map\n/unionArray/*/array\n\nIf the schema defines more than one array or more than one map in the union, they need to have defined alias for each (see Alias usage in next section).\n\nAlias\nAlias are used for refer to types (such as record type) that cannot be directly defined in the field due to same name conflict. Here is an example PDL with name “AliasTest” in “AliasTest.pdl”\nnamespace com.linkedin.pegasus.examples\n\n/**\n * Aliase examples\n */\n@aliases = [\"com.linkedin.pegasus.examples.AliasTest2\"]\nrecord AliasTest {\n  a1: AliasTest\n  a2: AliasTest\n}\n\nHere the AliasTest2 is an alias for another record with same name “AliasTest” in “AliasTest2.pdl”\n\nThen here are the PathSpec can be used\n/a1/a1\n/a1/a2\n/a1/a1/a2\n\n\nAlias in Unions\n\nIt is worth mentioning that in most cases you will want to use alias in Union. For example in Union you can define two arrays with\nnamespace com.linkedin.pegasus.examples\n\nrecord RecordWithAliasedUnion {\n\n  result: union[\n    message: string,\n\n    successResults: array[string],\n\n    failureResults: array[string]\n  ]\n\n  unionArray: array[union[\n    null,\n\n    successResults: array[string],\n\n    failureResults: array[string]\n  ]]\n}\n\nThen the PathSpec will be\n/result/successResults\n/result/failureResults\n/unionArray/*/successResults\n/unionArray/*/failureResults\n\n\nTypeRef and Fixed\nThe reference to TypeRef and fixed are following a similar rule: Use its name in the field.\n\nTypeRef\nThe “TypeRef” itself can be treated as just another type, so should use field name to refer to its path.\n\nFor example\nnamespace com.linkedin.pegasus.examples\n\nrecord TyperefTest {\n\n  intRefField: optional typeref IntRef = int\n  intRefField2: IntRef\n\n  bar1: typeref RecordBarRef = RecordBar\n  bar2: RecordBarRef\n\n  barRefMap: map[string, RecordBarRef]\n}\n\n\nThe paths to the example integer fields here (used TypeRef) are\n/intRefField\n/intRefField2\n\nAnd for Record field reference within TypeRefed field\n/bar1/location\n/bar2/location\n\nIn the Map type example\n/barRefMap/*/location\n\n\nFixed#\nFixed type can be defined in a separate file and then refered in another schema by name, for example in schema files:\nnamespace com.linkedin.pegasus.example\n\nfixed FixedMD5 16\n\nThen FixedMD5 can be used as a type.\nnamespace com.linkedin.pegasus.examples\n\n/**\n * a record containing a map field, which holds values of union of fix type and inline fix type\n */\nrecord RecordExampleForFixType {\n\n  unionMap: map[string, union[fixed InlineFixedField 1, FixedMD5]]\n}\n\nAs above example shows, when fixed type are defined inline, they will still have field name, so the PathSpec reference to it uses this name, therefore below are PathSpec to the types in the UnionMap defined in this record\n/unionMap/*/InlineFixedField\n/unionMap/*/FixedMD5\n\n\nPathSpec Syntax in its java binded class form\nAll auto-generated RecordTemplate class has a static nested class Fields which extends PathSpec. To find out, after you build the Rest.li project, you can check such RecordTemplate classes in GeneratedDataTemplate folder and to find following codes.\n  public static class Fields extends PathSpec\n  {\n    ....\n  }\n\nCheck example code here.\n\nTherefore it is very easy to get the PathSpec java binded class. Let’s say you have a Foo schema which has bar fields. You can get the PathSpec by following\nPathSpec pathSpec = Foo.fields().bar();\n\nThis has also been documented in How to Use Projections in Java\n\nMore resources and examples\nMore example can be referred from our Rest.li Framework test code example. TestPathSpec.java is a very uesful file that shows and tests what string should look like for the fields defined.\n",
      tags: null,
      id: 44
    });
    
    

  
    index.add({
      title: "Migrating from PDSC to PDL",
      category: null,
      content: "Migrating from PDSC to PDL\n\nGuide for migrating from PDSC to PDL with a side-by-side comparison showing notable differences.\n\nContents\n\n  Why Use PDL?\n  How to Convert Schemas to PDL\n  Notable Differences Between PDSC and PDL\n    \n      PDL is More Java-Like\n      Import Statements\n      Shorthand for Custom Properties\n      Cleaner Enum Declarations\n      Specifying Default Field Values\n    \n  \n\n\nWhy Use PDL?\n\nPDL as a schema definition language was designed to be read and written by humans, making it a much more enjoyable experience for developers\nto define their service’s models. PDSC, despite its syntax being simply a subset of JSON, cannot boast this human-readability factor.\n\nIn addition to its inherently more readable syntax, PDL also features some extra shorthand which developers can leverage to write less and\nmake their schemas more readable for others. See below for more information on this.\n\nHow to Convert Schemas to PDL\n\nIf you have an existing project with PDSC schemas, converting everything to PDL is pretty straightforward.\nRest.li’s Gradle plugin provides a task convert&lt;sourceSet&gt;ToPdl which will automatically convert your schemas for you.\nIt should be noted that this task verifies the converted schemas against the original schemas.\nIf this verification fails, then the whole conversion will be aborted.\n\nThe minimum required version for running this task is 28.1.3, though it’s always recommended to use the latest version\nso that all bug fixes and improvements are picked up.\n\nThe following command will convert every PDSC schema in your project to PDL.\n\ngradle convertToPdl\n\n\nYou can selectively migrate only one particular module of your project with the following:\n\ngradle :&lt;moduleName&gt;:convertToPdl\n\n\nThis task also takes in a few options:\n\ngradle :&lt;moduleName&gt;:convertToPdl \\\n    [-PconvertToPdl.reverse=(true|false)] \\\n    [-PconvertToPdl.keepOriginal=(true|false)] \\\n    [-PconvertToPdl.preserveSrcCmd]\n\n\n\n  \n    \n      Property\n      Type\n      Description\n    \n  \n  \n    \n      convertToPdl.reverse\n      boolean\n      If true, converts PDL schemas back to PDSC (and vice versa if false).\n    \n    \n      convertToPdl.keepOriginal\n      boolean\n      If true, keeps the source schemas (the source schemas are deleted by default).\n    \n    \n      convertToPdl.preserveSrcCmd\n      string\n      Command which is run for each file, useful for running special VCS logic. The command should be a template string containing $src and $dst as references to the source and destination filename, respectively (e.g. gradle convertToPdl -PconvertToPdl.preserveSrcCmd \"/usr/bin/svn mv $src $dst\").\n    \n  \n\n\nNotable Differences Between PDSC and PDL\n\nYou can find in-depth documentation on PDSC syntax and PDL syntax elsewhere,\nbut this section will point out notable differences between PDSC and PDL.\n\nPDL is More Java-Like\n\nPDL is arguably much more human-readable than PDSC because of its Java-like syntax.\nWhereas reading and writing PDSC is like reading and writing plain JSON, reading and\nwriting PDL is like reading and writing a Java interface definition.\n\n\n  \n    PDSC\n    PDL\n  \n  \n    \n\n{\n  \"type\" : \"record\",\n  \"name\" : \"Foo\",\n  \"namespace\" : \"com.example.models\",\n  \"doc\" : \"Foo is a record.\",\n  \"fields\" : [ {\n    \"name\" : \"x\",\n    \"type\" : \"int\",\n    \"default\" : 1\n  }, {\n    \"name\" : \"y\",\n    \"type\" : {\n      \"type\" : \"map\",\n      \"values\" : \"long\"\n    },\n    \"doc\" : \"y is a field.\",\n    \"optional\" : true\n  }, {\n    \"name\" : \"z\",\n    \"type\" : {\n      \"type\" : \"enum\",\n      \"name\" : \"Fruits\",\n      \"symbols\" : [ \"APPLE\", \"BANANA\", \"PEAR\" ]\n    },\n    \"default\" : \"PEAR\",\n    \"someAnnotation\" : 123\n  } ]\n}\n\n    \n    \n\nnamespace com.example.models\n\n/**\n * Foo is a record.\n */\nrecord Foo {\n  x: int = 1,\n\n  /**\n   * y is a field.\n   */\n  y: optional map[string, long],\n\n  @someAnnotation = 123\n  z: enum Fruits{ APPLE, BANANA, PEAR } = \"PEAR\"\n}\n\n    \n  \n\n\nImport Statements\n\nIn PDSC, all references to types outside the schema’s own namespace have to be written as fully-qualified type names.\nPDL, on the other hand, features imports statements (similar to those in Java) which allow the user to specify types\nthat can be referenced by their simple name rather than their full name. This can help to reduce the amount of\nredundant data written in schemas that refer to the same type numerous times.\n\n\n  \n    PDSC\n    PDL\n  \n  \n    \n\n{\n  \"namespace\": \"com.example.models\",\n  \"type\": \"record\",\n  \"name\": \"Redundancies\",\n  \"doc\": \"Imports help to reduce redundant FQNs.\",\n  \"fields\": [ {\n    \"name\": \"a\",\n    \"type\": \"org.external.types.SomeType\"\n  }, {\n    \"name\": \"b\",\n    \"type\": \"org.external.types.SomeType\"\n  }, {\n    \"name\": \"c\",\n    \"type\": {\n      \"type\": \"array\",\n      \"items\": \"org.external.types.SomeType\"\n    }\n  } ]\n}\n\n    \n    \n\nnamespace com.example.models\n\nimport org.external.types.SomeType\n\n/**\n * Imports help to reduce redundant FQNs.\n */\nrecord Redundancies {\n  a: SomeType,\n  b: SomeType,\n  c: array[SomeType]\n}\n\n    \n  \n\n\nShorthand for Custom Properties\n\nCustom properties (also referred to as “annotations”) were supported in PDSC as just arbitrary values keyed at anything that’s not a reserved keyword.\nIn PDL, the syntax for custom properties is cleaner and more Java-like.\n\n\n  \n    PDSC\n    PDL\n  \n  \n    \n\n{\n  \"namespace\": \"com.example.models\",\n  \"type\": \"record\",\n  \"name\": \"CustomProperties\",\n  \"something\": [ 1, 2, 3 ],\n  \"fields\": []\n}\n\n    \n    \n\nnamespace com.example.models\n\n@something = [ 1, 2, 3 ]\nrecord CustomProperties {}\n\n    \n  \n\n\nFurthermore, PDL supports a path-like shorthand, where dot-separated keys can be used to specify nested custom properties.\nSome property written as such:\n\n@prop = {\n    \"nested\": {\n        \"foo\": 1,\n        \"bar\": 2\n    }\n}\n\n\nMay alternatively be written as such:\n\n@prop.nested.foo = 1\n@prop.nested.bar = 2\n\n\nOne can easily imagine a scenario in which this would really come in handy:\n\n// Not so pretty...\n@a = {\n    \"b\": {\n        \"c\": {\n            \"d\": {\n                \"e\": {\n                    \"f\": \"hello\"\n                }\n            }\n        }\n    }\n}\n\n// That's better\n@a.b.c.d.e.f = \"hello\"\n\n\nAnother interesting shorthand for custom properties is omission of an explicit value to indicate true:\n\n\n  \n    PDSC\n    PDL\n  \n  \n    \n\n{\n  \"namespace\": \"com.example.models\",\n  \"type\": \"record\",\n  \"name\": \"ImplicitProperty\",\n  \"truthy\": true,\n  \"fields\": []\n}\n\n    \n    \n\nnamespace com.example.models\n\n@truthy\nrecord ImplicitProperty {}\n\n    \n  \n\n\nCleaner Enum Declarations\n\nOne major pain-point in PDSC is declaring enums with metadata such as doc strings, custom properties, and deprecation.\nIn PDSC schemas, this metadata must all be specified in individual mappings which are separate from the main symbol list.\nIn this way, defining complex enums in PDSC is unintuitive and can be hard to read and maintain.\n\nOn the other hand, PDL’s syntax for defining enum symbol metadata is quite intuitive. Each doc string, custom property, and\ndeprecation annotation can be placed right alongside the symbol.\n\n\n  \n    PDSC\n    PDL\n  \n  \n    \n\n{\n  \"type\" : \"enum\",\n  \"name\" : \"Fruits\",\n  \"symbols\" : [ \"APPLE\", \"BANANA\", \"PEAR\", \"PLUM\" ],\n  \"symbolDocs\" : {\n    \"BANANA\" : \"Comment for the BANANA symbol.\",\n    \"PLUM\" : \"This symbol has been deprecated.\"\n  },\n  \"deprecatedSymbols\" : {\n    \"PLUM\" : true\n  },\n  \"symbolProperties\" : {\n    \"PEAR\" : {\n      \"someAnnotation\" : false\n    },\n    \"BANANA\" : {\n      \"a\" : {\n        \"b\" : {\n          \"c\" : 123\n        }\n      }\n    }\n  }\n}\n\n    \n    \n\nenum Fruits {\n  APPLE,\n\n  /**\n   * Comment for the BANANA symbol.\n   */\n  @a.b.c = 123\n  BANANA,\n\n  @someAnnotation = false\n  PEAR,\n\n  /**\n   * This symbol has been deprecated.\n   */\n  @deprecated\n  PLUM\n}\n\n    \n  \n\n\nSpecifying Default Field Values\n\nAlthough this is more of a similarity than a difference, it should be noted that specifying default values for record\nfields in PDL is the same as in PDSC. In both PDSC and PDL, default values are specified using the value’s\nserialized JSON representation. This may be confusing at first\nbecause PDL is not a JSON-like format, however field default values are still represented using JSON.\n\n\n  \n    PDSC\n    PDL\n  \n  \n    \n\n{\n  \"type\" : \"record\",\n  \"name\" : \"Defaults\",\n  \"namespace\" : \"com.example.models\",\n  \"fields\" : [ {\n    \"name\" : \"x\",\n    \"type\" : {\n      \"type\" : \"array\",\n      \"items\" : \"int\"\n    },\n    \"default\" : [ 1, 2, 3 ]\n  }, {\n    \"name\" : \"y\",\n    \"type\" : {\n      \"type\" : \"map\",\n      \"values\" : \"string\"\n    },\n    \"default\" : {\n      \"key\" : \"value\"\n    }\n  }, {\n    \"name\" : \"z\",\n    \"type\" : \"SomeRecord\",\n    \"default\" : {\n      \"fieldA\" : 1,\n      \"fieldB\" : true\n    }\n  }, {\n    \"name\" : \"a\",\n    \"type\" : \"SomeUnion\",\n    \"default\" : {\n      \"float\" : 1.0\n    }\n  }, {\n    \"name\" : \"b\",\n    \"type\" : \"SomeEnum\",\n    \"default\" : \"SYMBOL1\"\n  } ]\n}\n\n    \n    \n\nnamespace com.example.models\n\nrecord Defaults {\n  x: array[int] = [ 1, 2, 3 ],\n  y: map[string, string] = { \"key\": \"value\" },\n  z: SomeRecord = { \"fieldA\": 1, \"fieldB\": true },\n  a: SomeUnion = { \"float\": 1 },\n  b: SomeEnum = \"SYMBOL1\"\n}\n\n    \n  \n\n",
      tags: null,
      id: 45
    });
    
    

  
    index.add({
      title: "PDL Schema",
      category: null,
      content: "PDL Schema\n\nContents\n\n  PDL Schema Definition\n  Creating a Schema\n  Record Type\n  Primitive Types\n  Enum Type\n  Array Type\n  Map Type\n  Union Type\n  Typeref\n  Fixed Type\n  Imports\n  Properties\n  Package\n  Escaping\n\n\nPDL Schema Definition\n\nPegasus is a schema definition and serialization system developed as part of\nthe Rest.li framework. It provides multi-language\nsupport\nfor services built using Rest.li and handles seemless serialization and\ndeserialization of data between server and clients.\n\nPDL is a schema definition language for Pegasus, developed\nas a user friendly and concise format replacement for the older JSON based\nPDSC schema format.\n\nCreating a Schema\n\nPegasus supports different types of schemas: Records,\nPrimitive types, Enums, Arrays,\nMaps, Unions, Fixed and\nTyperefs. Records, Enums, and Typerefs have names (Named schemas)\nand thus can be defined as top-level schemas. Named schemas can specify an\noptional namespace to avoid naming conflict between schemas with same name.\nThe name prefixed with the namespace using the dot(.) separator becomes the\nfully qualified name (FQN) of the schema. Named schemas can be\nreferenced from other schemas using the fully qualified name.\n\nEach top-level schema should be stored in its own file with a .pdl extension.\nName of the file should match the schema name and the directory structure should\nmatch the namespace (similar to how Java classes are organized).\n\nThe Pegasus code generator implements a resolver that is similar to Java class\nloaders. If there is a reference to a named schema, the code generator will try\nto look for a file in the code generator’s resolver path. The resolver path is\nsimilar to a Java classpath. The fully qualified name of the named schema will\nbe translated to a relative file name. The relative file name is computed by\nreplacing dots (.) in the fully qualified name by the directory path separator\n(typically /) and appending a .pdl extension. This relative file name looked\nup using each location in the resolver path until it finds a file that contains\nthe named schema.\n\nRecord Type\nRecords are the most common type of Pegasus schemas and usually the starting\npoint when defining data models using Pegasus. A record represents a Named\nentity with fields representing attributes of that entity. The fields can be\nprimitive types, enums, unions, maps, arrays, other records or any valid Pegasus\ntype.\n\nFor example:\nnamespace com.example.time\n\nrecord Date {\n  day: int\n  month: int\n  year: int\n}\n\nThe above example is defining a record called Date. This is defined in the\nnamesapce com.example.time, giving it the fully qualified name\ncom.example.time.Date. So this schema should be defined in the following file\n&lt;project-dir&gt;/pegasus/com/example/time/Date.pdl\n\n\nThe Date record can then be referenced in other schemas:\nnamespace com.example.models\n\nimport com.example.time.Date\n\nrecord User {\n  firstName: string\n  birthday: Date\n}\n\nThe above example is defining a record called User:\n\n  This record is using the namespace com.example.models. So the fully\n qualified name is com.example.models.User.\n  This record is defining two fields, firstName and birthday.\n  firstName is a primitive field of type string.\n  birthday references the type com.example.time.Date from the first example\n  PDL supports Java like import feature that allows you to define references\n to external types and then use them in the schema using their simple name.\n\n\nRecord Field attributes\nPegasus supports additional features for defining the behavior of fields in a\nrecord.\nRecord fields can be defined as optional.\nIn Pegasus, a field is required unless the field is explicitly declared as\noptional using the optional keyword. An optional field may be present or\nabsent in the in-memory data structure or serialized data.\n\nIn the generated client bindings, Pegasus provides has[Field] methods(eg,\nhasBirthDay()) to determine if an optional field is present.\n\nFor example:\nnamespace com.example.models\n\nimport com.example.time.Date\n\nrecord User {\n  firstName: string\n  birthday: optional Date\n}\n\nThe above example defines the birthday field as optional.\n\nRecord fields may have default values.\nPegasus supports specifying default values for fields. Though the definition\nlanguage allows default values for both required and optional fields, it is\nrecommended to use default values only for required fields.\n\nThe default value for a field is expressed as a JSON value confirming to the\ntype of the field.\n\nIn Pegasus generated bindings, the get[Field] accessors will return the value of\nthe field it is present or the default value from the schema if the field is\nabsent. The bindings also provide a specialized get accessor that allows the\ncaller to specify whether the default value or null should be returned when an\nabsent field is accessed.\n\nSee GetMode for more\ndetails on accessing optional fields and default values.\n\nFor example:\nnamespace com.example.models\n\nimport org.example.time.Date\n\nrecord User {\n  firstName: string\n  birthday: optional Date\n  isActive: boolean = true\n}\n\nThe above example defines a boolean field isActive, which has a default value\ntrue.\n\nInlined schemas\nIn Pegasus, records and other named schemas need not be top-level schemas. They\ncan be inlined within other record schemas.\n\nFor example:\nnamespace com.example.models\n\nimport com.example.time.Date\n\nrecord User {\n  firstName: string\n  birthday: optional Date\n  isActive: boolean = true\n  address: record Address {\n    state: string\n    zipcode: string\n  }\n}\n\nAddress record in the above example is inlined. It inherits the namespace of\nthe parent record User, making its fully qualified name com.example.models.Address.\n\n\n  Namespace of inline types can be specified/overriden by defining a Namespace\nsurrounding the inlined type.\n  The default value of fields using inlined types can be expressed using its\nserialized JSON representation.\n\n\nFor example:\nnamespace com.example.models\n\nimport com.example.time.Date\n\nrecord User {\n  firstName: string\n  birthday: optional Date\n  isActive: boolean = true\n  address: {\n    namespace com.example.models.address\n\n    record Address {\n      state: string\n      zipCode: string\n    }\n  } = {\n    \"state\": \"CA\",\n    \"zipCode\": \"12345\"\n  }\n}\n\nNote: If a record or a named schema is referenced by other schemas, it should\nbe a top-level schema. Referencing in-line schemas outside the schema in which\nthey are defined is not allowed.\n\nDoc Strings\n\nPegasus types and fields may be documented using “doc strings” following the\nJava style comments.\n\n  Comments using /** */ syntax are treated as schema documentation. They will\nbe included in the in-memory representation and the generated binding classes.\n  Comments using /* */ or // syntax are allowed but not treated as schema\ndocumentation.\n\n\nFor example:\nnamespace com.example.models\n\nimport com.example.time.Date\n\n/**\n * A record representing an user in the system.\n */\nrecord User {\n  /** First name of the user */\n  firstName: string\n\n  /** User's birth day */\n  birthday: optional Date\n\n  // TODO: Can this be an enum?\n  /** Status of the user. */\n  isActive: boolean = true\n}\n\n\nDeprecation\nPegasus supports marking types or fields as deprecated by adding @deprecated\nannotation. The deprecation details from the schema will be carried over to the\nbindings generated in  different languages.\nIn Java language, the classes, getter and setter methods generated for\ndeprecated types will be marked as deprecated.\n\nIt is recommended to specify a string describing the reason for deprecation and\nthe alternative as the value for the @deprecated annotation.\n\nDeprecate a field:\nnamespace com.example.models\n\nimport com.example.time.Date\n\nrecord User {\n  firstName: string\n\n  @deprecated = \"Use birthday instead.\"\n  birthYear: int\n\n  birthday: Date  \n}\n\nDeprecate a record:\nnamespace com.example.models\n\n@deprecated = \"Use Person type instead.\"\nrecord User {\n  firstName: string\n}\n\n\nSee Enum documentation and deprecation\nfor details on deprecating enum symbols.\n\nIncluding fields\nPegasus records support including fields from one or more other records. When\na record is included, all its fields will be included in the current record. It\ndoes not include any other attribute of the other record.\n\nIncludes are transitive, if record A includes record B and record B includes\nrecord C, record A contains all the fields declared in record A, record B and\nrecord C.\n\nThe value of the “include” attribute should be a list of records or typerefs of\nrecords. It is an error to specify non-record types in this list.\n\nFor example:\nnamespace com.example.models\n\n/**\n * User includes fields of AuditStamp, User will have fields firstName from\n * itself and fields createdAt and updatedAt from AuditStamp.\n */\nrecord User includes AuditStamp {\n  firstName: string\n}\n\n\nnamespace com.example.models\n\n/**\n * A common record to represent audit stamps.\n */\nrecord AuditStamp {\n  /** Time in milliseconds since epoch when this record was created */\n  createdAt: long\n\n  /** Time in milliseconds since epoch when this record was last updated */\n  updatedAt: long\n}\n\n\nIncludes feature allows including fields from multiple records.\n\nFor example:\nnamespace com.example.models\n\n/** A common record for specifying version tags of a record */\nrecord VersionTag {\n  versionTag: string\n}\n\n\nnamespace com.example.models\n\n/**\n * User includes fields of AuditStamp, User will have fields firstName from\n * itself and fields createdAt and updatedAt from AuditStamp.\n */\nrecord User includes AuditStamp, VersionTag {\n  firstName: string\n}\n\n\nNote: Record inclusion does not imply inheritance, it is merely a\nconvenience to reduce duplication when writing schemas.\n\nPrimitive Types\nThe above examples already introduced int, long, string and boolean\nprimitive types that are supported by Pegasus.\nThe full list of supported Pegasus primitive types are: int, long, float,\ndouble, boolean, string and bytes.\n\nThe actual types used for the primitives depends on the language specific\nbinding implementation. For details on Java bindings for Pegasus primitives, see\nPrimitive Types\n\nPrimitive types cannot be named (except through typerefs) and thus\ncannot be defined as top-level schemas.\n\nSome examples showing how different primitive fields can be defined and the\nsyntax for specifying default values:\nnamespace com.example.models\n\nrecord WithPrimitives {\n  intField: int\n  longField: long\n  floatField: float\n  doubleField: double\n  booleanField: boolean\n  stringField: string\n  bytesField: bytes\n}\n\n\nPrimitive types with default values:\nnamespace com.example.models\n\nrecord WithPrimitiveDefaults {\n  intWithDefault: int = 1\n  longWithDefault: long = 3000000000\n  floatWithDefault: float = 3.3\n  doubleWithDefault: double = 4.4E38\n  booleanWithDefault: boolean = true\n  stringWithDefault: string = \"DEFAULT\"\n  bytesWithDefault: bytes = \"\\u0007\"\n}\n\n\nEnum Type\n\nEnums, as the name suggests contains an enumeration of symbols. Enums are named\nschemas and can be defined as top-level schema or inlined.\n\nFor example:\nnamespace com.example.models\n\nenum UserStatus {\n  ACTIVE\n  SUSPENDED\n  INACTIVE\n}\n\n\nEnums can be referenced by name in other schemas:\nnamespace com.example.models\n\nrecord User {\n  firstName: string\n  status: UserStatus\n}\n\n\nEnums can also be defined inline:\nnamespace com.example.models\n\nrecord User {\n  firstName: string\n  status: UserStatus\n  suspendedReason: enum StatusReason {\n    FLAGGED_BY_SPAM_CHECK\n    REPORTED_BY_ADMIN\n  }\n}\n\n\nEnum documentation and deprecation\nDoc comments and deprecation can be added directly to individual enum symbols.\n\nFor example:\nnamespace com.example.models\n\n/**\n * Defines the states of a user in the system.\n */\nenum UserStatus {\n  /**\n   * Represents an active user.\n   */\n  ACTIVE\n\n  /**\n   * Represents user suspended for some reason.\n   */\n  SUSPENDED\n\n  /**\n   * Represents an user who had deleted/inactivated their account.\n   */\n  @deprecated = \"Use INACTIVE for users pending deletion. Deleted users should not be in system\"\n  DELETED\n\n  /**\n   * Represents users requested for deletion and in the process of being deleted.\n   */\n   INACTIVE\n}\n\n\nEnum defaults\n\nTo specify the default value for an enum field, use the string representation of\nthe enum symbol to set as default.\n\nFor example:\nnamespace com.example.models\n\nrecord User {\n  firstName: string\n  status: UserStatus = \"ACTIVE\"\n  suspendedReason: enum StatusReason {\n    FLAGGED_BY_SPAM_CHECK\n    REPORTED_BY_ADMIN\n  } = \"FLAGGED_BY_SPAM_CHECK\"\n}\n\n\nArray Type\n\nPegasus Arrays are defined as a homogeneous collection of some “items” type,\nwhich can be any valid PDL type. Arrays are ordered,\nas in the items in an array have a specific ordering and this ordering will\nbe honored when the data is serialized, sent over the wire, and de-serialized.\n\nSome examples below for defining arrays and default values for them.\n\nPrimitive arrays:\nnamespace com.example.models\n\nrecord WithPrimitivesArray {\n  ints: array[int]\n  longs: array[long]\n  floats: array[float]\n  doubles: array[double]\n  booleans: array[boolean]\n  strings: array[string]\n  bytes: array[bytes]\n}\n\n\nPrimitive arrays with default values:\nnamespace com.example.models\n\nrecord WithPrimitivesArrayDefaults {\n  ints: array[int] = [1, 2, 3]\n  longs: array[long] = [3000000000, 4000000000]\n  floats: array[float] = [3.3, 2.5]\n  doubles: array[double] = [4.4E38, 3.1E24]\n  booleans: array[boolean] = [true, false]\n  strings: array[string] = [\"hello\"]\n  bytes: array[bytes] = [\"\\u0007\"]\n}\n\n\nRecord or Enum arrays:\nnamespace com.example.models\n\nrecord SuspendedUsersReport {\n  users: array[User]\n  reasons: array[SuspendReason]\n}\n\n\nRecord or Enum arrays with default values:\nnamespace com.example.models\n\nrecord SuspendedUsersReport {\n  users: array[User] = [{ \"firstName\": \"Joker\" }, { \"firstName\": \"Darth\"}]\n  reasons: array[SuspendReason] = [\"FLAGGED_BY_SPAM_CHECK\"]\n}\n\n\nMap Type\n\nMaps are defined with a key type and a value type. The value type can\nbe any valid PDL type, but currently string is\nthe only supported key type. Entries in a Map are not ordered and the\norder can change when the data is serialized/de-serialized.\n\nSome examples below for defining maps and default values for them.\n\nPrimitive maps:\nnamespace com.example.models\n\nrecord WithPrimitivesMap {\n  ints: map[string, int]\n  longs: map[string, long]\n  floats: map[string, float]\n  doubles: map[string, double]\n  booleans: map[string, boolean]\n  strings: map[string, string]\n  bytes: map[string, bytes]\n}\n\n\nPrimitive maps with default values:\nnamespace com.example.models\n\nrecord WithPrimitivesMapDefaults {\n  ints: map[string, int] = { \"int1\": 1, \"int2\": 2, \"int3\": 3 }\n  longs: map[string, long] = { \"long1\": 3000000000, \"long2\": 4000000000 }\n  floats: map[string, float] = { \"float1\": 3.3, \"float2\": 2.1 }\n  doubles: map[string, double] = {\"double1\": 4.4E38, \"double2\": 3.1E24}\n  booleans: map[string, boolean] = { \"boolean1\": true, \"boolean2\": true, \"boolean3\": false }\n  strings: map[string, string] = { \"string1\": \"hello\", \"string2\": \"world\" }\n  bytes: map[string, bytes] = { \"bytes\": \"\\u0007\" }\n}\n\n\nMaps with complex values:\nnamespace com.example.models\n\nrecord UserReport {\n  /** Users grouped by their status. Key is the string representation of status enum */\n  usersByStatus: map[string, array[User]]\n\n  /**\n   * Count of users grouped by firstName and then by status.\n   * First level key is the firstName of users.\n   * Second level key is the string representation of status.\n   */\n  countByFirstNameAndStatus: map[string, map[string, int]]\n}\n\n\nUnion Type\nUnion is a powerful way to model data that can be of different types at\ndifferent scenarios. Record fields that have this behavior can be defined as a\nUnion type with the expected value types as its members.\n\nA union type may be defined with any number of member types. Member type can be\nprimitive, record, enum, map or array. Unions are not allowed as members inside\nan union.\n\nThe fully qualified member type names also serve as the “member keys” (also\ncalled as “union tags”), and identify which union member type data holds. These\nare used to represent which member is present when the data is serialized.\n\nFor example:\nnamespace com.example.models\n\nrecord Account {\n  /**\n   * Owner of this account. Accounts can be owned either by a single User or an\n   * user group.\n   */\n  owner: union[User, UserGroup]\n}\n\n\nThe above example defines an owner field that can either be an User or an\nUserGroup.\n\nUnion with default value:\n\nnamespace com.example.models\n\nrecord Account {\n  /**\n   * Owner of this account. Accounts can be owned either by a single User or an\n   * user group.\n   * By default, All your accounts are belong to CATS.\n   */\n  owner: union[User, UserGroup] = { \"com.example.models.User\": { \"firstName\": \"CATS\" }}\n}\n\n\nTypes can be declared inline within a union definition:\nnamespace com.example.models\n\nrecord User {\n  firstName: string\n  status: UserStatus = \"ACTIVE\"\n  statusReason: union [\n    enum ActiveReason {\n      NEVER_SUSPENDED\n      SUSPENSION_CLEARED\n    }\n    enum SuspendReason {\n      FLAGGED_BY_SPAM_CHECK\n      REPORTED_BY_ADMIN\n    }\n    enum InactiveReason {\n      USER_REQUESTED_DELETION\n      REQUESTED_BY_ADMIN\n    }\n  ] = { \"com.example.models.ActiveReason\": \"NEVER_SUSPENDED\" }\n}\n\n\nUnion with aliases\nNote: Union with aliases is a recent feature in the Pegasus schema\n*language and it might not be fully supported in non-java languages. Please\n*check the support level on all\n*languages you intend to use before using aliases\n\nUnion members can optionally be given an alias. Aliases can be used to create\nunions with members of the same type or to give better naming for union members.\n\nUnion with aliases is required for cases where the union contains multiple\nmembers that are:\n\n  of same primitive type\n  of same named complex type\n  arrays of same or different item types\n  maps of same or different value types\n  typerefs dereferenced to same type\n\n\nSuch unions must specify an unique alias for each member in the union\ndefinition. When an alias is specified, it acts as the member’s discriminator\nunlike the member type name on the standard unions defined above.\n\nAliased unions are defined as:\nunion [alias: type, /* ... */]\n\nThere are few constraints that must be taken in consideration while specifying\naliases for union members,\n\n\n  Aliases must be unique for each member in a union definition.\n  Aliases must be specified for either all or none of the members in a union\ndefinition.\n  Aliases cannot be specified for null member types which means there can\nonly be one null member inside a union definition.\n\n\nAn example showing union with aliases:\nnamespace com.example.models\n\n/**\n * A record with user's contact information.\n */\nrecord Contacts {\n  /** Primary phone number for the user */\n  primaryPhoneNumber: union[\n    /** A mobile phone number */\n    mobile: PhoneNumber,\n\n    /**\n     * A work phone number\n     */\n    work: PhoneNumber,\n\n    /** A home phone number */\n    home: PhoneNumber\n  ]\n}\n\nSince all three union members mobile, work and home are of the same type\nPhoneNumber, aliases are required for this union.\n\nAliased union members can have doc strings and custom properties. This is not supported for non-aliased union members.\n\nWhen using unions with aliases, the alias should be used as the key within the default value:\nnamespace com.example.models\n\n/**\n * A record with user's contact information.\n */\nrecord Contacts {\n  /** Primary phone number for the user */\n  primaryPhoneNumber: union[\n    /** A mobile phone number */\n    mobile: PhoneNumber,\n\n    /**\n     * A work phone number\n     */\n    work: PhoneNumber,\n\n    /** A home phone number */\n    home: PhoneNumber\n  ] = {\"mobile\": { \"number\": \"314-159-2653\" }}\n}\n\n\nTyperef\n\nPegasus supports a new schema type known as a typeref. A typeref is like a\ntypedef in C. It does not declare a new type but declares an alias to an\nexisting type.\n\nTyperefs can provide additional clarity when using primitive types.\nTyperefs are useful for differentiating different uses of the same type. For\nexample, we can use to a typeref to differentiate a string field that holds an\nURL from an arbitrary string value or a long field that holds an epoch time in\nmilliseconds from a generic long value.\n\nFor example:\nnamespace com.example.models\n\n/** Number of milliseconds since midnight, January 1, 1970 UTC. */\ntyperef Time = long\n\n\nnamespace com.example.models\n\n/**\n * A common record to represent audit stamps.\n */\nrecord AuditStamp {\n  /** Time when this record was created */\n  createdAt: Time\n\n  /** Time when this record was last updated */\n  updatedAt: Time\n}\n\nA typeref allows additional meta-data to be associated with primitive and\nunnamed types. This meta-data can be used to provide documentation or support\ncustom properties.\n\nTyperefs can be used to name anonymous types.\nA typeref provides a way to refer to common unnamed types such as arrays, maps,\nand unions. Without typerefs, users may have to wrap these unnamed types with a\nrecord in order to address them. Alternatively, users may cut-and-paste common\ntype declarations, resulting in unnecessary duplication and potentially causing\ninconsistencies if future changes are not propagated correctly to all copies.\n\nFor example:\nnamespace com.example.models\n\ntyperef PhoneContact = union[\n  /** A mobile phone number */\n  mobile: PhoneNumber,\n\n  /**\n   * A work phone number\n   */\n  work: PhoneNumber,\n\n  /** A home phone number */\n  home: PhoneNumber\n]\n\n\n\nTyperefs can then be referred to by name from any other type:\nnamespace com.example.models\n\nrecord Contacts {\n  primaryPhone: PhoneContact\n  secondaryPhone: PhoneContact\n}\n\n\nTyperefs can be used to specify custom types and coercers\n\nFor example, Joda time has a convenient DateTime Java class. If we wish to use\nthis class in Java to represent date-times, all we need to do is define a\nPegasus custom type that binds to it:\n\nnamespace com.example.models\n\n@java.class = \"org.joda.time.DateTime\"\n@java.coercerClass = \"com.example.time.DateTimeCoercer\"\ntyperef DateTime = string\n\nThe coercer is responsible for converting the Pegasus “referenced” type, in this\ncase “string” to the Joda DateTime class.\n\nSee [Java Binding]\n(https://linkedin.github.io/rest.li/java_binding#custom-java-class-binding-for-primitive-types)\nfor more details on defining and using custom types.\n\nFixed Type\n\nThe Fixed type is used to define schemas with a fixed size in terms of number of\nbytes.\n\nFor example:\nnamespace com.example.models\n\nfixed MD5 16\n\nThe example above defines a type called MD5, which has a size of 16 bytes.\n\nImports\nImports are optional statements which allow you to avoid writing fully-qualified\nnames. They function similarly to imports in Java.\n\nFor example, the following record can be expressed without imports like this:\n\nnamespace com.example.models\n\nrecord AuditStamp {\n  createdAt: com.example.models.time.Time\n  updatedAt: com.example.models.time.Time\n}\n\n\nAlternatively, the record can be expressed using imports, minimizing the need\nfor repetitive code:\n\nnamespace com.example.models\n\nimport com.example.models.time.Time\n\nrecord AuditStamp {\n  createdAt: Time\n  updatedAt: Time\n}\n\n\nNote:\n\n  Any type that is not imported and is not within the namespace from which it’s\nreferenced must be referenced by fully qualified name.\n  Imports take precedence when Pegasus resolves identifiers in a schema. So\nbe careful when adding an import that conflicts with types in the same namespace\nof the root-document.\n  Using imports in the following ways will lead to PDL parser errors. You should\navoid doing so.\n    \n      Importing types declared inside the document.\n      Importing types within the root namespace of the document.\n      Declaring types that conflict with existing imports.\n    \n  \n\n\nProperties\nProperties can be used to present arbitrary data and added to records, record\nfields, enums, enum symbols or aliased union members. Properties are defined\nusing the @propertyName syntax. They provide custom data and are available\nto users for extending the functionality of the Pegasus schema.\n\nIt is up to the users to define the semantic meaning for these custom\nproperties. Pegasus language will treat them as key-value pairs and make them\navailable with the in-memory representation of the schema.\n\nAdd properties to record and record field:\n@hasPii = true\nrecord User {\n  @validate.regex.regex = \"^[a-zA-Z]+$\"\n  firstName: string\n}\n\nThe above example shows a record level property hasPii that can be used to\nindicate if the record has personally identifiable information. This can then\nbe used by application specific business logic.\n\nAdd properties to enum and enum symbols:\nnamespace com.example.models\n\n/**\n * Defines the states of a user in the system.\n */\n@hasPii = false\nenum UserStatus {\n  /**\n   * Represents an active user.\n   */\n  @stringFormat = \"active\"\n  ACTIVE\n\n  /**\n   * Represents user suspended for some reason.\n   */\n   @stringFormat = \"suspended\"\n  SUSPENDED\n\n  /**\n   * Represents an user who had deleted/inactivated their account.\n   */\n  @deprecated = \"Use INACTIVE for users pending deletion. Deleted users should not be in system\"\n  @stringFormat = \"deleted\"\n  DELETED\n\n  /**\n   * Represents users requested for deletion and in the process of being deleted.\n   */\n   @stringFormat = \"not active\"\n   INACTIVE\n}\n\n\nAdd properties to aliased union members:\nnamespace com.example.models\n\ntyperef PhoneContact = union[\n  /** A mobile phone number */\n  @allowText = true\n  mobile: PhoneNumber,\n\n  /**\n   * A work phone number\n   */\n  @allowText = false\n  work: PhoneNumber,\n\n  /** A home phone number */\n  @allowText = false\n  home: PhoneNumber\n]\n\n\n\nProperty values can be any valid JSON type:\n\nFor example:\n@prop = 1\n\n@prop = \"string\"\n\n@prop = [1, 2, 3]\n\n@prop = { \"a\": 1, \"b\": { \"c\": true }}\n\n\nProperty values can also be empty:\n\nIf you don’t indicate an explicit property value, it will result in an implicit\nvalue of true.\n\nFor example:\n@hasPii\n\n\nProperty keys can be expressed as JSON:\n\nFor example:\n@validate = {\n  \"regex\": {\n    \"pattern\": \"^[a-z]+$\"\n  }\n}\n\n\nProperty keys can be expressed as paths:\n\nThe JSON style property key is complicated to write and read, so we provide a\nshorthand - the dot separate format to express the property keys.\n\nThe following example is equivalent to the previous JSON example:\n\n@validate.regex.pattern = \"^[a-z]+$\"\n\n\nPackage\n\nPackage is used to qualify the language binding namespace for the named schema.\n\nFor example:\nnamespace com.example.models\npackage com.example.api\n\nrecord User {\n  firstName: string\n}\n\nIf package is not specified, language binding class for the named schema will\nuse namespace as its default namespace.\n\nIn Java binding, the package\nof the generated class will be determined by the package specified in the\nschema.\n\nEscaping\nThere are some\nkeywords\nwhich are reserved in Pegasus. If these reserved keywords are used to define any\nnames or identifiers, they need to be escaped using  back-ticks: ` `.\n\nKeyword Escaping\n\nnamespace com.example.models\n\nrecord PdlKeywordEscaping {\n  `namespace`: string\n  `record`: string\n  `null`: string\n  `enum`: string\n  recordName: record `record` { }\n}\n\n\nNamespace/Package escaping\n\nReserved keywords also need to be escaped when used in namespace declarations,\npackage declarations and import statements.\n\nnamespace com.example.models.`record`\npackage com.example.models.`typeref`\n\nimport com.example.models.`optional`\n\nrecord NamespacePackageEscaping { }\n\n\nProperty key escaping\n\nIf you want Pegasus to treat property key name with dots as one string key,\nplease use backticks to escape such string. Escaping property keys can also be\nused to escape reserved keywords such as “namespace” or to escape\nnon-alpha-numberic characters in the keys.\n\nFor example:\n\nnamespace com.example.models\n\nrecord User {\n  @`namespace` = \"foo.bar\"\n  @validate.`com.linkedin.CustomValidator` = \"foo\"\n  firstName: string\n}\n\n",
      tags: null,
      id: 46
    });
    
    

  
    index.add({
      title: "PDSC Syntax",
      category: null,
      content: "PDSC Syntax\n\nContents\n\n\n  PDSC Schema Definition\n  Creating a Schema\n  Typeref\n  Record Field Attributes\n  Optional Fields\n  Union\n  Default Values\n  Including Fields from Another Record\n  Deprecation\n\n\nPDSC Schema Definition\n\nPDSC is a Pegasus schema definition language, and it is inspired by the Avro 1.4.1\nspecification.The Pegasus schema definition language is inspired by the Avro 1.4.1\nspecification.\n\n\n  Pegasus leverages Avro’s JSON-based schema definition syntax, type\nsystem, and JSON serialization format.\n  Pegasus does not implement or use Avro’s binary data serialization\nformat, object container files, protocol wire format, and schema\nresolution.\n  Pegasus does not require or re-use any Avro implementation.\n  The code generator supports all of Avro’s types: primitive types,\nrecords, enums, arrays, maps, unions, fixed.\n\n\nThe Pegasus schema differs from Avro in the following ways:\n\n\n  Pegasus uses “optional” to indicate that a field in a record is\noptional. Union with null should not be used to indicate an optional\nfield. Future versions of Pegasus may remove the null type from the\nschema definition language.\n  Pegasus unions can have more than one member of the same type. This\nis supported by specifying an alias for each member which is unique\nwithin that union.\n  Pegasus default values for union types must include the member\ndiscriminator. Avro default values for union types do not include\nthe member discriminator. The type of the Avro union default value\nis always the 1st member type of the union.\n  Pegasus implements a mechanism for including all the fields from\nanother record into the current record.\n  Pegasus schema files have file names that ends with .pdsc instead\nof .avsc.\n  The Pegasus code generator also implements resolvers similar to Java\nclass loaders that allow each named schema to be specified in a\ndifferent source file.\n(Note the Avro code generator requires all referenced named schemas\nto be in the same source file. The schema resolver of the Pegasus\ncode generator is unrelated to the schema resolution feature of\nAvro.)\n  Pegasus ignores the aliases (but allows them in the schema\ndefinition). Future versions of Pegasus may remove aliases.\n  Pegasus schema resolution and compatibility rules are different.\n  Pegasus named schema provides option to override language binding\nnamespace for generated data model from the schema. Note that this\nbinding namespace override will not change how data is transported\nover the wire.\n\n\nCreating a Schema\n\nEach schema should be stored in its own file with a .pdsc extension.\nThe Pegasus code generator implements a resolver that is similar to Java\nclass loaders. If there is a reference to a named schema, the code\ngenerator will try to look for a file in the code generator’s resolver\npath. The resolver path is similar to a Java classpath. The fully\nqualified name of the named schema will be translated to a relative file\nname. The relative file name is computed by replacing dots (“.”) in the\nfully qualified name by the directory path separator (typically “/”) and\nappending a .pdsc extension. This relative file name is appended to\neach path in the resolver path. The resolver opens each of these files\nuntil it finds a file that contains the named schema.\n\nThe named schema declarations support the following attributes:\n\n\n  type a JSON string providing the type of the named schema\n(required).\n  name a JSON string providing the name of the named schema\n(required).\n  namespace a JSON string that qualifies the namespace for the named\nschema.\n  package a JSON string that qualifies the language binding\nnamespace for the named schema (optional). If this is not specified,\nlanguage binding class for the named schema will use namespace as\nits default namespace.\n  doc a JSON string providing documentation to the user of this\nnamed schema (optional).\n\n\nThe named schemas with type “enum” also supports a symbolDocs\nattribute to provide documentation for each enum symbol.\n\nNote: Due to the addition of doclint in JDK8, anything under the\ndoc or symbolDocs attribute must be W3C HTML 4.01 compliant. This is\nbecause the contents of this string will appear as Javadocs in the\ngenerated Java ‘data template’ classes later. Please take this into\nconsideration when writing your documentation.\n\nThe following are a few example schemas and their file names.\n\ncom/linkedin/pegasus/generator/examples/Foo.pdsc\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"Foo\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.examples\",\n  \"doc\" : \"A foo record\",\n  \"fields\" : [\n    { \"name\" : \"intField\",       \"type\" : \"int\" },\n    { \"name\" : \"longField\",      \"type\" : \"long\" },\n    { \"name\" : \"floatField\",     \"type\" : \"float\" },\n    { \"name\" : \"doubleField\",    \"type\" : \"double\" },\n    { \"name\" : \"bytesField\",     \"type\" : \"bytes\" },\n    { \"name\" : \"stringField\",    \"type\" : \"string\" },\n    { \"name\" : \"fruitsField\",    \"type\" : \"Fruits\" },\n    { \"name\" : \"intArrayField\",  \"type\" : { \"type\" : \"array\", \"items\" : \"int\" } },\n    { \"name\" : \"stringMapField\", \"type\" : { \"type\" : \"map\", \"values\" : \"string\" } },\n    {\n      \"name\" : \"unionField\",\n      \"type\" : [\n        \"int\",\n        \"string\",\n        \"Fruits\",\n        \"Foo\",\n        { \"type\" : \"array\", \"items\" : \"string\" },\n        { \"type\" : \"map\", \"values\" : \"long\" },\n        \"null\"\n      ]\n    }\n  ]\n}\n\n\ncom/linkedin/pegasus/generator/examples/FooWithNamespaceOverride.pdsc\n(please see Java Binding section to see how this “package” affects\ngenerated java class.)\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"FooWithNamespaceOverride\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.examples\",\n  \"package\" : \"com.linkedin.pegasus.generator.examples.record\",\n  \"doc\" : \"A foo record\",\n  \"fields\" : [\n    { \"name\" : \"intField\",       \"type\" : \"int\" },\n    { \"name\" : \"longField\",      \"type\" : \"long\" },\n    { \"name\" : \"floatField\",     \"type\" : \"float\" },\n    { \"name\" : \"doubleField\",    \"type\" : \"double\" },\n    { \"name\" : \"bytesField\",     \"type\" : \"bytes\" },\n    { \"name\" : \"stringField\",    \"type\" : \"string\" },\n    { \"name\" : \"fruitsField\",    \"type\" : \"Fruits\" },\n    { \"name\" : \"intArrayField\",  \"type\" : { \"type\" : \"array\", \"items\" : \"int\" } },\n    { \"name\" : \"stringMapField\", \"type\" : { \"type\" : \"map\", \"values\" : \"string\" } },\n    {\n      \"name\" : \"unionField\",\n      \"type\" : [\n        \"int\",\n        \"string\",\n        \"Fruits\",\n        \"Foo\",\n        { \"type\" : \"array\", \"items\" : \"string\" },\n        { \"type\" : \"map\", \"values\" : \"long\" },\n        \"null\"\n      ]\n    }\n  ]\n}\n\n\ncom/linkedin/pegasus/generator/examples/Fruits.pdsc\n\n{\n  \"type\" : \"enum\",\n  \"name\" : \"Fruits\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.examples\",\n  \"doc\" : \"A fruit\",\n  \"symbols\" : [ \"APPLE\", \"BANANA\", \"ORANGE\", \"PINEAPPLE\" ],\n  \"symbolDocs\" : { \"APPLE\":\"A red, yellow or green fruit.\", \"BANANA\":\"A yellow fruit.\", \"ORANGE\":\"An orange fruit.\", \"PINEAPPLE\":\"A yellow fruit.\"} \n}\n\n\ncom/linkedin/pegasus/generator/examples/MD5.pdsc\n\n{\n  \"type\" : \"fixed\",\n  \"name\" : \"MD5\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.examples\",\n  \"doc\" : \"MD5\",\n  \"size\" : 16\n}\n\n\ncom/linkedin/pegasus/generator/examples/StringList.pdsc\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"StringList\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.examples\",\n  \"doc\" : \"A list of strings\",\n  \"fields\" : [\n    { \"name\" : \"element\", \"type\" : \"string\" },\n    { \"name\" : \"next\"   , \"type\" : \"StringList\", \"optional\" : true }\n  ]\n\n}\n\n\ncom/linkedin/pegasus/generator/examples/InlinedExample.pdsc\n\n{\n  \"type\": \"record\",\n  \"name\": \"InlinedExample\",\n  \"namespace\": \"com.linkedin.pegasus.generator.examples\",\n  \"doc\": \"Example on how you can declare an enum and a record inside another record\",\n  \"fields\": [\n    {\n      \"name\": \"myEnumField\",\n      \"type\": {\n      \"type\" : \"enum\",\n      \"name\" : \"EnumDeclarationInTheSameFile\",\n      \"symbols\" : [\"FOO\", \"BAR\", \"BAZ\"]\n      },\n      \"doc\": \"This is how we inline enum declaration without creating a new pdsc file\",\n      \"symbolDocs\": {\"FOO\":\"It's a foo!\", \"HASH\":\"It's a bar!\", \"NONE\":\"It's a baz!\"}\n    },\n    {\n      \"name\": \"stringField\",\n      \"type\": \"string\",\n      \"doc\": \"A regular string\"\n    },\n    {\n      \"name\": \"intField\",\n      \"type\": \"int\",\n      \"doc\": \"A regular int\"\n    },\n    {\n      \"name\": \"UnionFieldWithInlineRecordAndEnum\",\n      \"doc\": \"In this example we will declare a record and an enum inside a union\",\n      \"type\": [\n        {\n          \"type\" : \"record\",\n          \"name\" : \"myRecord\",\n          \"fields\": [\n            {\n              \"name\": \"foo1\",\n              \"type\": \"int\",\n              \"doc\": \"random int field\"\n            },\n            {\n              \"name\": \"foo2\",\n              \"type\": \"int\",\n              \"doc\": \"random int field\"\n            }\n          ]\n        },\n        {\n          \"name\": \"anotherEnum\",\n          \"type\" : \"enum\",\n          \"symbols\" : [\"FOOFOO\", \"BARBAR\"],\n          \"doc\": \"Random enum\",\n          \"symbolDocs\": {\"FOOFOO\":\"description about FOOFOO\", \"BARBAR\":\"description about BARBAR\"}\n        }\n      ],\n      \"optional\": true\n    }\n  ]\n}\n\n\nTyperef\n\nPegasus supports a new schema type known as a typeref. A typeref is like\na typedef in C. It does not declare a new type but declares an alias to\nan existing type.\n\n\n  Typerefs are useful for differentiating different uses of the same\ntype. For example, we can use to a typeref to differentiate a string\nfield that holds an URN (uniform resource name) from an arbitrary\nstring value or a long field that holds an epoch time in\nmilliseconds from a generic long value.\n  A typeref allows additional meta-data to be associated with\nprimitive and unnamed types. This meta-data can be used to provide\ndocumentation or support custom properties.\n  A typeref provides a way to refer to common unnamed types such as\narrays, maps, and unions. Without typerefs, users may have to wrap\nthese unnamed types with a record in order to address them.\nAlternatively, users may cut-and-paste common type declarations,\nresulting in unnecessary duplication and potentially causing\ninconsistencies if future changes are not propagated correctly to\nall copies.\n\n\nTyperefs use the type name typeref and support the following attributes:\n\n\n  name a JSON string providing the name of the typeref (required).\n  namespace a JSON string that qualifies the name;\n  package a JSON string that qualifies the language binding\nnamespace for this typeref (optional). If this is not specified,\nlanguage binding class for the typeref will use namespace as its\ndefault namespace.\n  doc a JSON string providing documentation to the user of this\nschema (optional).\n  ref the schema that the typeref refers to.\n\n\nNote: Due to the addition of doclint in JDK8, anything under the\ndoc attribute must be W3C HTML 4.01 compliant. This is because the\ncontents of this string will appear as Javadocs in the generated Java\n‘data template’ classes later. Please take this into consideration\nwhen writing your documentation.\n\nHere are a few examples:\n\nDifferentiate URN from string\n\n{\n  \"type\" : \"typeref\",\n  \"name\" : \"URN\",\n  \"ref\"  : \"string\",\n  \"doc\"  : \"A URN, the format is defined by RFC 2141\"\n}\n\n\nDifferentiate time from long\n\n{\n  \"type\" : \"typeref\",\n  \"name\" : \"time\",\n  \"ref\"  : \"long\",\n  \"doc\"  : \"Time in milliseconds since Jan 1, 1970 UTC\"\n}\n\n\nTyperefs (by default) do not alter the serialization format or in-memory\nrepresentation.\n\nRecord Field Attributes\n\nYou can use the following record field attributes to annotate fields\nwith metadata.\n\n\n  optional - marks the field as optional, meaning it doesn’t require\na value. See Optional Fields.\n\n\n\n\nOptional Fields\n\nPegasus supports optional fields implicitly through the “optional” flag\nin the field definition. A field is required unless the field is\ndeclared with the optional flag set to true. A field without the\noptional flag is required. A field with the optional flag set to false\nis also required. An optional field may be present or absent in the\nin-memory data structure or serialized data.\n\nThe Java binding provides methods to determine if an optional field is\npresent and a specialized get accessor allows the caller to specify\nwhether the default value or null should be returned when an absent\noptional field is accessed. The has field accessor may also be used to\ndetermine if an optional field is present.\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"Optional\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.examples\",\n  \"fields\" :\n  [\n    {\n      \"name\" : \"foo\",\n      \"type\" : \"string\",\n      \"optional\" : true\n    }\n  ]\n}\n\n\nOptional field present\n\n{\n  \"foo\" : \"abcd\"\n}\n\n\nOptional field absent\n\n{\n}\n\n\nSee GetMode for more detailed information on how the code\ngenerated data stubs access optional/default fields.\n\nAvro’s Approach to Declaring Optional Fields\n\nDO NOT USE UNION WITH NULL TO DECLARE AN OPTIONAL FIELD IN PEGASUS.\n\nAvro’s approach to declaring an optional field is to use a union type\nwhose values may be null or the desired value type. Pegasus discourages\nthis practice and may remove support for union with null as well as the\nnull type in the future. The reason for this is that declaring an\noptional field using a union causes the optional field to be always\npresent in the underlying in-memory data structure and serialized data.\nAbsence of a value is represented by a null value. Presence of a value\nis represented by a value in the union of the value’s type.\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"OptionalWithUnion\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.examples\",\n  \"fields\" :\n  [\n    {\n      \"name\" : \"foo\",\n      \"type\" : [\"string\", \"null\"]\n    }\n  ]\n}\n\n\noptional field present\n\n{\n  \"foo\" : { \"string\" : \"abcd\" }\n}\n\n\noptional field absent\n\n{\n  \"foo\" : null\n}\n\n\nNote Avro uses the union approach because Avro serialization is\noptimized to not include a field identifier in the serialized data.\n\nUnion\n\nUnion is a powerful way to model data that can be of different types at\ndifferent scenarios. Record fields that have this behavior can be\ndefined as a Union type with the expected value types as its members.\nHere is an example of a record field defined an a union with string\nand array as its members.\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"RecordWithUnion\",\n  \"namespace\" : \"com.linkedin.pegasus.examples\",\n  \"fields\" :\n  [\n    {\n      \"name\" : \"result\",\n      \"type\" : [\n        \"string\",\n        {\n          \"type\" : \"array\",\n          \"items\" : \"Result\"\n        }\n      ]\n    }\n  ]\n}\n\n\nUnion With Members Of The Same Type\n\nIn addition to the unions with disparate member types, Pegasus unions\ncan have more than one member of the same type inside a single union\ndefinition. For example a union can have multiple members that are of,\n\n\n  same primitive type\n  same named complex type\n  array of same or different item types\n  map of same or different value types\n  typeref dereferenced to same type.\n\n\nSuch unions must specify an unique alias for each member in the\nunion definition. When an alias is specified, it acts as the member’s\ndiscriminator unlike the member type name on the standard unions\ndefined above.\n\nIn the example below, the union definition has a string and two\narray members with unique aliases for each of them.\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"RecordWithAliasedUnion\",\n  \"namespace\" : \"com.linkedin.pegasus.examples\",\n  \"fields\" :\n  [\n    {\n      \"name\" : \"result\",\n      \"type\" : [\n        {\n          \"type\" : \"string\",\n          \"alias\" : \"message\"\n        },\n        {\n          \"type\": {\n            \"type\" : \"array\",\n            \"items\" : \"Result\"\n          },\n          \"alias\" : \"successResults\"\n        },\n        {\n          \"type\": {\n            \"type\" : \"array\",\n            \"items\" : \"Result\"\n          },\n          \"alias\" : \"failureResults\"\n        }\n      ]\n    }\n  ]\n}\n\n\nThere are few constraints that must be taken in consideration while\nspecifying aliases for union members,\n\n\n  Aliases must be unique for each member in a union definition.\n  Aliases must be specified for either all or none of the members in a\nunion definition.\n  Aliases cannot be specified for null member types which means\nthere can only be one null member inside a union definition.\n\n\nDefault Values\n\nA field may be declared to have a default value. The default value will\nbe validated against the schema type of the field. For example, if the\ntype is record and it has non-optional fields, then the default value\nmust include the non-optional fields in the default value.\n\nA default value may be declared for a required or an optional field. The\ndefault value is used by the get accessor. Unlike Avro, default values\nare not assigned to absent fields on de-serialization.\n\nIf a default value is declared for a required field, for all three\nGetMode, the get accessor behaves the same as that for an\noptional field with a default value. More specifically, the get accessor\ndoesn’t fail even if the field is absent. However, in this case, data\nvalidation may fail if\nRequiredMode is set to MUST_BE_PRESENT.\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"Default\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.examples\",\n  \"fields\" :\n  [\n    {\n      \"name\" : \"mandatoryWithDefault\",\n      \"type\" : \"string\",\n      \"default\" : \"this is the default string\"\n    },\n    {\n      \"name\" : \"optionalWithDefault\",\n      \"type\" : \"string\",\n      \"optional\" : true,\n      \"default\" : \"this is the default string\"\n    }\n  ]\n}\n\n\nSee GetMode for more detailed information on how the code\ngenerated data stubs access optional/default fields.\n\nDefault Values for Union Types\n\nAn Avro default value for a union type does not include the member\ndiscriminator and the type of the default value must be the first member\ntype in the list of member types.\n\n{\n  ...\n  \"fields\" :\n  [\n    {\n      \"name\" : \"foo\",\n      \"type\" : [ \"int\", \"string\" ],\n      \"default\"  : 42\n    }\n  ]\n}\n\n\nA Pegasus default value for a union type must include the member\ndiscriminator. This allows the same typeref’ed union to have default\nvalues of different member types.\n\n{\n  ...\n  \"fields\" :\n  [\n    {\n      \"name\" : \"foo\",\n      \"type\" : [ \"int\", \"string\" ]\n      \"default\" : { \"int\" : 42 }\n    }\n  ]\n}\n\n\nFor unions with aliased members, the specified alias is used as member\ndiscriminator instead of the type name.\n\n{\n  ...\n  \"fields\" :\n  [\n    {\n      \"name\" : \"foo\",\n      \"type\" : [\n        { \"type\" : \"int\", \"alias\" : \"count\" },\n        { \"type\" : \"string\", \"alias\" : \"message\" }\n      ],\n      \"default\" : { \"count\" : 42 }\n    }\n  ]\n}\n\n\nNote the Avro syntax optimizes the most common union with null pattern\nthat is used to represent optional fields to be less verbose. However,\nPegasus has explicit support for optional fields and discourages the use\nof union with null. This significantly reduces the syntactical sugar\nbenefit of the Avro optimization.\n\nIf a union type may be translated to Avro, all default values provided\nfor the union type must be of the first member type in the list of\nmember types. This restriction is because Avro requires default values\nof a union type to be of the first member type of the union.\n\nIncluding Fields from Another Record\n\nPegasus implements the “include” attribute. It is used to include all\nfields from another record into the current record. It does not include\nany other attribute of the record. Include is transitive, if record A\nincludes record B and record B includes record C, record A contains all\nthe fields declared in record A, record B and record C.\n\nThe value of the “include” attribute should be a list of records or\ntyperefs of records. It is an error to specify non-record types in this\nlist.\n\n{\n  \"doc\"  : \"Bar includes fields of Foo, Bar will have fields f1 from itself and b1 from Bar\",\n  \"type\" : \"record\",\n  \"name\" : \"Bar\",\n  \"include\" : [ \"Foo\" ],\n  \"fields\" : [\n    {\n      \"name\" : \"b1\",\n      \"type\" : \"string\",\n    }\n  ]\n}\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"Foo\",\n  \"fields\" : [\n    {\n      \"name\" : \"f1\",\n      \"type\" : \"string\",\n    }\n  ]\n}\n\n\nDeprecation\n\nNamed schema, record field, and enum symbol can be deprecated using\ndeprecated property or deprecatedSymbols property in enum\ndeclaration. The property value can be a string describing why the\nschema element is deprecated or an alternative, or simply boolean\ntrue. However, the latter use is discouraged and may be removed in the\nfuture. The Java binding generated for these elements will be marked as\ndeprecated.\n\nNamed Schema\n\nTo deprecate a named schema, add deprecated property to its\ndeclaration.\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"Deprecated\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.test\",\n  \"deprecated\": \"Use Foo instead.\",\n  \"fields\" : [\n    ...\n  ]\n}\n\n\nRecord Field\n\nTo deprecate a record field, add deprecated property to its\ndeclaration.\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"Foo\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.test\",\n  \"fields\" : [\n    {\n      \"name\" : \"deprecatedInt\",\n      \"type\" : \"int\",\n      \"deprecated\": \"Reason for int deprecation.\"\n    }\n  ]\n}\n\n\nEnum Symbol\n\nTo deprecate an enum symbol, add deprecatedSymbols property to the\nenum declaration. The value of the property is a map from the symbol\nname to a string description.\n\n{\n  \"name\" : \"Planet\",\n  \"namespace\" : \"com.linkedin.pegasus.generator.test\",\n  \"type\" : \"enum\",\n  \"symbols\" : [ \"MERCURY\", \"VENUS\", \"EARTH\", \"MARS\", \"JUPITER\", \"SATURN\", \"URANUS\", \"NEPTUNE\", \"PLUTO\" ],\n  \"deprecatedSymbols\": {\n    \"PLUTO\": \"Reclassified as dwarf planet.\"\n  }\n}\n\n",
      tags: null,
      id: 47
    });
    
    

  
    index.add({
      title: "Rest.li Protocol",
      category: null,
      content: "Rest.li Protocol\n\nContents\n\n\n  URI Syntax\n  Online Documentation\n  Content Types\n  Rest.li Protocol 2.0 Object and List/Array Representation\n  Complex types, complex keys, and compound keys in the Rest.li 2.0 Protocol\n  Collection Resources\n  Simple Resources\n  Association Resources\n  Finders\n  BatchFinders\n  Actions\n  URI Modifiers\n  Response Status Codes\n  Message Headers\n  Request Message Body\n  Response Message Body\n  Complex Types\n  Empty List Parameters\n\n\nNote - Any time there is a difference between Rest.li protocol 1.0\nand Rest.li protocol 2.0 it will be explicitly mentioned. If nothing is\nsaid, that means that there is no difference.\n\nURI Syntax\n\nURIs are described using URI templates as defined in the IETF Draft\nSpec.\n\nOnline Documentation\n\nRest.li provides online\ndocumentation\nfor any loaded resource. The documentation shows example request and\nresponse for the resource methods, finders, batchFinders and actions. Use it to\ndocument the Rest.li protocols.\n\nContent Types\n\nThe content types of Rest.li data are application/json and\napplication/pson. PSON is a compressed version of JSON.\n\nRest.li Protocol 2.0 Object and List/Array Representation\n\nDefinitions\n\nAn object, as used in this document, is a map or a dictionary. It is\nsimply a collection of key-value pairs, where the keys are strings and\nthe values are primitives, arrays, or an object. The terms object and\nmap are used interchangeably throughout this document to refer to the\nsame concept.\n\nConsider the following functions:\n\nencoded(v) is defined as follows -\n\n\n  if v is a primitive then encoded(v) = the URL encoded value for\nv\n  if v is a map then encoded(v) = the URL encoding for a map as\ndescribed in the Rest.li 2.0 protocol object URL\nrepresentation\n  if v is an array then encoded(v) = the URL encoding for an array\nas described in the Rest.li 2.0 protocol array URL\nrepresentation\n\n\nreducedEncoded(v) is defined as follows -\n\n\n  if v is a primitive then reducedEncoded(v) = v with the\ncharacters ,, (, ), ', and : URL encoded\n  if v is a map then reducedEncoded(v) = the HTTP body/header\nencoding for a map as described in the Rest.li 2.0 protocol object\nHTTP body and headers\nrepresentation\n  if v is an array then reducedEncoded(v) = the HTTP body/header\nencoding for an array as described in the Rest.li 2.0 protocol\narray HTTP body and headers\nrepresentation\n\n\nencoded and reducedEncoded will be used in the sections below.\n\nJSON Serialization of Data Schema\n\nRest.li objects defined using using pegasus data schemas are serialized\nin a JSON representation for transportation over the wire. For detailed\ntransport serialization, please see\nHow Data is Serialized for Transport.\n\nRest.li Protocol 2.0 Object Representation\n\nURL Representation\n\nIn Rest.li 2.0, the way to represent an object in the URL is using\nkey:value pairs. More concretely -\n\n(encoded(k1):encoded(v1),encoded(k2):encoded(v2),...)\n\n\nNote that all keys are strings.\n\nHTTP body and headers representation\n\nHeader representation\n\nAn object can be present as values for the following headers -\n\n\n  Location - if present here we simply use the\nRest.li 2.0 protocol object URL\nrepresentation\n  X-RestLi-Id or X-LinkedIn-Id - we use the\nRest.li 2.0 protocol object HTTP body and headers\nrepresentation\n\n\nBody representation\n\nIf present in the HTTP body a key:value representation is used. More\nconcretely -\n\n(reducedEncoded(k1):reducedEncoded(v1),reducedEncoded(k2):reducedEncoded(v2),...)\n\n\nRest.li 2.0 Protocol Array Notation\n\nURL Representation\n\nAn array \\[a1, a2, a3, …\\] is encoded in the URL as -\n\nList(encoded(a1),encoded(a2),encoded(a3),...)\n\n\nHTTP body and headers representation\n\nHeader representation\n\nAn array can be present as values for the following headers -\n\n\n  Location - if present here we simply use the\nRest.li 2.0 protocol array URL\nrepresentation\n  X-RestLi-Id or X-LinkedIn-Id - we use the\nRest.li 2.0 protocol array HTTP body and headers\nrepresentation\n\n\nBody representation\n\nAn array [a1, a2, a3, ...] is encoded as follows -\n\nList(reducedEncoded(a1),reducedEncoded(a2),reducedEncoded(a3),...)\n\n\nExample\n\nConsider the object, which we will call exampleObject, expressed in a\nJSON notation here -\n\n{\n  \"k1\": \"v1\",\n  \"k2\": \"value with spaces\",\n  \"k3\": [1, 2, 3],\n  \"k4\": \"value:with:reserved:char\"\n  \"k5\":\n  {\n    \"k51\": \"v51\",\n    \"k52\": \"v52\"\n  }\n}\n\n\nHere is how exampleObject would look if present in the URL -\n\n(k1:v1,k2:value%20with%20spaces,k3:List(1,2,3),k4:value%3Awith%3Areserved%3Achar,k5:(k51:v51,k52:v52))\n\n\nHere is how exampleObject would look if present in the HTTP headers -\n\nLocation: ...(k1:v1,k2:value%20with%20spaces,k3:List(1,2,3),k4:value%3Awith%3Areserved%3Achar,k5:(k51:v51,k52:v52))\nX-RestLi-Id: (k1:v1,k2:value with spaces,k3:List(1,2,3),k4:value%3Awith%3Areserved%3Achar,k5:(k51:v51,k52:v52))\n\n\nIf we were doing a BATCH_GET request and exampleObject was one of the\nkeys requested here is how the HTTP body would look -\n\n...\n\"entities\": {\n  \"(k1:v1,k2:value with spaces,k3:List(1,2,3),k4:value%3Awith%3Areserved%3Achar,k5:(k51:v51,k52:v52))\": {\n    ...\n  }\n  ...\n}\n...\n\n\nComplex types, complex keys, and compound keys in the Rest.li 2.0 Protocol\n\nA complex type is simply a map.\n\nA complex key is a key made up of two parts, key and $params, each\nof which is a complex type.\n\nA compound key is a complex type with the restriction that all the\nvalues are primitives. A compound key cannot have maps or arrays as\nvalues for the keys making up its map.\n\nComplex and compound keys have a similar structure: they are essentially\na collection of key-value pairs. Because of this similarity in structure\nwe decided to represent both using the Rest.li 2.0 object notation. Any\nlist present as a value in the complex key uses the Rest.li 2.0 list\nnotation. Details can be found in the Association\nkeys\nand complex\nkeys\n\nCollection Resources\n\nThe URI templates below assume variables with types as follows:\n\ncollection : simple string or \"complex key\"\nentity_id : simple string\nids : list\nfinder : simple string\nbatch_finder : simple string\nparams : associative array\n\n\nCollection URIs\n\n\n  \n    \n      Resource\n      URI Template\n      Example\n      Method\n      Semantics\n    \n    \n      Collection\n      /{collection}\n      /statuses\n      POST\n      CREATE - creates an entity in the collection\n    \n    \n      Collection\n      /{collection}/{entity_id}\n      /statuses/1\n      GET\n      READ - returns the referenced entity\n    \n    \n      Collection\n      /{collection}/{entity_id}\n      /statuses/1\n      PUT\n      UPDATE - updates the referenced entity\n    \n    \n      Collection\n      /{collection}/{entity_id}\n      /statuses/1\n      POST\n      PARTIAL UPDATE - partially updates the referenced entity\n    \n    \n      Collection\n      /{collection}/{entity_id}\n      /statuses/1\n      DELETE\n      DELETE - deletes the referenced entity\n    \n    \n      Collection\n      /{collection}?{ids}\n      Protocol 1.0 - /statuses?ids=1&amp;ids=2&amp;ids=3Protocol 2.0 - /statuses?ids=List(1,2,3)\n      GET\n      BATCH_GET - returns a map containing the referenced entities\n    \n    \n      Collection\n      /{collection}\n      /statuses\n      GET\n      GET_ALL - returns all entities in the collection\n    \n    \n      Collection\n      /{collection}?q={finder}\n      /statuses?q=search\n      GET\n      FINDER - returns a list containing entities satisfying the query\n    \n    \n      Collection\n      /{collection}?q={finder}{&amp;params*}\n      /statuses?q=search&amp;keywords=linkedin\n      GET\n      FINDER - returns a list containing entities satisfying the query\n    \n    \n      Collection\n      /{collection}?bq={batch_finder}\n      /statuses?bq=search\n      GET\n      BATCH_FINDER - returns a list containing entities satisfying the query\n    \n    \n      Collection\n      /{collection}?bq={batch_finder}{&amp;params*}\n      /statuses?bq=search&amp;criteria=List((id:1, title:bar),(id:2, title:foo))\n      GET\n      BATCH_FINDER - returns a list containing entities satisfying the query\n    \n    \n      Collection\n      /{collection}?action={action}\n      /statuses?action=purge\n      POST\n      ACTION - some operation, rest.li does not specify any standard behavior\n    \n  \n\n\nSimple Resources\n\nThe URI templates below assume variables with types as follows:\n\nsimple : simple string\nparams : associative array\n\n\nSimple URIs\n\n\n  \n    \n      Resource\n      URI Template\n      Example\n      Method\n      Semantics\n    \n  \n  \n    \n      Simple\n      /{simple}\n      /selectedItem\n      GET\n      READ - returns the entity\n    \n    \n      Simple\n      /{simple}\n      /selectedItem\n      PUT\n      UPDATE - updates the entity\n    \n    \n      Simple\n      /{simple}\n      /selectedItem\n      DELETE\n      DELETE - deletes the entity\n    \n    \n      Simple\n      /{simple}?action={action}\n      /selectedItem?action=investigate\n      POST\n      ACTION - some operation, rest.li does not specify any standard behavior\n    \n  \n\n\nAssociation Resources\n\nAssociations contain entities referenced by compound keys, referred to\nhere as assockeys\n\nThe URI templates below assume variables with types as follows:\n\nfirstkey : associative array containing a single element\nkeys : associative array\nassociation : simple string\nassockey : simple string conforming to the assockey syntax described below\nassockeys : list of strings conforming to the assockey syntax\nfinder : simple string\nbatch_finder : simple string\nparams : associative array\n\n\nAssociation Keys\n\nAssociation keys are composed of one or more named assocKey parts.\n\nIn protocol 1.0 the key is represented over the wire in the form:\n\n{firstkey}{&amp;keys*}\n\n\nIn protocol 2.0 the key is represented using the protocol 2.0 object\nnotation, with each assocKey being a key in the map.\n\nFor example, a two part key identifying an edge in a following graph in\nprotocol 1.0 might be:\n\nfollowerID=1&amp;followeeID=3\n\n\nIn protocol 2.0 the key would be\n\n(followerID:1,followeeID:3)\n\n\nHere’s an example association GET request/response. In protocol 1.0:\n\nGET /associations/src=KEY1&amp;desk=KEY2\n{\n    \"message\": \"Hi!\",\n    \"id\": \"1\"\n}\n\n\nIn protocol 2.0:\n\nGET /associations/(src:KEY1,desk:KEY2)\n{\n    \"message\": \"Hi!\",\n    \"id\": \"1\"\n}\n\n\nIn finders, only some keys from the full association key might be\nrequired. For example, in protocol 1.0:\n\nfollowerID=1\n\n\nIn protocol 2.0:\n\n(followerID:1)\n\n\nAll string values for association keys are url encoded. E.g. for the\nassociation key composed of code=“1=2b” and widget=“xyz widget”, a GET\nrequest in protocol 1.0 using the key would be:\n\nGET /resourceName/code=1%32b&amp;widget=xyz%20widget\n\n\nIn protocol 2.0 it would be:\n\nGET /resourceName/(code:1%32b,widget:xyz%20widget)\n\n\nWhen association keys are used in a batch operation, each key is url\nencoded. For protocol 1.0 the form is:\n\nids=urlencoded(associationKey1)&amp;ids=urlencoded(associationKey2)...\n\n\nFor protocol 2.0 is ids use the protocol 2.0 array notation.\n\nids=List((encoded(associationKey1)),(encoded(associationKey2)),...)\n\n\nFor example, in protocol 1.0 a batch get for the keys:\nsrc=KEY1\\&amp;dest=KEY3 and src=KEY1\\&amp;dest=KEY2,\nwould\n    be:\n\nGET /associations?ids=src%3DKEY1%26dest%3DKEY2&amp;ids=src%3DKEY1%26dest%3DKEY3\n\n{\n  \"errors\": {},\n  \"results\": {\n      \"dest=KEY3&amp;src=KEY1\": {\n          \"message\": \"Hi!\",\n          \"id\": \"1\"\n      },\n      \"dest=KEY2&amp;src=KEY1\": {\n          \"message\": \"Hello!\",\n          \"id\": \"2\"\n      }\n  }\n}\n\n\nIn protocol 2.0 a batch get for the keys:\n(src:KEY1,dest:KEY3) and (src:KEY1,dest:KEY2),\nwould\n    be:\n\nGET /associations?ids=List((src:KEY1,dest:KEY3),(src:KEY1,dest:KEY2))\n\n{\n  \"errors\": {},\n  \"results\": {\n      \"(dest:KEY3,src:KEY1)\": {\n          \"message\": \"Hi!\",\n          \"id\": \"1\"\n      },\n      \"(dest:KEY2,src:KEY1)\": {\n          \"message\": \"Hello!\",\n          \"id\": \"2\"\n      }\n  }\n}\n\n\nHere’s the basic form of a batch update request using association keys.\nIn protocol 1.0:\n\nPUT /resourceName?ids=urlencoded(key1=urlencoded(value)&amp;key2=urlencoded(value)&amp;...)&amp;ids=...\n\n{\n  \"entities\": {\n    \"key=urlencoded(value)&amp;key2...\": { ... },\n    ...\n  }\n}\n\n\nNote that in the URL the ids are url encoded AND any strings values for\nthe assocKey parts are double url encoded.\n\nIn protocol 2.0 the protocol 2.0 array representation is used for the\nids.\n\nFor example, for a batch update for the association keys: (code=“1=2b”,\nname=“xyz widget”) and (code=“567”, name=“rachet”)\n\nThe batch update request in protocol 1.0 would be:\n\nPUT /widgets?ids=code%3D1%2532b%26widget%3Dxyz%2520widget&amp;ids=code%3D567%26widget%3Drachet\n\n{\n  \"entities\": {\n     \"code=1%32b&amp;name=xyz%20widget\": {...},\n     \"code=567&amp;name=rachet\": {...}\n  }\n}\n\n\nIn protocol 2.0 the request would be:\n\nPUT /widgets?ids=List((code:1%202b,name:xyz%20widget),(code:567,name:rachet))\n\n{\n  \"entities\": {\n     \"(code:1=2b,name:xyz widget)\": {...},\n     \"(code:567,name:rachet)\": {...}\n  }\n}\n\n\nAssociation URIs\n\n\n  \n    \n      Resource\n      URI Template\n      Example\n      Method\n      Semantics\n    \n    \n      Association\n      /{association}/{+assockey}\n      Protocol 1.0 - /follows/followerID=1&amp;followeeID=1 Protocol 2.0 - /follows/(followerID:1,followeeID:1)\n      GET\n      READ - returns the referenced association entity\n    \n    \n      Association\n      /{association}/{+assockey}\n      Protocol 1.0 - /follows/followerID=1&amp;followeeID=1Protocol 2.0 - /follows/(followerID:1,followeeID:1)\n      PUT\n      UPDATE - updates the referenced association entity\n    \n    \n      Association\n      /{association}/{+assockey}\n      Protocol 1.0 - /follows/followerID=1&amp;followeeID=1Protocol 2.0 - /follows/(followerID:1,followeeID:1)\n      DELETE\n      DELETE - deletes the referenced association entity\n    \n    \n      Association\n      /{association}/{+assockeys*}\n      Protocol 1.0 - /follows/?ids=followerID%3D1%26followeeID%3D1&amp; ids=followerID%3D1%26followeeID%3D3&amp;ids=followerID%3D1%26followeeID%3D2 \\Note: followerID%3D1%26followeeID%3D1 unescapes to followerID=1&amp;followeeID=1Protocol 2.0 - /follows/?ids=List((followerID:1,followeeID:1),(followerID:1,followeeID:2))\n      GET\n      BATCH_GET - returns a map containing the referenced association entities\n    \n    \n      Association\n      /{association}\n      /follows\n      GET\n      GET_ALL - returns all the association entities\n    \n    \n      Association\n      /{association}?q={finder}\n      /follows?q=search\n      GET\n      FINDER - returns a list containing entities satisfying the query\n    \n    \n      Association\n      /{association}?q={finder}{&amp;params*}\n      /follows?q=followers&amp;userID=1\n      GET\n      FINDER - returns a list containing entities satisfying the query\n    \n    \n      Association\n      /{association}/{+assockey}\n      Protocol 1.0 - /follows/followerID=1?q=otherProtocol 2.0 - /follows/(followerID:1)?q=other\n      GET\n      FINDER - returns a list containing the entities satisfying the query\n    \n    \n      Association\n      /{association}/{+assockey}?q={finder}{&amp;params*}\n      Protocol 1.0 - /follows/followerID=1?q=other&amp;someParam=valueProtocol 2.0 - /follows/(followerID:1)?q=other&amp;someParam=value\n      GET\n      FINDER - returns a list containing the entities satisfying the query\n    \n    \n      Association\n      /{association}?bq={batch_finder}\n      /statuses?bq=search\n      GET\n      BATCH_FINDER - returns a list containing entities satisfying the query\n    \n    \n      Association\n      /{association}?bq={batch_finder}{&amp;params*}\n      /statuses?bq=search&amp;criteria=List((id:1, title:bar),(id:2, title:foo))\n      GET\n      BATCH_FINDER - returns a list containing entities satisfying the query\n    \n    \n      Association\n      /{association}/{+assockey}\n      Protocol 1.0 - /follows/followerID=1?bq=otherProtocol 2.0 - /follows/(followerID:1)?bq=other\n      GET\n      BATCH_FINDER - returns a list containing the entities satisfying the query\n    \n    \n      Association\n      /{association}/{+assockey}?bq={batch_finder}{&amp;params*}\n      Protocol 2.0 - /follows/(followerID:1)?q=other&amp;someParam=List((id:1, title:bar),(id:2, title:foo))\n      GET\n      BATCH_FINDER - returns a list containing the entities satisfying the query\n    \n    \n      Association\n      /{association}?action={action}\n      /follows?action=purge\n      POST\n      ACTION - some operation, Rest.li does not specify any standard behavior\n    \n  \n\n\nFinders\n\nThe URI templates below assume variables with types as follows:\n\nfinder : simple string identifying a finder\n\n\nFinder URIs\n\n\n  \n    \n      Resource\n      URI Template\n      Example\n      Method\n      Semantics\n    \n  \n  \n    \n      Collection, Association, ActionSet\n      {resource}?q={finder}\n      /accounts?q=keywordSearch\n      GET\n      invokes the specified finder\n    \n  \n\n\nBatch Finders\nBatch Finder URIs\nThe URI templates below assume variables with types as follows:\n\nbatch_finder : simple string identifying a batch_finder method name\nresource : simple string identifying a resource \nsearch_criteria : simple string identifying the criteria filter name\n\n\n\n  \n    \n      Resource\n      URI Template\n      Example\n      Method\n      Semantics\n    \n  \n  \n    \n      Collection, Association\n      {resource}?bq={batch_finder}&amp;{search_criteria}=\n      /PhotoResource?bq=searchPhotos&amp;photoCriteria=List((id:1, format:JPG),(id:2, format:BMP))\n      GET\n      invokes the specified batch_finder\n    \n  \n\n\nAt least, 2 query parameters will have to be set for a batch finder:\n\n\n  The “bq” query parameter is reserved for passing the batch finder method name\n  A second query parameter will be used to pass a set of different search criteria. The name of this query parameter is set in the BatchFinder method annotation.\nFor example, with @BatchFinder(value=”findUsers”, batchParam=”batchCriteria”), the batch query parameter name is “batchCriteria”. \nThe type of this query parameter is a List.\n  “q” cannot be used as a name for query parameters. It is reserved for passing Finder’s method name. For URL constructed which uses both “bq” and “q”, e.g. “ {resource}?bq={bq_argument}&amp;q={q_argument}”, it will be interpreted as using “q_argument” as a finder method name and “bq_argument” as argument for a query parameter named “bq”.\n\n\nDifferent data type has different representation in Rest.li protocol 1.0 and 2.0. See more details in Rest.li Protocol.\n\nEg.\nIn Rest.li protocol 1.0\n\ncurl \"http://localhost:8080/userSearchResults?bq=findUsers&amp;batchCriteria[0].firstName=pauline&amp;batchCriteria[0].age=12&amp;batchCriteria[1].lastName=iglou\" --globoff\n\n\nIn Rest.li protocol 2.0\n\ncurl --header \"X-RestLi-Protocol-Version: 2.0.0\" \"http://localhost:10546/userSearchResults?q=findUsers&amp;batchCriteria=List((firstName:pauline, age:12),(lastName:iglou))\"\n\n\nThe other query parameters will be applied as common filters across all batch requests.\n\nHere is an example batch request with two individual finders using the following criteria:\n\n\n  filter by first name and age\n  filter by last name and age\n\n\nEg.\n\ncurl \"http://localhost:8080/userSearchResults?bq=findUsers&amp;batchCriteria=List((firstName:pauline),(lastName:iglou))&amp;age=21\" -X GET --header \"X-RestLi-Protocol-Version: 2.0.0\"\n\n\nPagination support\n1) Common pagination for all search criteria\nThe developer can pass additional parameters to specify a common pagination. It will be more efficient than adding a pagination context inside each criteria object.\nEg.\n\ncurl \"http://localhost:8080/userSearchResults?q=findUsers&amp;batchCriteria=List((firstName:pauline, age:12),(lastName:iglou))&amp;firstName=max&amp;start=10&amp;count=10\" -X GET  --header \"X-RestLi-Protocol-Version: 2.0.0\"\n\n\nThe “start” and “count” params will be automatically mapped to a PagingContext object that will be passed to the resource method.\npublic BatchFinderResult&lt;SearchCriteria, User, EmptyRecord&gt; findUsers(@PagingContextParam PagingContext context, \n                                                                      @QueryParam(\"batchCriteria\") SearchCriteria[] criteria, \n                                                                      @QueryParam(\"firstName\") String firstName)\n\n\n2) Custom pagination per criteria object\nIf the developer wants to apply a custom pagination for each search criteria, the pagination information can be passed into the the search criteria object itself.\nCaution: Rest.li doesn’t validate how the developer models the pagination in the Search criteria RecordTemple. For consistency purpose, we recommend to use a PagingContext.\nIt’s the developer responsibility to apply the right pagination (common or custom) based on its need in the resource method implementation.\n\nActions\n\nThe URI templates below assume variables with types as follows:\n\naction : simple string\n\n\nAction URIs\n\n\n  \n    \n      Resource\n      URI Template\n      Example\n      Method\n      Semantics\n    \n  \n  \n    \n      Collection, Simple, Association, ActionSet\n      {resource}?action={action}\n      /accounts?action=register\n      POST\n      invokes the specified action\n    \n  \n  \n    \n      Collection, Association\n      {resource}/{resourceId}?action={action}\n      /accounts/10086?action=revoke\n      POST\n      invokes the specified action under specified entity\n    \n  \n\n\nURI Modifiers\n\nThe URI templates below assume variables with types as follows:\n\nfinder_uri : simple string ...\nbatch_finder_uri : simple string\nbase_uri : simple string generated via one of the uri templates above\nstart : simple string\ncount : simple string\nfields : list\n\n\nURIs\n\n\n  \n    \n      Feature\n      Base URI Type\n      URI Template\n      Example\n    \n  \n  \n    \n      Paging\n      Finder, BatchFinder\n      {+finder_uri}{\\&amp;start,count} &lt;/br&gt; {+batch_finder_uri}{\\&amp;start,count}\n      /statuses?q=search\\&amp;start=0\\&amp;count=10 &lt;/br&gt; /statuses?bq=search\\&amp;start=0\\&amp;count=10\n    \n    \n      Projection\n      Get, BatchGet, Finder, BatchFinder\n      {+base_uri}{\\&amp;fields}\n      Protocol 1 - /groups?q=emailDomain\\&amp;fields=locale,state Protocol 2 - /groups?q=emailDomain\\&amp;fields=List(locale,state)\n    \n    \n      Schema Return\n      Any\n      {+base_uri}\\&amp;metaDesc\n       \n    \n    \n      Links\n      Any\n      {+base_uri}\\&amp;metaLinks\n       \n    \n  \n\n\nResponse Status Codes\n\nStatus codes should be interpreted according to the HTTP\nspecification\n\nCommon status codes used in Rest.li:\n\n\n  200 OK\n  201 Created\n  204 No Content\n  400 Bad Request\n  404 Not Found\n  405 Method Not Allowed\n  500 Internal Server\nError\n\n\nMessage Headers\n\n\n  \n    \n      Message Type\n      Header\n      Semantics\n      Notes\n    \n  \n  \n    \n      Response\n      X-LinkedIn-Error-Response\n      indicates whether the message body contains a JSON-serialized ErrorResponse object\n      The header value is set to “true” when an error response is returned. The header is omitted otherwise. Only used in protocol 1.0\n    \n    \n      Response\n      X-RestLi-Error-Response\n      indicates whether the message body contains a JSON-serialized ErrorResponse object\n      The header value is set to “true” when an error response is returned. The header is omitted otherwise. Only used in protocol 2.0\n    \n    \n      Response\n      X-LinkedIn-Id\n      indicates the id assigned by the server to a new entity created in a collection.\n      set on response messages resulting from a successful POST request to create an entity. The header value is set to the entity id, represented as a string. Only used in protocol 1.0\n    \n    \n      Response\n      X-RestLi-Id\n      indicates the id assigned by the server to a new entity created in a collection.\n      set on response messages resulting from a successful POST request to create an entity. The header value is set to the entity id, represented as a string. Only used in protocol 2.0\n    \n    \n      Response\n      Location\n      indicates the URI of a new entity created in a collection.\n      Location is set on response messages resulting from a successful POST request to create an entity. The header value is set to a URI referencing the newly created entity\n    \n    \n      Response\n      Content-Type\n       \n      The Content-Type is always set to “application/json”\n    \n    \n      Request\n      X-RestLi-Method\n      Set whenever content is POSTed. Can be “get_all”, “get”, “batch_get”, “create”, “batch_create”, “update”, “partial_update”, “delete”, “batch_delete”, “action”, “finder”, “batch_finder”, “batch_partial_update”\n      Is only required for “batch_create”, “batch_partial_update”, all other method types can be inferred by a RestLi server from the URI string and HTTP Method.\n    \n    \n      Request and Response\n      X-RestLi-Protocol-Version\n      Version of the Rest.li protocol used to generate the request or response. Example value: “2.0.0”\n      The version that we get back in the response is dictated by the version sent in the request. They will always be the same.\n    \n  \n\n\nRequest Message Body\n\nSingle Entity\n\nSingle entities are sent as the JSON serialized DataMap representing\nthat entity. The \"Content-Type: application/json\" header may be\nincluded for PUT and POST requests, if omitted, the server will assume\nthe request is content type is \"application/json\".\n\nCreate:\n\nPOST /widgets\nContent-Type: application/json\n\n{\"widgetName\":\"Lever\"}\n\n\nRead:\n\nfor collection and association resources:\n\nGET /widgets/1\n\n\nfor simple resources:\n\nGET /currentWidget\n\n\nUpdate:\n\nfor collection and association resources:\n\nPUT /widgets/1\nContent-Type: application/json\n\n{\"widgetName\":\"Lever\"}\n\n\nfor simple resources:\n\nPUT /currentWidget\nContent-Type: application/json\n\n{\"widgetName\":\"Lever\"}\n\n\nDelete:\n\nfor collection and association resources:\n\nDELETE /widgets/1\n\n\nfor simple resources:\n\nDELETE /currentWidget\n\n\nBatch Create\n\nA DataMap with field\n\n\n  “elements”: A JSON array of the resources to batch create/update and\nthe objects are the json serialized values of each resource to\ncreate.\n\n\nE.g.\n\nPOST /widgets HTTP/1.1\nContent-Type: application/json\nX-RestLi-Method: batch_create\n\n{\n  \"elements\": [\n    {\"widgetName\":\"Ratchet\"},\n    {\"widgetName\":\"Cog\"},\n    {\"widgetName\":\"!@&amp;%@$#\"}\n  ]\n}\n\n\nResponse:\n\n{\n  \"elements\": [\n    {\n      \"status\": 201,\n      \"id\": \"100\"\n    },\n    {\n      \"status\": 201,\n      \"id\": \"101\"\n    },\n    {\n      \"status\": 406,\n      \"error\": {\n        \"status\": 406,\n        \"stackTrace\": \"...\",\n        \"errorDetails\": { ... },\n        \"serviceErrorCode\": 999, \n        \"exceptionClass\": \"...\", \n        \"message\": \"...\"\n      }\n  ]\n}\n\n\nResponses are associated to the request by array index. E.g. “Rachet”\nwas assigned id 100, “Cog” was assigned id 101 and “!&amp;%$#” resulted\non a 406 response code and was not assigned an id.\n\nNote: Batch create requests must include the HTTP Header:\n\nX-RestLi-Method: batch_create\n\nBatch Update\n\nA DataMap with field\n\n\n  “entities”: A JSON serialized map where the keys are keys of the\nresources to update and the objects are the json serialized\nreplacement values of each resource to update.\n\n\nE.g.\n\nPUT /widgets?ids=1&amp;ids=2 HTTP/1.1\nContent-Type: application/json\nX-RestLi-Method: batch_update\n\n{\n  \"entities\": {\n    \"1\": {\"widgetName\":\"Trebuchet\"},\n    \"2\": {\"widgetName\":\"Gear\"}\n   }\n}\n\n\nResponse:\n\n{\n  \"errors\": {},\n  \"results\": {\n    \"1\": {\n      \"status\": 204\n    },\n    \"2\": {\n      \"status\": 204\n    }\n  ]\n}\n\n\nPartial Update\n\nPartial update is a set of operations on data object, which is also an\ninstance of DataMap. Operations are expressed using fields with reserved\nword names. Every operation relates to the object that contains it,\ni.e., it’s parent. The following is an example of setting the zipCode\nof the businessAddress, setting name and homeAddress, and deleting\nnote and birthday of a record.\n\nE.g.\n\nPOST /widgets/1 HTTP/1.1\nContent-Type: application/json\n\n{\n  \"patch\": {\n    \"businessAddress\": {\n      \"$set\": {\n        \"zipCode\": \"94086\"\n      }\n    },\n    \"$set\": {\n      \"name\": \"John\",\n      \"homeAddress\": {\n        \"street\": \"10th\",\n        \"city\": \"Sunnyvale\"\n      }\n    },\n    \"$delete\": [\"note\", \"birthday\"]\n  }\n}\n\n\nPatch\n\nA patch object describes the operations needed to change one map to\nanother, which are the underlying data structure for record, union and\nmap. A patch object is represented by a map itself. It has three types\nkeys.\n\n\n  “$set” indicates a set operation. Its value is a map, indicating the\nkeys and values to be set. This is usually used to set fields in a\nrecord.\n  “$delete” indicates a delete operation. Its value is a string array,\nindicating the map entries to be deleted. This is usually used to\ndelete fields in a record.\n  Other key indicates a patch operation. It must be an existing key,\nwhose value must be a map, of the map to be changed. Its value is\nanother patch object. This is usually used to update a field in a\nnested record.\n\n\nBatch Partial Update\n\nSee Partial Update and Batch Update above for details, here the two are\ncombined.\n\nE.g.\n\nPOST /widgets?ids=1&amp;ids=2 HTTP/1.1\nContent-Type: application/json\nX-RestLi-Method: batch_partial_update\n\n{\n  \"entities\": {\n    \"1\": {\"patch\": { \"$set\": { \"name\":\"Sam\"}}},\n    \"2\": {\"patch\": { \"$delete\": [\"name\"]}}\n   }\n}\n\n\n\n\nAction\n\nAction params are provided in the request body which must contain a data\nmap keyed by param names. E.g.\n\nPOST /widgets?action=purge HTTP/1.1\nContent-Type: application/json\n{\n  \"reason\": \"spam\",\n  \"purgedByAdminId\": 1\n}\n\n\nThe X-RestLi-Method: action may optionally be included, but is not\nrequired because rest.li is able to determine the request is an action\nbased on the “action” query param key.\n\nResponse Message Body\n\nSingle Entity\n\nSingle entities are returned as the JSON serialized DataMap representing\nthat entity\n\nList of Entities\n\nLists of entities are returned in a com.linkedin.rest.CollectionResponse\nwrapper. They are returned by finders and getAll.\n\nCollectionResponse fields:\n\n\n  “elements” : JSON serialized list of entity types\n  (optional) “paging” : JSON serialized CollectionMetadata object\n\n\nE.g.\n\n{\n  \"elements: [\n    { \"id\": 1, \"message\": \"Good morning!\", \"tone\": \"FRIENDLY\" }\n    // ...\n  ],\n  \"metadata\" { // only returned by finders that define a metadata schema as part of the interface\n    // ...\n  },\n  \"paging\": {\n    \"count\": 10,\n    \"links\": [\n      \"href\": \"/greetings?count=10&amp;start=10&amp;q=search\",\n      \"rel\": \"next\",\n      \"type\": \"application/json\"\n    ],\n    \"start\": 0\n  }\n}\n\n\nMap of Entities\n\nMaps of entities are returned in a\ncom.linkedin.restli.common.BatchResponse or\ncom.linkedin.restli.client.response.BatchKVResponse\nwrapper.\n\nBatchResponse fields:\n\n\n  “results” : JSON object containing name/value pairs\n    \n      name is the string value of each map key. This is serialized\naccording to the protocol version.\n      value is the JSON serialized entity for each map value. This is\nserialized according to the protocol version.\n    \n  \n  “errors”: another map mapping keys which failed to generate a 2xx\nresponse to the reason, which is a ErrorResponse\n\n\nProtocol 1 example -\n\nGET /fortunes?ids=1&amp;ids=2&amp;ids=unacceptableKey&amp;ids=faultyKey\n{\n  \"errors\": {\n     \"unacceptableKey\": {\n       \"status\": 416,\n       \"message\": \"Not Acceptable\"\n     },\n     \"faultyKey\": {\n       \"status\": 500,\n       \"exceptionClass\": \"org.example.SomeServerException\",\n       \"stacktrace\": \"SomeServerException at org.example.SomeClass.someMethod(SomeClass.java:1)\\n...\"\n     }\n   },\n  \"results\": {\n    \"1\": {...},\n    \"2\": {...}\n    }\n  }\n}\n\n\nProtocol 2 example -\n\nGET /fortunes?ids=List(1,2,unacceptableKey,faultyKey)\n{\n  \"errors\": {\n     \"unacceptableKey\": {\n       \"status\": 416,\n       \"message\": \"Not Acceptable\"\n     },\n     \"faultyKey\": {\n       \"status\": 500,\n       \"exceptionClass\": \"org.example.SomeServerException\",\n       \"stacktrace\": \"SomeServerException at org.example.SomeClass.someMethod(SomeClass.java:1)\\n...\"\n     }\n   },\n  \"results\": {\n    \"1\": {...},\n    \"2\": {...}\n    }\n  }\n}\n\n\nCollection Metadata\n\n{\"type\":\"record\",\n \"name\":\"CollectionMetadata\",\n \"namespace\":\"com.linkedin.common.rest\",\n \"doc\":\"Metadata and pagination links for this collection\",\n \"fields\":[\n {\n   \"name\":\"start\",\n   \"type\":\"int\",\n   \"doc\":\"The start index of this collection\"\n },{\n   \"name\":\"count\",\n   \"type\":\"int\",\n   \"doc\":\"The number of elements in this collection segment\"\n },{\n   \"name\":\"total\",\n   \"type\":\"int\",\n   \"doc\":\"The total number of elements in the entire collection (not just this segment)\",\n   \"default\":0\n },{\n   \"name\":\"links\",\n   \"type\":{\n     \"type\":\"array\",\n     \"items\":{\n       \"type\":\"record\",\n       \"name\":\"Link\",\n       \"doc\":\"A atom:link-inspired link\",\n       \"fields\":[\n       {\n         \"name\":\"rel\",\n         \"type\":\"string\",\n         \"doc\":\"The link relation e.g. 'self' or 'next'\"\n       },{\n         \"name\":\"href\",\n         \"type\":\"string\",\n         \"doc\":\"The link URI\"\n       },{\n         \"name\":\"type\",\n         \"type\":\"string\",\n         \"doc\":\"The type (media type) of the resource\"\n       }]\n     }\n   },\n   \"doc\":\"Previous and next links for this collection\"\n}]}\n\n\nError Response\n\n{\n  \"type\":\"record\",\n  \"name\":\"ErrorResponse\",\n  \"namespace\":\"com.linkedin.common.rest\",\n  \"doc\":\"A generic ErrorResponse\",\n  \"fields\":[\n  {\n    \"name\":\"status\",\n    \"type\":\"int\",\n    \"doc\":\"The HTTP status code\"\n  },{\n    \"name\":\"serviceErrorCode\",\n    \"type\":\"int\",\n    \"doc\":\"An service-specific error code (documented in prose)\"\n  },{\n    \"name\":\"message\",\n    \"type\":\"string\",\n    \"doc\":\"A human-readable explanation of the error\"\n  },{\n    \"name\":\"exceptionClass\",\n    \"type\":\"string\",\n    \"doc\":\"The FQCN of the exception thrown by the server (included the case of a server fault)\"\n  },{\n    \"name\":\"stackTrace\",\n    \"type\":\"string\",\n    \"doc\":\"The full (??) stack trace (included the case of a server fault)\"\n  }]\n}\n\n\n\n\nAction Response\n\nActions may optionally return a response body. If they do, it must\ncontain a data map with a single “value” key, where the value of the key\nis either a primitive type, e.g.:\n\n{\n  \"value\": 1\n}\n\n\nor complex data type, e.g.:\n\n{\n  \"value\": {\n    \"firstName\": \"John\",\n    \"lastName\": \"Smith\"\n  }\n}\n\n\nBatch Collection Response\nA list of BatchFinderCriteriaResult are returned in a BatchCollectionResponse wrapper. \nIt is used for returning an ordered, variable-length, navigable collection of resources for BATCH_FINDER.\nThis means, BatchFinderCriteriaResult objects are expected to be returned in the same order and position as the respective input search criteria.\n\nFor each batchFinder search criteria, it will either return a successful CollectionResponse which contains a list of entities Or \nan ErrorResponse in failing case. Such 2 kinds cases are wrapped into BatchFinderCriteriaResult corresponding to \neach search criteria.\n\nBatchFinderCriteriaResult fields:\n\n\n  (optional) “elements” : JSON serialized list of entity types (in success case)\n  (optional) “metadata”:\n  (optional) “paging” : JSON serialized CollectionMetadata object\n  (optional) “error” : it’s an ErrorResponse which fail to get a list of entities to corresponding search criteria(in failure)\n  “isError” : which indicates whether the result is a successful case or not\n\n\nE.g.\n\nGET http://localhost:7279/photos?bq=searchPhotos&amp;criteria=List((format:JPG,title:bar),(format:PNG,title:bar))&amp;exif=() HTTP/1.1\nHTTP/1.1 200 OK Content-Type: application/jsonX-RestLi-Protocol-Version: 2.0.0\n\n{\n  \"elements\" : [ {\n    \"elements\" : [ { // in success case: return a list of entities\n      \"urn\" : \"foo\",\n      \"format\" : \"JPG\",\n      \"id\" : 9,\n      \"title\" : \"baz\",\n      \"exif\" : { }\n    }, {\n      \"urn\" : \"foo\",\n      \"format\" : \"JPG\",\n      \"id\" : 10,\n      \"title\" : \"bar\",\n      \"exif\" : { }\n    } ],\n    \"paging\" : {\n      \"total\" : 2,\n      \"count\" : 10,\n      \"start\" : 0,\n      \"links\" : [ \n      {           \n        \"href\": \"/PhotoResource?PhotoCriteria=List((urn:foo, format:JPG))&amp;start=1&amp;count=1&amp;bq=searchPhotos\",\n        \"type\": \"application/json\",\n        \"rel\": \"next\"\n      ]\n    }\n  }, { // in failure : return an ErrorResponse\n    \"isError\" : true,\n    \"elements\" : [ ],\n    \"error\" : {\n      \"exceptionClass\" : \"com.linkedin.restli.server.RestLiServiceException\",\n      \"stackTrace\" : \"com.linkedin.restli.server.RestLiServiceException [HTTP Status:404]: The server didn't find a representation for this criteria\\n\\tat......\",\n      \"message\" : \"The server didn't find a representation for this criteria\",\n      \"status\" : 404\n    }\n  } ]\n}\n\n\nComplex Types\n\nComplex types as keys in protocol 1.0\n\nThe serialized form of a complex key uses path keys to specify the\nvalues of data elements in a complex data type. For example, given the\ncomplex data:\n\n{\n  \"key\": {\n    \"x\": [\n      \"a1\",\n      \"a2\"\n    ],\n    \"y\": 123,\n    \"key.with.dots\": \"val\"\n  }\n}\n\n\nIts serialized form is:\n\n// Complex key as a serialized string:\nkey.x[0]=a1&amp;key.x[1]=a2&amp;key.y=123&amp;key~2Ewith~2Edots=val\n\n\nIf this serialized form is put into a URI (as it usually is), the ‘[’\nmust be escaped as ‘%5B’ and the ‘]’ must be escaped as ‘%5D’ (URIs\nrequire this), so you have the URI form:\n\n// Complex key as a serialized string, escaped for URI:\nkey.x%5B0%5D=a1&amp;key.x%5B1%5D=a2&amp;key.y=123&amp;key~2Ewith~2Edots=val\n\n\nWhere, in the values of the query params, the chars ‘.[]’ are “~\nencoded” to their ascii values. This encoding is the same as “%\nencoding” except that the escape char is ‘~’ and the only reserved\nchars are ‘.[]’.\n\n. -&gt; ~2E\n[ -&gt; ~5B\n] -&gt; ~5D\n~ -&gt; ~7E\n\n\nThe Params of a ComplexResourceKey are always prefixed with\n“$params.” when represented in a URI, e.g.:\n\n$params.x=a1\n\n\nComplex types as keys in protocol 2.0\n\nThe serialized form of a complex key uses the Rest.li 2.0 protocol\nobject\nnotation\n. For example, given the complex data:\n\n{\n  \"key\": {\n    \"x\": [\n      \"a1\",\n      \"a2\"\n    ],\n    \"y\": 123,\n    \"key.with.dots\": \"val\"\n  }\n}\n\n\nIts serialized form is:\n\n// Complex key as a serialized string:\n(key:(x:List(a1,a2)),y:123,key.with.dots:val)\n\n\nThe Params of a ComplexResourceKey are always prefixed with\n“$params.” when represented in a URI, e.g.:\n\n$params:(x:a1)\n\n\nComplex keys in batch requests in protocol 1.0\n\nIf used in batch requests, each key in the batch is represented as a\nelement in an array, the complex data representation is:\n\n[\n  { &lt;complekey1&gt; }, { &lt;complexkey2&gt; }\n]\n\n\nAnd it’s serialized representation is just the list flattened using the\nsame rules as with any complex key, and with the same “~ encoding”\napplied.\n\nFor example,\n\n[\n  { \"keypart1\":\"v1\", \"keypart2\":\"v2\" }, { \"keypart1\":\"v3\", \"keypart2\":\"v4\" }\n]\n\n\nIt’s serialized form is:\n\n// Complex key as a serialized string:\nids[0].keypart1=v1&amp;ids[0].keypart2=v2&amp;ids[1].keypart1=v3&amp;ids[1].keypart2=v4\n\n\nIf this serialized form is put into a URI (as it usually is), the ‘[’\nmust be escaped as ‘%5B’ and the ‘]’ must be escaped as ‘%5D’ (URIs\nrequire this), so you have the URI form:\n\n// Complex key as a serialized string, escaped for URI:\nids%5B0%5D.keypart1=v1&amp;ids%5B0%5D.keypart2=v2&amp;ids%5B1%5D.keypart1=v3&amp;ids%5B1%5D.keypart2=v4\n\n\nIf $params are in a batch complex key key, they are also prefixed by\ntheir key’s position in the ids array, e.g.\n“ids[0].$params.parmkeypart1=v5”\n\nWhen complex keys are used in batch requests, they are often included\nboth in the URI and in the json body.\n\nFor example, a batch update request has the ids in the URI as well as\nthe “entities” part of the body:\n\nPUT /widgets?ids%5B0%5D.keypart1=v1&amp;ids%5B0%5D.keypart2=v2&amp;ids%5B1%5D.keypart1=v3&amp;ids%5B1%5D.keypart2=v4\n\n{\n  \"entities\": {\n    \"keypart1=v1&amp;keypart2=v2\": { &lt;content to put for v1,v2&gt; }\n    \"keypart1=v3&amp;keypart2=v4\": { &lt;content to put for v3,v4&gt; }\n  }\n}\n\n\nNote how the paths for the keys in the URI are prefixed by an ids array\nposition, but the paths for the keys in the JSON body are not.\n\nComplex keys in batch requests in protocol 2.0\n\nIf used in batch requests, each key in the batch is represented as an\nelement in the ids array using the protocol 2.0 array\nnotation\n.\n\nFor example,\n\n[\n  { \"keypart1\":\"v1\", \"keypart2\":\"v2\" }, { \"keypart1\":\"v3\", \"keypart2\":\"v4\" }\n]\n\n\nIt’s serialized form is:\n\n// Complex key as a serialized string:\nids=List((keypart1:v1,keypart2:v2),(keypart1:v3,keypart2:v4))\n\n\nIf $params are in a batch complex key key, they included in\nthe same object as their id portion. e.g.\n\nids=List(($params:(parmkeypart1:v5),keypart1:v1,keypart2:v2),($params:(parmkeypart1:v55),keypart1:v11,keypart2:v22))\n\n\nWhen complex keys are used in batch requests, they are often included\nboth in the URI and in the JSON body.\n\nFor example, a batch update request has the ids in the URI as well as\nthe “entities” part of the body:\n\nPUT /widgets?ids=List((keypart1:v1,keypart2:v2),(keypart1:v3,keypart2:v4))\n\n{\n  \"entities\": {\n    \"(keypart1:v1,keypart2:v2)\": { &lt;content to put for v1,v2&gt; }\n    \"(keypart1:v3,keypart2:v4)\": { &lt;content to put for v3,v4&gt; }\n  }\n}\n\n\nAs long as the protocol 2.0 array\nnotation\nis being adhered to no addional escaping is required for complex keys in\na batch request.\n\nQuery parameters as complex types in protocol 1.0\n\nComplex types work the same for query params as they do for keys. See\nthe above sections for details. E.g. for a complex type:\n\n{\n  \"a\": 1,\n  \"b\": 2\n}\n\n\nIn protocol 1.0 a query param named “param1” of this type would be:\n\n...?param1.a=1&amp;param1.b=2\n\n\nIn protocol 2.0 a query param named “param1” of this type would be:\n\n...?param1=(a:1,b:2)\n\n\nEmpty List Parameters\n\nSuppose you have a finder called search with a parameter\ncalled filters which is a list of some type.\n\nIn protocol 1, if a client attempted to send an empty list the URL would\nlook as follows -\n\n/resource?q=search\n\n\nNote that the parameter is lost in this case, and on the server side it\nwould be treated as a null or uninitialized field. This was a bug in\nprotocol 1!\n\nWe have fixed this in protocol 2. Here is how the URL looks in protocol\n2 when you send in an empty list -\n\n/resource?q=search&amp;filters=List()\n\n\nIn other words, List() denotes an empty list in protocol 2.\nThis applies to all cases in protocol 2 where lists are being used.\n\nEmpty map parameters\n\nSuppose you have a finder called search with a parameter\ncalled preferences which is a map of some type.\n\nIn protocol 1, if a client attempted to send an empty map the URL would\nlook as follows -\n\n/resource?q=search\n\n\nNote that the parameter is lost in this case, and on the server side it\nwould be treated as a null or uninitialized field. This was a bug in\nprotocol 1!\n\nWe have fixed this in protocol 2. Here is how the URL looks in protocol\n2 when you send in an empty map -\n\n/resource?q=search&amp;preferences=()\n\n\nIn other words, () denotes an empty map in protocol 2. This\napplies to all cases in protocol 2 where maps are being used.\n\nEmpty string parameters\n\nEmpty strings are a special case in Rest.li Protocol 2.0. They must be\nrepresented with two single quotes ’’. This applies if the\nempty string is a value or a map key. In Rest.li 1.0, empty strings\nwould simply appear as\nnothing.\n\n\n  \n    \n      Request Details\n      Protocol 1.0 URL\n      Protocol 2.0 URL\n    \n  \n  \n    \n      FINDER with string param; client attempts to send empty string\n      /resource?q=finderWithString\\&amp;myStringParam=\n      /resource?q=finderWithString\\&amp;myStringParam=’’\n    \n    \n      FINDER with list param; client attempts to send list containing a single empty string\n      /resource?q=finderWithList\\&amp;myListParam[ 0 ]=\n      /resource?q=finderWithList\\&amp;myListParam=List(’’)\n    \n  \n\n",
      tags: null,
      id: 48
    });
    
    

  
    index.add({
      title: "Return entity in Rest.li",
      category: null,
      content: "Return Entity in Rest.li\n\nContents\n\n\n  Supported Methods\n  How to Enable\n  Client-Specified Behavior\n    \n      Query Parameter\n      Access from Resource Method\n      Optimizations\n    \n  \n\n\nThis page describes returning the entity for resource methods that are not originally intended to return the entity.\nFor example, returning the entity is normal behavior for GET and FINDER, so this page does not apply to them.\n\nFor methods such as CREATE, however, the created entity is not returned in the response because the client\nalready has the entity when sending the request. Despite this, there are use cases where the server will\nattach additional data to the new entity. Returning the entity in the CREATE response saves the client\nfrom having to make an extra GET request.\n\nSupported Methods\n\nCurrently, this extra functionality is supported for the following resource methods:\n\n\n  CREATE\n  PARTIAL_UPDATE\n  BATCH_CREATE\n  BATCH_PARTIAL_UPDATE\n\n\nHow to Enable\n\nTo enable returning the entity for a given resource method, there are two requirements\nthat must be fulfilled.\n\nFirst, the resource method must be annotated with the @ReturnEntity annotation.\nThis applies to all resource methods.\n\nSecond, the return type of the method must be a valid “Return Entity” return type.\nThis is specific to each resource method. The following table lists which “Return Entity”\nreturn type corresponds to which resource method:\n\n\n  \n    \n      Resource Method\n      Standard Return Type\n      “Return Entity” Return Type\n      More Info\n    \n  \n  \n    \n      CREATE\n      CreateResponse\n      CreateKVResponse\n      Link\n    \n    \n      PARTIAL_UPDATE\n      UpdateResponse\n      UpdateEntityResponse\n      Link\n    \n    \n      BATCH_CREATE\n      BatchCreateResult\n      BatchCreateKVResult\n      Link\n    \n    \n      BATCH_PARTIAL_UPDATE\n      BatchUpdateResult\n      BatchUpdateEntityResult\n      Link\n    \n  \n\n\nIf both of these requirements are fulfilled, then the entity will be returned in the response by default.\n\nFor a resource method that has this behavior enabled, the application developer must make sure to populate\nthe returned object with the entity that’s being returned. Returning an object without a non-null entity\nwhen one is expected will cause a runtime exception.\n\nExamples\n\nHere is an example method signature for a CREATE resource method that will enable the entity to be returned.\nNote how both the annotation and the required return type are present:\n\n@ReturnEntity\npublic CreateKVResponse create(V entity);\n\n\nHere is an example implementation for the above method signature. Note how the returned entity is included\nin the constructor of the returned CreateKVResponse object:\n\n@ReturnEntity\npublic CreateKVResponse&lt;Long, Greeting&gt; create(Greeting entity)\n{\n    Long id = 1L;\n    entity.setId(id);\n    return new CreateKVResponse&lt;Long, Greeting&gt;(entity.getId(), entity);\n}\n\n\nFor more information on how to implement a “Return Entity” method for each resource method, see the “More Info”\nlinks in the above table.\n\nClient-Specified Behavior\n\nBy default, all requests to a “Return Entity” resource method will return the entity in the response.\nHowever, if the client decides that it doesn’t want the entity to be returned (to reduce network traffic, for instance),\nthen the query parameter $returnEntity can be used to indicate this.\n\nQuery Parameter\n\nThe value of this query parameter must be a boolean value, otherwise the server will treat it\nas a bad request. A value of true indicates that the entity should be returned, a value of\nfalse indicates that the entity shouldn’t be returned, and omitting the query parameter\naltogether defaults to treating the value as if it were true. Note that if the resource\nmethod doesn’t have a “Return Entity” return type, then the $returnEntity parameter will\nbe ignored, regardless of its value.\n\nExamples\n\nHere is an example of a PARTIAL_UPDATE curl request indicating that the entity shouldn’t be returned in the response:\n\n\ncurl -X POST localhost:/fortunes/1?$returnEntity=false -d '{\"patch\": {\"$set\": {\"fortune\": \"you will strike it rich!\"}}}'\n\n\nHere is an example in Java of how one would use this parameter when building a CREATE request,\nmaking use of the request builder’s returnEntity(boolean value) method:\n\nCreateIdEntityRequest&lt;Long, Greeting&gt; request = builders.createAndGet()\n    .input(greeting)\n    .returnEntity(false)\n    .build();\n\n\nSee more about request builders.\n\nAccess from Resource Method\n\nAn application developer can access this query parameter in order to define their own\nconditional behavior based on whether the entity should be returned. For ease of use,\nthe ResourceContext class provides a helper method that determines whether the entity is\nto be returned or not. From within the resource method, the ResourceContext#shouldReturnEntity\nmethod on the current resource context can be used to determine this. The method returns a boolean\nvalue consistent with the logic specified in the above “Query Parameter” section of this documentation.\n\nExamples\n\nHere is an example implementation of a CREATE method that makes use of this helper method\nto form conditional logic:\n\n@ReturnEntity\npublic CreateKVResponse&lt;Long, Greeting&gt; create(Greeting entity)\n{\n    \n    final ResourceContext resourceContext = getContext();\n    if (resourceContext.isReturnEntityRequested())\n    {\n        // make upstream call..\n        Long id = _upstream.getId(entity);\n        entity.setId(id);\n        return new CreateKVResponse&lt;Long, Greeting&gt;(entity.getId(), entity);\n    }\n    else\n    {\n        Long id = 1L;\n        entity.setId(id);\n        return new CreateKVResponse&lt;Long, Greeting&gt;(entity.getId(), null);\n    }\n}\n\n\nOptimizations\n\nThis feature can be harnessed by an application developer to optimize their service.\nThe obvious optimization is that potentially large payloads don’t have to be\ntransmitted over the wire, reducing latency and network traffic. Another possible\noptimization comes from the fact that the application developer can access this\nquery parameter from the resource method, allowing them to conditionally avoid\nupstream service calls that would cause unnecessary slowdown.\n\n\n",
      tags: null,
      id: 49
    });
    
    

  
    index.add({
      title: "Service Errors",
      category: null,
      content: "Service Errors\n\nThis page describes how service errors are returned by Rest.li and how they are documented in a resource’s IDL.\n\nSee Configuring Service Errors in Java for a step-by-step guide showing how\nto configure service errors in Rest.li Java.\n\nContents\n\n\n  Error Responses\n    \n      Fields\n      Example\n      Returning a Subset of Fields\n    \n  \n  Documenting Service Errors in the IDL\n    \n      Resource-Level Errors\n      Method-Level Errors\n      Success Codes\n      Backward Compatibility\n    \n  \n\n\nError Responses\n\nAll error responses are returned by Rest.li in a format conforming to the\nErrorResponse schema,\nwhich contains various fields describing the service failure.\n\nFields\n\n\n  \n    \n      Field\n      Description\n    \n  \n  \n    \n      status\n      The HTTP status code.\n    \n    \n      code\n      The canonical error code, e.g. for ‘400 Bad Request’ it can be ‘INPUT_VALIDATION_FAILED’. Only predefined codes should be used.\n    \n    \n      message\n      A human-readable explanation of the error.\n    \n    \n      docUrl\n      URL to a page that describes this particular error in more detail.\n    \n    \n      requestId\n      The unique identifier that would identify this error. For example, it can be used to identify requests in the service’s logs.\n    \n    \n      exceptionClass\n      The FQCN of the exception thrown by the server.\n    \n    \n      stackTrace\n      The full stack trace of the exception thrown by the server.\n    \n    \n      errorDetailType\n      The type of the error detail model, e.g. com.example.api.BadRequest. Clients can use this field to identify the actual error detail schema.\n    \n    \n      errorDetails\n      This field should be used for communicating extra error details to clients.\n    \n  \n\n\nExample\n\nHere is an example error response serialized to JSON as it would be sent over the wire and received by the client. This\nscenario involves a client which has used up its daily quota for a particular service, triggering the resource method to\nthrow a fictitious QuotaExceededException. The error details object conforms to the fictitious QuotaDetails schema,\nwhich contains relevant information about the quota usage.\n\nPlease note that the stack trace has been truncated for the sake of space.\n\n{\n    \"status\": 429,\n    \"code\": \"QUOTA_EXCEEDED\",\n    \"message\": \"You've exceeded your daily request quota.\",\n    \"docUrl\": \"https://example.com/docs/errors/QUOTA_EXCEEDED\",\n    \"requestId\": \"cgA4qNoE48AJabrC\",\n    \"exceptionClass\": \"com.example.QuotaExceededException\",\n    \"stackTrace\": \"Exception in thread \\\"main\\\" com.example.QuotaExceededException: ...\",\n    \"errorDetailType\": \"com.example.api.QuotaDetails\",\n    \"errorDetails\": {\n        \"interval\": \"DAILY\",\n        \"quota\": 10000,\n        \"usage\": 10034\n    }\n}\n\n\nReturning a Subset of Fields\n\nBy default, Rest.li returns the error response containing all the fields described above, in addition to\nan X-RestLi-Error-Response HTTP header. However, Rest.li supports returning only a subset of these fields.\n\nJava\n\nIn Java, the RestLiConfig class is used to configure which ErrorResponse fields should be included\nin the response.\n\nUsing a predefined error response format to only return the status, code, and message:\n\nrestLiConfig.setErrorResponseFormat(ErrorResponseFormat.MESSAGE_AND_SERVICECODE);\n\n\n{\n    \"status\": 429,\n    \"code\": \"QUOTA_EXCEEDED\",\n    \"message\": \"You've exceeded your daily request quota.\"\n}\n\n\nCreating a custom subset of fields to return:\n\nEnumSet&lt;ErrorResponseFormat.ErrorResponsePart&gt; parts = EnumSet.of(HEADERS,\n                                                                  STATUS_CODE_IN_BODY,\n                                                                  DOC_URL,\n                                                                  STACKTRACE);\nrestLiConfig.setErrorResponseFormat(new ErrorResponseFormat(parts));\n\n\n{\n    \"status\": 429,\n    \"docUrl\": \"https://example.com/docs/errors/QUOTA_EXCEEDED\",\n    \"stackTrace\": \"Exception in thread \\\"main\\\" com.example.QuotaExceededException: ...\"\n}\n\n\nDocumenting Service Errors in the IDL\n\nSemantically, an error response is still a valid API response – it’s one of the possible outputs of an API call - so it\nmust be part of the formal contract. Having well-documented service errors as part of an API enables clients to handle\nfailures resiliently, and allows developers to intelligently prepare their clients for failures.\n\nFor a full, in-depth example of what an IDL looks like documented with service errors and success codes, see\nAlbumEntryResource\nand its generated IDL.\n\nResource-Level Errors\n\nIf a resource has been given a set of service errors (done in Java using @ServiceErrors, see\nhere), those service errors will be documented in the IDL as a list of\nobjects, each containing the HTTP status code, the application-specific string error code, the message, and the\nerror detail type (for each that is present). For example, a resource someResource that may fail due to some\nrate-limiting restrictions will be documented in the IDL as:\n\n{\n  \"name\" : \"someResource\",\n  ...\n  \"collection\" : {\n    \"serviceErrors\" : [ {\n      \"status\" : 429,\n      \"code\" : \"QUOTA_EXCEEDED\",\n      \"message\" : \"You've exceeded your daily request quota.\",\n      \"errorDetailType\" : \"com.example.api.QuotaDetails\"\n    } ],\n    ...\n  }\n  ...\n}\n\n\nMethod-Level Errors\n\nIf a resource has been given a set of service errors (done in Java using @ServiceErrors or @ParamErrors, see\nhere), those service errors will be documented in the IDL as a list of\nobjects similar to what is shown above. The difference here is that these will be documented per-method, and also\nsupport indicating specific parameter names. For example, a finder method that may fail on some parameter albumId\nwill documented in the IDL as:\n\n{\n  \"serviceErrors\" : [ {\n    \"status\" : 422,\n    \"code\" : \"INVALID_ID\",\n    \"message\" : \"Id cannot be less than 0\",\n    \"errorDetailType\" : \"com.example.api.IdError\",\n    \"parameters\" : [ \"albumId\" ]\n  } ],\n  \"name\" : \"search\",\n  \"parameters\" : [ {\n    \"name\" : \"albumId\",\n    \"type\" : \"long\",\n  }\n}\n\n\nSuccess Codes\n\nIf a resource method has been given a set of success codes (done in Java using @SuccessResponse), those success codes\nwill be documented in the IDL as a list of integers. For example, a get method that returns an HTTP 200 response on\nsuccess will be documented in the IDL as:\n\n{\n  \"success\" : [ 200 ],\n  \"method\" : \"get\",\n  \"doc\" : \"Gets a thing.\"\n}\n\n\nBackward Compatibility\n\nMaking changes to the service error information in a resource’s IDL has an impact on the\ncompatibility checker.\n\nThe following changes are considered backward compatible:\n\n\n  Removing a service error code from a resource or a method.\n  Removing a service error code from a resource then adding it to a subset of its methods.\n\n\nThe following changes are considered backward incompatible:\n\n\n  Adding a new service error code to a resource or a method.\n    \n      Note: Changing an existing code is treated semantically as removing one and adding another.\n    \n  \n  Changing the errorDetailType for an existing service error code.\n  Changing the HTTP status code for an existing service error code.\n  Changing the message for an existing service error code.\n\n",
      tags: null,
      id: 50
    });
    
    

  
    index.add({
      title: "Rest.li restspec (IDL) format",
      category: null,
      content: "Rest.li restspec format\n\nRest.li uses a RESTSpec JSON schema as its interface definition language\n(IDL).\n\nEach RESTSpec file contains the full definition of a root level resource\nand all its sub-resources.\n\nRESTSpec files use the .restspec.json file extension. The contents of\neach file is JSON describing the resource according to the\nResourceSchema\ndata schema.\n\nFor example:\n\nThe RESTSpec for the\nGreetingsResource\nimplementation is\ncom.linkedin.restli.examples.greetings.client.greetings.restspec.json\n",
      tags: null,
      id: 51
    });
    
    

  

  

  
    index.add({
      title: "Quickstart - A Tutorial Introduction to Rest.li",
      category: null,
      content: "# Tutorial to Create a Rest.li Server and Client\n\n## Contents\n\n  - [Introduction](#introduction)\n  - [Example Source Code - Top Level\n    Structure](#example-source-code--top-level-structure)\n  - [Creating a Server](#creating-a-server)\n      - [Step 1. Define Data Schema](#step-1-define-data-schema)\n      - [Step 2. Generate Java Bindings](#step-2-generate-java-bindings)\n      - [Step 3. Implement Rest.li Server\n        Resource](#step-3-implement-restli-server-resource)\n      - [Step 4. Build and Run the\n        Server](#step-4-build-and-run-the-server)\n  - [Publishing Server’s Interface\n    Definition](#publishing-servers-interface-definition)\n  - [Creating a Client](#creating-a-client)\n      - [Step 1. Generate Client Request\n        Builders](#step-1-generate-client-request-builders)\n      - [Step 2. Implement Client Class](#step-2-implement-client-class)\n      - [Step 3. Build and Run the\n        Client](#step-3-build-and-run-the-client)\n  - [Recap](#recap)\n\n## Introduction\n\nIn this tutorial, we’ll take a first look at Rest.li and learn about some of its most basic features. We’ll construct a server that responds with *Fortunes* for GET requests and also creates a client that sends a request to the server and prints a fortune returned by the server.\n\nRest.li uses an inversion of control model in which Rest.li defines the\nclient and server architecture and handles many details of constructing,\nreceiving, and processing RESTful requests. On the server side, Rest.li\ncalls your code at the appropriate time to respond to requests. You only\nneed to worry about your application-specific response to requests. On\nthe client side, Rest.li helps send type-safe requests to the server and\nreceives type-safe responses.\n\nTo allow Rest.li to perform its tasks, you need to conform to a simple\narchitecture, in which you define a schema for your data, and classes\nthat support REST operations on that data. Your classes will designate\nhandlers for REST operations using Annotations and return objects that\nrepresent your data schema. Rest.li will handle mostly everything else.\n\nWe’ll see how Rest.li helps you perform these actions using automatic\ncode generation, supporting base classes and other infrastructure.\n\n**Note**: You will notice references to ‘Pegasus’ in various places as\nyou work through this tutorial and read other Rest.li documents. Pegasus\nis the code name for the project that includes Rest.li and some related\nmodules. It is also used in some package names.\n\n## Example Source Code - Top Level Structure\n\nIf you like to do things yourself, you should be able to enter the code\nin this tutorial into whatever editor you like and construct each step\nof the process. You can also follow along using the ready-made source in\nthe repository under\n[examples/quickstart](https://github.com/linkedin/rest.li/tree/master/examples/quickstart)\ndirectory. Using the provided source tree frees you from worrying about\nthe build scripts and directory structure until you want to use Rest.li\nin your own projects.\n\nThe example can be built using Gradle. Many of the steps involve code\ngeneration that is automated by Gradle plugins provided as part of\nRest.li. We’ll show you the basic build scripts you need for this\nexample as we go along. For more details about the build process see\n[Gradle Build Integration](/rest.li/setup/gradle). You will need\nGradle 1.6+ (run `gradle --version` to check). If you have a different\nGradle version and do not want to install the version required by this\nexample globally, we recommend quickly setting up a [Gradle\nwrapper](http://www.gradle.org/docs/current/userguide/gradle_wrapper.html)\nfor this project).\n\nBefore we get started, you’ll need to create a basic directory structure\nto hold your classes. At the root of the example source tree, you should\nhave three sub-directories, `api/`, `client/` and `server/`.\n\nYou will also need `build.gradle` and `settings.gradle` files at the top\nlevel.\n\nThe `settings.gradle` file just includes the sub-projects:\n\n##### file: example-standalone-app/settings.gradle\n\n```gradle\ninclude 'api'\ninclude 'server'\ninclude 'client'\n```\n\nThe file `build.gradle` should contain:\n\n##### file: example-standalone-app/build.gradle\n\n```gradle\nbuildscript {\n  repositories {\n    mavenLocal()\n    mavenCentral()\n    maven {\n      url \"https://linkedin.jfrog.io/artifactory/open-source\"\n    }\n  }\n  dependencies {\n    classpath 'com.linkedin.pegasus:gradle-plugins:29.19.2'\n  }\n}\n\ntask wrapper(type: Wrapper) {\n  gradleVersion = '4.6'\n}\n\nfinal pegasusVersion = '29.19.2'\next.spec = [\n  'product' : [\n    'pegasus' : [\n      'data' : 'com.linkedin.pegasus:data:' + pegasusVersion,\n      'generator' : 'com.linkedin.pegasus:generator:' + pegasusVersion,\n      'r2Netty' : 'com.linkedin.pegasus:r2-netty:' + pegasusVersion,\n      'restliCommon' : 'com.linkedin.pegasus:restli-common:' + pegasusVersion,\n      'restliClient' : 'com.linkedin.pegasus:restli-client:' + pegasusVersion,\n      'restliServer' : 'com.linkedin.pegasus:restli-server:' + pegasusVersion,\n      'restliTools' : 'com.linkedin.pegasus:restli-tools:' + pegasusVersion,\n      'gradlePlugins' : 'com.linkedin.pegasus:gradle-plugins:' + pegasusVersion,\n      'restliNettyStandalone' : 'com.linkedin.pegasus:restli-netty-standalone:' + pegasusVersion,\n      'restliServerStandalone' : 'com.linkedin.pegasus:restli-server-standalone:' + pegasusVersion\n    ]\n  ]\n]\n\nallprojects {\n  apply plugin: 'idea'\n  apply plugin: 'eclipse'\n}\n\next.enablePDL=true\n\nsubprojects {\n  apply plugin: 'maven'\n\n  afterEvaluate {\n    // add the standard pegasus dependencies wherever the plugin is used\n    if (project.plugins.hasPlugin('pegasus')) {\n      dependencies {\n        dataTemplateCompile spec.product.pegasus.data\n        restClientCompile spec.product.pegasus.restliClient\n      }\n    }\n  }\n\n  repositories {\n    mavenLocal()\n    mavenCentral()\n    maven {\n      url \"https://linkedin.jfrog.io/artifactory/open-source\"\n    }\n  }\n}\n```\n\nThis gradle build file pulls all required jars from a global Maven\nrepository. It also loads some plugins that facilitate the build process\nand various code generation steps. Notice that plugins are also provided\nfor IntelliJ Idea and Eclipse. For example, executing:\n\n    $ gradle idea\n\nwill generate an Idea project ready to open in Idea. Using Idea or\nEclipse is a handy way to explore and follow along as you read this\ntutorial.\n\nHere’s how the structure of your top-level project should look as we\nbegin:\n\n    example-standalone-app/\n    +- build.gradle\n    +- settings.gradle\n    +- api/\n    +- client/\n    +- server/\n    \n\n## Creating a Server\n\nThe first thing we will do is implement a very simple server that\nresponds to GET requests.\n\nThe basic steps you will follow to create a Rest.li server are:\n\n1.  Define data schema. Rest.li uses [Pegasus Data Schema](/rest.li/pdl_schema) to\n    define the resource data.\n\n2. Generate language bindings. Rest.li will generate java class\nbindings for these data schemas to be used in your server and clients.\n\n3. Implement resource classes containing methods to act on your data.\nRest.li provides a set of base classes and annotations that will map\nthese methods to URIs and REST operations.\n\n4. Create an HTTP server that instantiates a Rest.li server. The\nRest.li server will automatically locate your resource classes and\ninvoke the appropriate methods when a request is received.\n\nRest.li provides tools to make these steps simple, including code\ngenerators that create classes from the data schema, base classes, and\nannotations that map entry points in your code to REST operations.\n\nLet’s walk through each step of the process.\n\n### Step 1. Define Data Schema\n\nThe first step in creating a Rest.li service is to define a data model\nor schema for the data that will be returned by your server. We will\ndefine the data model in the `api/` directory, which serves to define\nthe API or interface between the server and clients.\n\nAll Rest.li data models are defined in Pegasus Data Schema files, which\nhave a `.pdl` suffix. We’ll define a `Fortune` data model in\n`Fortune.pdl`. The location of this file is important. Be sure to place\nit in a path corresponding to your namespace, under\n`api/src/main/pegasus/`:\n\n##### file: example-standalone-app/api/src/main/pegasus/com/example/fortune/Fortune.pdl\n\n```pdl\nnamespace com.example.fortune\n\n/**\n * Generate a fortune cookie\n */\nrecord Fortune {\n\n  /**\n   * The Fortune cookie string\n   */\n  fortune: string\n}\n``` \n\n`Fortune.pdl` defines a record named Fortune, with an associated\nnamespace. The record has one field, a string whose name is `fortune`.\nFields as well as the record itself can have optional documentation\nstrings. This is, of course, a very simple schema. See\n[Data Schemas](/rest.li/pdl_schema) for\ndetails on the syntax and more complex examples.\n\n### Step 2. Generate Java Bindings\n\nRest.li uses the data model in `.pdl` files to generate java versions\nof the model that can be used by the server. The easiest way to generate\nthese classes is to use the Gradle integration provided as part of\nRest.li. You will need a `build.gradle` file in the `api/` directory\nthat looks like this:\n\n##### file: example-standalone-app/api/build.gradle\n\n```gradle\napply plugin: 'pegasus'\n```\n\nWith `Fortune.pdl` and `build.gradle` files in place, you can generate\na Java binding for the data model. This Java version is what will\nactually be used by your server to return data to calling clients.\nChange into the `api/` directory and run the following command:\n\n    $ gradle build\n\nThe `pegasus` Gradle plugin will detect the presence of `Fortune.pdl`\nand use the *dataTemplateGenerator* to generate `Fortune.java`. The\ngenerated java classes will be placed under\n`api/src/mainGeneratedDataTemplate/` directory.\n\nYour file system structure should now look like this:\n\n    example-standalone-app/\n    +- build.gradle\n    +- settings.gradle\n    +- api/\n    |  +- build.gradle\n    |  +- src/\n    |     +- main/\n    |     |  +- pegasus/\n    |     |     +- com/\n    |     |        +- example/\n    |     |           +- fortune/\n    |     |              +- Fortune.pdl\n    |     +- mainGeneratedDataTemplate/\n    |        +- java/\n    |           +- com/\n    |              +- example/\n    |                 +- fortune/\n    |                    +- Fortune.java\n    +- client/\n    +- server/\n    \n\nThe generated java file contains a java representation of the data model\ndefined in the schema, and includes `get` and `set` methods for each\nelement of the model, as well as other supporting methods. You can look\nat the generated file to see the full implementation if you are curious;\nthe following excerpt should give you the general idea. This class is\nentirely derived from your data model and should not be\nmodified.\n\n##### file: example-standalone-app/api/src/mainGeneratedDataTemplate/java/com/example/fortune/Fortune.java\n\n```java\n@Generated(...)\npublic class Fortune extends RecordTemplate {\n  public String getFortune() {\n    return getFortune(GetMode.STRICT);\n  }\n\n  public Fortune setFortune(String value) {\n    putDirect(FIELD_Fortune, String.class, String.class, value, SetMode.DISALLOW_NULL);\n    return this;\n  }\n\n  // ... other methods\n}\n```    \n\n### Step 3. Implement Rest.li Server Resource\n\nNow that we have defined our data model, the next step is to define a\n`resource` class that will be invoked by the Rest.li server in\nresponse to requests from clients. We’ll create a class named\n`FortunesResource`. This class is written by hand, and implements any\nREST operations you want to support, returning data using the java data\nmodel class generated in the previous step. The file should be placed\naccording to your package path under\n`server/src/main/java`.\n\n##### file: example-standalone-app/server/src/main/java/com/example/fortune/impl/FortunesResource.java\n\n```java\npackage com.example.fortune.impl;\n\nimport com.linkedin.restli.server.annotations.RestLiCollection;\nimport com.linkedin.restli.server.resources.CollectionResourceTemplate;\nimport com.example.fortune.Fortune;\nimport java.util.HashMap;\nimport java.util.Map;\n\n/**\n * Simple Rest.li Resource that serves up a fortune cookie.\n */\n@RestLiCollection(name = \"fortunes\", namespace = \"com.example.fortune\")\npublic class FortunesResource extends CollectionResourceTemplate {\n\n  // In-memory store for the fortunes\n  static Map fortunes = new HashMap();\n  static {\n    fortunes.put(1L, \"Today is your lucky day.\");\n    fortunes.put(2L, \"There's no time like the present.\");\n    fortunes.put(3L, \"Don't worry, be happy.\");\n  }\n\n  @Override\n  public Fortune get(Long key) {\n    // Retrieve the requested fortune\n    String fortune = fortunes.get(key);\n    if (fortune == null) {\n      fortune = \"Your luck has run out. No fortune for id = \" + key;\n    }\n\n    // return an object that represents the fortune cookie\n    return new Fortune().setFortune(fortune);\n  }\n}\n```    \n\nFortunesResource extends a Rest.li class, `CollectionResourceTemplate`\nand, for this simple example, overrides a single method, `get`, which\ntakes a single argument, an id of a resource to be returned. Rest.li\nwill call this method when it dispatches a GET request to the Fortune\nresource. Additional REST operations could be provided by overriding\nother methods. See the [Rest.li User\nGuide](/rest.li/user_guide/server_architecture) for more details about\nsupporting additional REST methods and other types of resources.\n\nNotice that if this GET were to perform any IO it would be `blocking`,\nmeaning that the thread handling this request will wait for that IO to\ncomplete. Later we will show how we can build async GET methods that\nreturn [ParSeq](https://github.com/linkedin/parseq) `Promise` and `Task` \nclasses so that we do not block while performing IO operations.\n\nThe `RestLiCollection` annotation at the top of the file marks this class as a REST collection, and declares that this resource handles the `/fortunes` URI. The result is that calling `http://localhost/fortunes/` (assuming your server is running on localhost) will call `FortunesResource.get()`, which should return a `Fortune` object corresponding to the given fortune identifier. For this simple implementation, we will create a static HashMap that maps several fortune strings to ids. If a requested id is found in the HashMap, we will construct a `Fortune` object, set the message and id, and return the object. If the requested id is not found, we’ll return a default message. Rest.li will handle delivering the result to the calling client as a JSON object. (Recall that Fortune.java was generated in a previous step and is found under the `api` directory.)\n\nIn a real implementation, you would perform whatever steps\nare required to retrieve or construct your response to the request. But\nultimately, you will return an instance of your data model class that\nrepresents the data defined in your schema.\n\n### Step 4. Build and Run the Server\n\nWe’ve now completed the bulk of our application specific server side\ncode. We’ve defined our data model, and implemented a Resource class\nthat can respond to a GET request by returning data according to the\nmodel. The only thing remaining is to configure a HTTP framework to call\nour application logic. We will use Netty, an excellent framework that\nworks great with Rest.li to build fully async services. For details on\nhow to configure Rest.li with other servlet containers see\n[Rest.li with Servlet Containers](/rest.li/Rest_li-with-Servlet-Containers).\n\nRest.li also includes a Request Response layer (R2) that provides a\ntransport abstraction and other services.\n\nNotice that Rest.li automatically scans all resource classes in the\nspecified package and initializes the REST endpoints/routes without any\nhard-coded connection. Adding additional resources or operations can be\ndone simply by expanding your data schema and providing additional\nfunctionality in your Resource class(es).\n\nTo compile and run the server, we need a `build.gradle` file in the\n`server/` directory, which should look like this:\n\n##### file: example-standalone-app/server/build.gradle\n\n```gradle\napply plugin: 'pegasus'\n\next.apiProject = project(':api')\n\ndependencies {\n  compile project(path: ':api', configuration: 'dataTemplate')\n  compile spec.product.pegasus.restliServer\n  compile spec.product.pegasus.restliNettyStandalone\n}\n\ntask startFortunesServer(type: JavaExec) {\n  main = 'com.linkedin.restli.server.NettyStandaloneLauncher'\n  args = ['-port', '8080', '-packages', 'com.example.fortune.impl']\n  classpath = sourceSets.main.runtimeClasspath\n  standardInput = System.in\n}\n```\n\nNext, create a `gradle.properties` file containing the following line:\n\n##### file: example-standalone-app/server/gradle.properties\n\n```gradle\nrest.model.compatibility=ignore\n```\n\nThis disables some [compatibility checks](/rest.li/setup/gradle#compatibility) on the generated files. You will need these checks in a real project but to keep this example simple we are disabling these checks.\n\nWith these files in place, your server directory structure should look\nlike this:\n\n    example-standalone-app/\n    +- build.gradle\n    +- settings.gradle\n    +- api/\n    |  ...\n    +- client/\n    +- server/\n       +- build.gradle\n       +- gradle.properties\n       +- src/\n          +- main/\n             +- java/\n                +- com/\n                   +- example/\n                      +- fortune/\n                         +- impl/\n                           +- FortuneResource.java\n    \n\nNow you can build the server from the `server/` directory with:\n    $ gradle build\n\n**Note:** If prompted, run the build command a second time. The first build\nruns a bootstrapping code generation process, requiring a second build\nto compile the generated code.\n\nAfter building the server, you can launch the server using the following\ncommand:\n\n    $ gradle startFortunesServer\n\nOnce the server is running, you can perform tests using `curl`:\n\n    \n    $ curl -v http://localhost:8080/fortunes/1\n    \n    * About to connect() to localhost port 8080 (#0)\n    *   Trying ::1... connected\n    * Connected to localhost (::1) port 8080 (#0)\n    > GET /fortunes/1 HTTP/1.1\n    > User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.12.9.0 zlib/1.2.3 libidn/1.18 libssh2/1.2.2\n    > Host: localhost:8080\n    > Accept: */*\n    > \n     getRequest = getBuilder.id(fortuneId).build();\n```\n\nThe process of sending a request from a client basically consists of\ncreating a `RestClient` object and invoking its `sendRequest()` method\nto send the request to the\n    server:\n\n```java\nRestClient restClient = new RestClient(r2Client, \"http://localhost:8080/\");\nResponseFuture getFuture = restClient.sendRequest(getRequest);\nResponse response = getFuture.getResponse();\n```\n\n`RestClient.sendRequest()` returns a `Future`, which can be used to wait\non and retrieve the response from the server. Note that the response is\ntype-safe, and parametrized as type Fortune, so we can use the `Fortune`\ninterface to retrieve the results, like\n    this:\n\n```java\nString message = response.getEntity().getFortune();\nlong id = response.getEntity().getId();\n```\n\nHere is a completed `RestLiFortunesClient` class, which uses the R2\nlibrary to create the transport mechanisms. For this example, the client\nwill just generate a random ID between 0 and 5, and print the response.\nThis file should go under `client/src/main/java/` directory with the\nappropriate java package\nstructure.\n\n##### file: example-standalone-app/client/src/main/java/com/example/fortune/RestLiFortunesClient.java\n\n```java\npackage com.example.fortune;\n\nimport com.linkedin.common.callback.FutureCallback;\nimport com.linkedin.common.util.None;\nimport com.linkedin.r2.transport.common.Client;\nimport com.linkedin.r2.transport.common.bridge.client.TransportClient;\nimport com.linkedin.r2.transport.common.bridge.client.TransportClientAdapter;\nimport com.linkedin.r2.transport.http.client.HttpClientFactory;\nimport com.linkedin.restli.client.Request;\nimport com.linkedin.restli.client.Response;\nimport com.linkedin.restli.client.ResponseFuture;\nimport com.linkedin.restli.client.RestClient;\nimport com.example.fortune.FortunesRequestBuilders;\nimport java.util.Collections;\n\npublic class RestLiFortunesClient {\n  /**\n   * This stand-alone app demos the client-side Rest.li API.\n   * To see the demo, run the server, then start the client\n   */\n  public static void main(String[] args) throws Exception {\n\n    // Create an HttpClient and wrap it in an abstraction layer\n    final HttpClientFactory http = new HttpClientFactory();\n    final Client r2Client = new TransportClientAdapter(\n        http.getClient(Collections.emptyMap()));\n\n    // Create a RestClient to talk to localhost:8080\n    RestClient restClient = new RestClient(r2Client, \"http://localhost:8080/\");\n\n    // Generate a random ID for a fortune cookie, in the range 0 - 5\n    long fortuneId = (long) (Math.random() * 5);\n\n    // Construct a request for the specified fortune\n    FortunesGetRequestBuilder getBuilder = fortuneBuilders.get();\n    Request getRequest = getBuilder.id(fortuneId).build();\n\n    // Send the request and wait for a response\n    final ResponseFuture getFuture = restClient.sendRequest(getRequest);\n    final Response response = getFuture.getResponse();\n\n    // Print the response\n    System.out.println(response.getEntity().getFortune());\n\n    // Shutdown\n    restClient.shutdown(new FutureCallback());\n    http.shutdown(new FutureCallback());\n  }\n\n  private static final FortunesRequestBuilders fortuneBuilders = new FortunesRequestBuilders();\n}\n```\n\n### Step 3. Build and Run the Client\n\nWith your client code in place, your directory structure should look\nlike this:\n\n    example-standalone-app/\n    +- build.gradle\n    +- settings.gradle\n    +- api/\n    |  ...\n    +- client/\n    |  +- build.gradle\n    |  +- src/\n    |     +- main/\n    |        +- java/\n    |           +- com/\n    |              +- example/\n    |                 +- fortune/\n    |                      +- RestLiFortunesClient.java\n    +- server/\n       ...\n    \n\nBuild the client by building in the client directory:\n\n    $ gradle build\n\nTo test our final client/server pair, start the server in one terminal\nwindow:\n\n    $ gradle startFortunesServer\n\nThen in another window, run:\n\n    $ gradle startFortunesClient\n\nYou should see a “fortune cookie” printed out from the client before it\nexits.\n\nIf you want to inspect the request being sent by the client, stop the\nserver, and run `netcat` or a similar packet sniffer tool to listen on\nport 8080, and then run the client:\n\n    $ netcat -l -p 8080\n    \n    GET /fortunes/1 HTTP/1.1\n    Host: localhost:8080\n    X-LI-R2-W-MsgType: REST\n    Content-Length: 0\n    \n\n## Recap\n\nWe’ve now completed a quick tour of a few of the most basic features of\nRest.li. Let’s review the steps we took to create a server and a\ncorresponding client:\n\n1.  Define a data model (`Fortune.pdl`)\n2.  Generate Java language bindings (`Fortune.java RecordTemplate class`)\n3.  Create a Resource that responds to REST requests\n    (`FortuneResource.java`) by subclassing `CollectionResourceTemplate`\n    and using `RestLiAnnotations` to define operations and entry points\n4.  Create a server that locates our Resource classes and uses Netty to\n    dispatch requests\n5.  Generate IDL (`fortune.restpec.json`) and Java client request builders\n    from the server Resource file (`FortunesRequestBuilders.java` and\n    `FortunesGetRequestBuilder.java`)\n6.  Create a client that uses the `RestClient` to send requests\n    constructed by calling the builder classes\n    (`RestLiFortuneClient.java`)\n\nNotice that (ignoring Gradle build files) there are only three files in\nthis example that you had to create:\n\n  - The original Pegasus Data Model file (`Fortune.pdl`)\n  - The server resource file (`FortunesResource.java`)\n  - The client (`RestLiFortuneClient.java`)\n\nAlthough Rest.li has many more features that can be leveraged when\ncreating the server and client, most of your focus will usually be on\ndefining data models and implementing resource classes that provide\nand/or manipulate the data.\n\nTo learn more about Rest.li, proceed to the more complex examples in the source code and read the [Rest.li User’s Guide](/rest.li/user_guide/server_architecture).\n",
      tags: null,
      id: 52
    });
    
    

  
    index.add({
      title: "Multi-language Compatibility Matrix",
      category: null,
      content: "# Rest.li Multi-language Compatibility Matrix\n\n## Contents\n\n  - [Supported Resources](#supported-resources)\n  - [Resource Keys and Parameters](#resource-keys-and-parameters)\n  - [Supported Data Templates in Test Suite Spec](#supported-data-templates-in-test-suite-spec)\n  - [Supported HTTP Headers ](#supported-http-headers )\n  - [Request Format Differences](#request-format-differences)\n\nFollowing [Rest.li Test Suite Specification](/rest.li/test_suite), we've added test suites for Python and Java.\nBased on these test suites, we've made the following compatibility matrices:\n\n## Supported Resources\n\n| Resource | Java | Python | Additional Information |\n|--------|------|----------------|----------------|\n| ActionSet | x | x | Python requires a top-level \"namespace\", which is optional according to Rest.li protocol |\n| Association | x | | Python generates code using a template engine, which doesn’t support Association |\n| Collection | x | x | Python requires a top-level \"namespace\", which is optional according to Rest.li protocol |\n| Simple | x | x | Python requires a top-level \"namespace\", which is optional according to Rest.li protocol |\n\n\n## Resource Keys and Parameters\n\n| Key Feature | Java | Python | Additional Information |\n|--------|-----|-----|----------------|\n| Primitive&nbsp;Key | x | x | long |\n| Complex&nbsp;Key&nbsp;(Simple Record and Union)| x |  |In Java, ComplexResourceKey is a map of complex keys and params. In Python, complex key is supported, but only as a record, not a map with params |\n| Query&nbsp;Parameters | x | x | Integer, string, long, string array, message array, string map, primitive union, complex types union, optional string, url typeref |\n\n\n## Supported Data Templates in Test Suite Spec\n\n| Template | Java | Python | Additional Information |\n|--------|------|------|----------------|\n| Array&nbsp;of&nbsp;Maps | x | x | |\n| Complex&nbsp;Types | x | x | |\n| Defaults | x | x | Test spec provides a schema with default values. In Python, we cannot specify a default value for Fixed field because Python does not support Fixed in the same way as Java. |\n| Enums | x | x | |\n| Enum&nbsp;with&nbsp;Properties | x | x | |\n| Fixed | x | | Python does not support Fixed in the same way as Java|\n| Fixed5 | x | | Python does not support Fixed in the same way as Java|\n| Includes | x | x | |\n| Large&nbsp;Record | x | x | |\n| Map&nbsp;of&nbsp;Ints| x | x | |\n| MD5| x | x | |\n| Message | x | x | | \n| Optionals| x | x | |\n| Primitives | x | x | |\n| Record&nbsp;with&nbsp;Properties | x | x | |\n| Record&nbsp;with&nbsp;Typeref&nbsp;Field | x | x | |\n| Time | x | x | |\n| Type&nbsp;Defined&nbsp;Before&nbsp;Include | x | x | |\n| Type&nbsp;Defined&nbsp;in&nbsp;Include | x | x | |\n| Typeref&nbsp;Message | x | x | |\n| Typerefs | x | x | Typerefs.pdsc may need a different name in Python to avoid a naming conflict. If the filename is not changed, Python testsuite will have a folder for typeref resources, such as collectionTyperef, and it will also generate typeref.py from Typerefs.pdsc. |\n| Union&nbsp;of&nbsp;Complex Types | x | x | |\n| Union&nbsp;of&nbsp;Primitives | x | x | |\n| Union&nbsp;of&nbsp;Same&nbsp;Types | x | x | |\n| Url | x | x | |\n\n\n## Supported HTTP Headers \n\n| Header | Java | Python | Additional Information |\n|--------|------|----------------|----------------|\n| Content&#8209;Type:&nbsp;PSON | x | | Java's content type can be set to JSON, PSON, or any. In Python's restconstants.py, Content-Type is always set to \"application/json\". | \n| Accept | x | | Java can specify media accepted (e.g. JSON, or PSON). Python does not use an Accept header, so according to RFC, it is assumed all media types are accepted|\n| User&#8209;Agent | | x | This header is optional as per RFC. Java does not use it, while Python uses it| \n| X&#8209;RestLi&#8209;Method | x | x | According to Rest.li protocol, X-RestLi-Method is only required for BATCH_CREATE and BATCH_PARTIAL_UPDATE. Java always includes it for all POST requests, and Python uses the header only when required.|\n\n## Request Format Differences\n\n| Request Feature | Java | Python |\n|--------|------|----------------|\n|Unfilled&nbsp;Optional&nbsp;ActionParams|If optional ActionParam is not set, it is omitted from the serialized data.|If optional ActionParam is not set, it is still explicitly set to null in the serialized body. For an example, see the ```actionset-multiple-inputs-no-optional``` test, in which Python Rest.li includes \" 'optionalString': null\" in the request.|\n| Scheme&nbsp;and&nbsp;Host&nbsp;Components&nbsp;of&nbsp;URL | Request URL is relative, scheme and host can be configured in instantiating a RestClient | URL scheme is hard-coded to http in requesturlbuilders.py, and requests.models.py requires a host |\n",
      tags: null,
      id: 53
    });
    
    

  
    index.add({
      title: "Test Suite - How To",
      category: null,
      content: "How to Run the Java TestNG Tests and Expand the Test Suite Specification\n========================\nThis page is meant to help a new user get familiarized with using the Rest.li Test Suite Specification.\nIt explains how to run the example Java TestNG tests, and how to expand the Test Suite Specification. \n\nFor an overview of the test suite, refer to the [test suite overview](testsuite_overview.md).\n\n\nHow to Run the Java TestNG Tests\n--------------------\n### Test Categories\nThe Rest.li Test Suite Specification is language-independent, and it provides test data and guidelines to follow when\ntesting a Rest.li client.\nTo validate that the spec's tests all pass using the Rest.li Java client binding, *and* to demonstrate how\nto use the Test Suite Specification, this project contains a Java TestNG test suite,\nlocated in the `client-testsuite/src/test/java` folder.\n\nThe Java TestNG test suite follows the Rest.li Test Suite Specification, and it has two types of tests: data driven\nautomated tests and manual assertion tests. \n##### Data Driven Automated Tests\nThese tests are located in the `TestRestClientAgainstStandardTestSuite` class, and the tests run are specified by `manifest.json`.\n\nFor an example, consider the Java schema tests in the Java suite. Our testSchema() method verifies that Rest.li correctly\ngenerates a data template from a schema, and `manifest.json` specifies a list of schema-data pairs. These pairs are passed\nto testSchema() using a TestNG data provider, so all the schema cases specified by `manifest.json` are tested in the Java suite.  \n\n##### Manual Assertion Tests\nThese tests are located in the `TestRestClientWithManualAssertions` class.\nManual assertions are used to verify that Rest.li responses are decoded properly from http responses. Each test verifies\nthe content of the Rest.li response fields. These assertions are built by hand, but they should be similar across test suite languages. \n\n\n### Run the Tests\n\nTo run all tests in the provided TestNG suite:\n\n```\ngradle test\n```\n\nThis command should run successfully, since the Java implementation of the Rest.li client should pass all the tests.  \n\nBecause the build is successful, it's not immediately clear what is happening when ```gradle test```  is run. \nFor better illustration, this guide will help you make tests that fail when run:\n\n* Try adding an untrue assertion to testSchema() in `TestRestClientAgainstStandardTestSuite.java`. \nRunning ```gradle test``` again will result in multiple tests failing, because testSchema() is used for all the schema\ntests defined in `manifest.json`.\n\n\n* Try adding an untrue manual assertion to `TestRestClientWithManualAssertions.java`. For example, the following assertion\nis untrue because the response should have 1 as its id:\n\n```java  \n@Test\npublic void testCollectionCreateAgain() throws Exception\n{\n  Response response = loadResponse(\"collection-create\", \"responses/collection-create.res\");\n\n  Assert.assertEquals(response.getId(), \"2\");\n}\n```\n  \n  Run `gradle test` again, and this test should fail, while the other tests pass.\n\n\n\n\nStep-by-Step Guide to Expand Test Suite Specification\n-------------------------------\nThis section explains how to add a new language-independent test to the Rest.li Test Suite Specification.\nPlease do not modify existing tests: multiple language implementations are using these test and changing existing tests \nmay break their test suites.\n\nKeep tests simple!  They should test a single case and test it well.\n\n### Add a JSON test\nTo add a new JSON corner case, e.g. `corner-case.json`:\n1. In `client-testsuite/src/data/`, add `corner-case.json`.\n2. Find the \"jsonTestData\" list in `manifest.json`, and add a list entry of the form {\"data\": \"data/[filename].json\"}.\n\n```js\n\"jsonTestData\": [\n  ...\n  { \"data\": \"data/corner-case.json\" }\n  ...\n],\n```\n\n### Add a Data Schema test\nTo test a new schema, `NewSchema.pdsc`:\n1. In `client-testsuite/src/schemas/testsuite/`, add a new .pdsc file, `NewSchema.pdsc`, with fields and\n   field types supported by Rest.li. Set \"namespace\" to \"testsuite\". Rest.li client should generate a data template from \n   this .pdsc file. \n2. In `client-testsuite/src/data/`, add a corresponding .json file, `new-schema.json`, with test data to fill your new data template's fields. \n3. In `manifest.json`, find the schemaTestData list. Add an entry for your new schema, following the general format:\n{\"schema\": \"testsuite.[SchemaName]\", \"data\": \"data/[json-name].json\"}\n\n```js\n\"schemaTestData\": [\n  ...\n  {\"schema\": \"testsuite.NewSchema\", \"data\": \"data/new-schema.json\"}\n  ...\n],\n```\n\n4. Run ```gradle build``` before running the test, so Rest.li will generate the data binding.\n\n\n### Add a Wire Protocol test \nWhen adding a new wire protocol test, you also need to add its associated flat `.req` and `.res` files. \n\nThe Java suite contains a convenience tool to generate `.restspec.json`, `.req`, and `.res` files for the\nRest.li Test Suite Specification. To use it:\n\n1. Add or update the `*Resource.java` classes in the `restli-testsuite-server` project under `src/main/java/testsuite.` You can override a new method in an existing resource, or add a completely new resource class. For example, the following class is a simple collection resource that only overrides create() using the option to return the created entity.\n\n```java\n@RestLiCollection(name = \"collectionReturnEntity\", namespace = \"testsuite\")\npublic class CollectionReturnEntityResource extends CollectionResourceTemplate\n{\n\n  @ReturnEntity\n  @Override\n  public CreateKVResponse create(Message entity) {\n\n    if(entity.getMessage().equals(\"test message\"))\n    {\n      return new CreateKVResponse(1l, entity, HttpStatus.S_201_CREATED);\n    }\n    else if(entity.getMessage().equals(\"another message\"))\n    {\n      return new CreateKVResponse(3l, entity, HttpStatus.S_201_CREATED);\n    }\n    else\n    {\n      return new CreateKVResponse(null, entity, HttpStatus.S_404_NOT_FOUND);\n    }\n  }\n}\n```\n\n2. Re-generate the `.restspec.json` and `.snapshot.json` files:\n\n    ```\n    gradle publishRestIdl\n    gradle publishRestSnapshot\n    ```\n\n3. Run the test server using:\n\n    ```\n    gradle JettyRunWar\n    ```\n    This will use the new restspecs to generate or update the appropriate request builders, which are used to make the\n    requests in the following step.\n\n4. Update `RequestResponseTestCases.java` in the language-specific suites of `client-testsuite` by adding a new\n  request and test name to the map of Rest.li requests to be tested. \n\n   For the java implementation, this is done by modifying the builtRequests map in the buildRequests() function,\n   in `client-testsuite/src/test/java/com/linkedin/pegasus/testsuite/RequestResponseTestCases.java`.\n\n   ```java\n    builtRequests.put(\"collectionReturnEntity-create\", new CollectionReturnEntityRequestBuilders(_options).create().input(testMessage).build());\n    ```\n\n5. Re-generate the request and response files.  Files will be written to `requests/` and `responses/`, and to `requests-v2/` and `responses-v2/`:\n\n    ```\n    gradle generateRequestAndResponseFiles\n    ```\n    Note that Java's requestBuilders are generating the request files with the desired output. \n    These flat files can be used to test the other implementations for well-formed requests.\n\n6. Update the \"wireProtocolTestData\" entry of `manifest.json` to include test data references to all the files you've added.\n \n   If overriding a new method for an existing resource: \n\n    - Find name of your resource in the wireProtocolTestData list. Under \"operations\", add test for the new method that you added\n    to `RequestResponseTestCases.java` in Step 4. The new operation test should look something like this, where test name is usually the resource and method, and \"status\" is the expected response status:\n \n```js\n{\n  \"name\": \"collection-get\",\n  \"method\": \"get\",\n  \"request\": \"requests/collection-get.req\",\n  \"response\": \"responses/collection-get.res\",\n  \"status\": 200\n},\n``` \n   \n   If adding a new resource:\n    - Create a new entry for your resource in wireProtocolTestData list. This should follow the general format:\n\n```js\n\"wireProtocolTestData\": [\n  { \n    \"name\": \"collectionReturnEntity\",\n    \"restspec\": \"restspecs/testsuite.collectionReturnEntity.restspec.json\",\n    \"snapshot\": \"snapshots/testsuite.collectionReturnEntity.snapshot.json\",\n    \"operations\": [\n      {\n        \"name\":\"collectionReturnEntity-create\",\n        \"method\": \"create\",\n        \"request\": \"requests/collectionReturnEntity-create.req\",\n        \"response\": \"responses/collectionReturnEntity-create.res\",\n        \"status\": 201\n      }\n   ] \n  },\n  ...\n]\n```\n    - For the list of operation tests, follow the instructions for overriding a new method for an existing resource.\n     \n7. Now that the test suite spec includes new responses, you need to update the manual assertion tests in the\n   language-specific test suites. You should write a manual assertion for decoding each new flat HTTP response. Ensure\n   that the Rest.li client can decode an HTTP response to a Rest.li response,\n   and that the Rest.li response is a correct representation of the HTTP response.\n   \n",
      tags: null,
      id: 54
    });
    
    

  
    index.add({
      title: "How To - Add new language",
      category: null,
      content: "How to Add Tests to a New Client Language Binding\n--------------------------\nThis page explains how to follow the Rest.li Test Suite Specification to validate a new Rest.li client binding.\nThis guide is based on experience with Python, and your language implementation may diverge from this guide.\n\nFor a broader overview of code structure and test coverage, refer to [test suite overview](testsuite_overview.md).\nFor Java client binding example, please refer to the provided Java TestNG suite, which follows the Rest.li Test Suite\nSpecification to validate a Java Rest.li client implementation.\n\n### New Language Set Up\nTo use the Rest.li Test Suite Specification for a new language implementation of Rest.li, you must set up code\ngeneration within the project so you can test Rest.li client's code generation.\nYou must also write methods in the new language so you can use the language-independent data included in the\nRest.li Test Suite Specification.\n\nHere is a step-by-step guide on how to set up a test suite for a new language implementation of Rest.li:\n1.  In client-testsuite's ```build.gradle```, apply the plugin for your new implementation of Rest.li to enable code generation.  \n\n    ```\n    apply plugin: 'li-python-restli'\n    ```\n2.  Run the Rest.li code generators against all the files in the ```schemas/``` directory (for data\n    bindings) and/or the ```restspecs/``` directory (for rest client bindings). \n    For Python and Java, this means running a ```gradle build```. In Java, the code is generated in ```src/```. In Python, \n    the code is generated  in ```build/```\n    \n    The generated code will be tested later in the testsuite.   \n    \n3.  Select a test framework.  You should use the same test framework you usually use for the language your Rest.li\n    client is written in. For the Python test suite, we use pytest. \n\n4.  In your test framework, load the ```manifest.json``` file into a convenient in-memory representation before running the tests.\n\n    In Python, a Manifest class is generated from the ```.pdsc``` provided by the test suite. \n    The ```manifest.json``` file is loaded into a Manifest representation that is used by the Python test suite.\n\n5.  Based on your test framework, decide how you want to use your Manifest representation to drive your automated tests. \n\n    For example, consider json tests in the Python test suite, shown below. We get the list of manifest tests from the\n    Manifest representation in json_test_data(). Then we use the pytest @parametrize decorator to pass the test names\n    and data files into the function test_json(), which contains the testing logic. \n    ```python\n    def json_test_data():\n        \"\"\"Data provider for json_test\"\"\"\n        results = []\n        for test in manifest.get_json_test_data():\n            results.append(test)\n        return results\n    \n    \n    json_data = json_test_data()\n    \n    \n    @pytest.mark.parametrize(\"json_test\", json_test_data(), ids=[test.data_ for test in json_data])\n    def test_json(json_test):\n        \"\"\"Tests for correct serialization/deserialization of provided 'data' files\"\"\"\n        ...\n    ```   \n \n6.  Set up utility methods in your new language. These methods should provide a way to read in the files referenced by\n    ```manifest.json```. You also need methods to compare and load flat HTTP requests and responses from the test suite's\n    provided test data. \n \n    In the Java TestNG suite, these utility methods are part of a test suite base class that is extended by the classes\n    containing actual tests. The Python pytest suite has no subclass/superclass structure because it makes pytest harder to use.\n \nNow your test suite is set up to generate code and read language-independent files from the Rest.li Test Suite\nSpecification. \n\n### Writing Tests with the Provided Test Data\nThe next step is to implement tests using the spec's guidelines and provided test data. \nThis will be a combination of manual assertions and data driven tests specified by ```manifest.json```.\n#### Data Driven Tests\nThe tests are driven by ```manifest.json``` and cover three main categories: \n* JSON: serialization/deserialization of JSON\n* schema: data template generation from schema\n* wire protocol: building HTTP requests and decoding Rest.li responses from HTTP responses\n\n##### JSON Tests \n\nJson tests ensure that Rest.li correctly serializes/deserializes all the provided ```data/``` files. \nThese files include corner cases to check that they are supported.\n \nOne possible approach, which is used by the Java and Python test suites:\n\n1. deserialize a JSON file to an in-memory representation\n2. serialize the in-memory representation back to bytes (or a file)\n3. deserialize the bytes again into another in-memory representation\n4. comparing the two in-memory representations\n(Note that comparing the serialized representations isn't always an option since JSON map entries are unordered)\n\n```python\ndef test_json(json_test):\n    \"\"\"Tests for correct serialization/deserialization of provided 'data' files\"\"\"\n    deserialized = load_json_file(os.path.join(*json_test.get_data_().split(\"/\")))\n    serialized = JSONRestliCodec.encode(deserialized)\n    twice_deserialized = JSONRestliCodec.decode(serialized)\n    for field in deserialized.keys():\n        assert deserialized[field] == twice_deserialized[field]\n    assert len(deserialized) == len(twice_deserialized) \n```  \n \n##### Schema Tests\n\nSchema tests ensure that Rest.li correctly generates language-specific data templates from language-independent data schemas.\n\nFor implementations that use data schemas to provide convenience data bindings from data schemas (either dynamically or\nvia code generation):\n\n1.  Make sure you have run ```gradle build``` to construct data bindings for all the ```schema/``` files.\n2.  Load instance data for each accessor according to the 'data' fields of the 'schemas' in ```manifest.json```.\n3.  Verify the accessor is correctly constructed and provides access to the data correctly, writing custom asserts as needed\n4.  If the schema contains default fields, validate that they are defaulted correctly\n\nFor implementations that provide validation:\n\n1. load instance data for each 'schemas.data' file and validate it against the 'schemas.schema' files.  Note that some data\n   files should fail to validate (and be marked as such in manifest.json).\n\n##### Wire Protocol Tests \n\nWhen testing wire protocol, we want to ensure that the generated request builders can build correct HTTP responses.\nWe also want to ensure that the Rest.li client can correctly create a Rest.li Response representation from an \nHTTP response.  \n\nTo test requests and responses, flat files containing HTTP requests and responses are provided in\nRest.li Test Suite Specification's ```requests/``` and ```responses/``` folders, which are for Rest.li protocol 1.0.0.\nThese flat files have counterparts in ```requests-v2/``` and ```responses-v2/``` that use Rest.li protocol version 2.0.0.\nbut are otherwise identical.\n\nA few possible approaches one might take to use these test files:\n\n1. Organize the Rest.li client code such that a HTTP request can be produced and tested without actually sending it\n   and such that a HTTP response stored in a flat file can be handled as though it was received in response to a request.\n2. Create a simple mock HTTP server that uses the flat files to send the appropriate response back for each request, and\n   returns a clear error message back if the client sends an incorrect request.\n\nThe Java and Python test suites use approach 1.\n\nFor Rest.li implementations that use restspecs to provide convenience rest client bindings from restspecs (either dynamically\nor via code generation):\n\n1. Construct rest client bindings for all the ```restspecs/``` files. \n2. Use the constructed rest client bindings to produce HTTP requests.  Do this for all the 'restspecs.operations.request'\n   entries in ```manifest.json```.  Instead of sending the requests,  keep the request in an in-memory representation (a string,\n   or basic HTTP representation) or write it to a file.\n   \n   In Python, the logic for this is as follows:\n   1. Using the generated request builders, build all the Rest.li requests specified by Manifest. \n   2. For each Rest.li request, use a RequestsRestClient to build an HTTP PreparedRequest. In the next step, we will compare\n   this with the expected HTTP request. \n   \n3. Verify the HTTP request created by the rest client bindings matches the expected HTTP as found in the\n   'restspecs.operations.request' files.  This may require writing a routine to compare HTTP requests that ignores header\n   order, and does not fail for optional headers. For instance, Python includes an optional \"User-Agent\" header, which \n   the flat requests do not include. You may want to ignore such differences in wire protocol. \n   \n4. For each of the 'restspecs.operations.response' files, verify the constructed rest client bindings are able to\n   handle the response correctly (as if it were sent as an actual HTTP response).  Write custom assertions to verify all the\n   fields in the HTTP response are correctly marshaled to the response representation provided by the constructed rest client\n   bindings. See \"Manual Assertions\" for more details about custom test cases. \n\nFor implementations that do not provide convenience rest client bindings:\n\n1. At a minimum, you should provide utilities to serialize data to request URLs and deserialize data from response headers\n  and batch map keys. Write tests to validate that all the ```.req``` files can be produced correctly using these utilities.\n\n#### Manual Assertions\nSome tests cannot be easily automated. For instance, if you want to check the correct value of a field of an in-memory \nrepresentation, you do not want to validate it against another in-memory representation.\nInstead, you can use manual assertions. \n\nExamples of test cases for manual assertions:\n* Test for properly generated data template accessors: The generated templates are filled using test data in ```data/```, \nso assert that the accessors return the correct test data.\n* Test for properly decoded HTTP responses: Rest.li decodes a Rest.li response from an HTTP response. Assert that the\ndecoded fields have the correct data. For instance, check that a returned entity has the proper fields. \n \n \n",
      tags: null,
      id: 55
    });
    
    

  
    index.add({
      title: "Test Suite",
      category: null,
      content: "# Rest.li Cross-Language Test Suite Framework\n\n## Contents\n  - [Introduction](#introduction)\n  - [Motivation](#motivation)\n  - [Getting Started](#getting-started)  \n  - [Design and Code Structure](#design-and-code-structure)\n  - [Next Steps](#next-steps)\n  - [Troubleshooting](#troubleshooting)\n\n## Introduction\n\nThe Rest.li Cross-Language Test Suite is a framework for testing and comparing Rest.li implementations.\nThe framework contains the Rest.li Test Suite Specification, which provides test data\nand testing guidelines that specify which features and scenarios to test, and how to test them.\nThese test guidelines should be followed when testing a particular language implementation of Rest.li. \nCurrently, the framework only supports client testing, though we hope to add server testing support in the future.\n\nThe test suite also includes Java tests that follow the [Rest.li Test Suite Specification](#restli-test-suite-specification), demonstrating how the spec\ncan be used to test the Java implementation of Rest.li client. \n\n\n## Motivation\n\nThe Rest.li Test Suite Specification will help us achieve consistency and quality across multiple Rest.li client\nlanguage bindings by:\n\n* Reducing testing effort required to add a new Rest.li client implementation\n* Standardizing test plans that can be shared among multiple Rest.li client implementations\n* \"Certifying\" Rest.li client implementations with a quantifiable measure of quality\n* Making it easier to develop compatible Rest.li implementations, so that a Rest.li client in one language can be used with a Rest.li server in another language. \n\nWe have leveraged this Rest.li Test Suite Specification to implement tests for Java and Python Rest.li client bindings,\nrespectively. With this sharable test suite spec, we have identified their coverage and feature parity in this \n[Compatibility Matrix](testsuite_compatibility_matrix.md).\n\n## Getting Started\n\n### How to Download Project\nMake sure you have installed [gradle](https://gradle.org/), which is used for building the test suite.\n \nDownload the test suite from its git repository: \n```\ngit clone \ncd restli-testsuite\n```\n\n### How to Run Java Tests and Expand Test Spec\nFor new test suite users, take a look at [How to Run the Java TestNG Tests and Expand the Test Suite Specification](testsuite_how_to.md). This will walk you through running\nthe provided Java TestNG tests. It will also explain how to expand the Rest.li Test Suite Specification.\n\n### How to Add Tests to a New Client Language Binding\nIf you would like to follow the spec and add tests for a new language binding, refer to\n[How to Add Tests to a New Client Language Binding](testsuite_new_language.md). \nThis is a more involved process that requires understanding the spec. It will be helpful to go over the Design and Code\nStructure section of this document first.\n\n\n## Design and Code Structure\n\n### Background\nThis documentation assumes a background in [Rest.li](https://github.com/linkedin/rest.li). Specifically, test developers should be familar with Rest.li data\nschema, IDL, and wire protocol format as demonstrated by the following links: \n\n* Data schema (```.pdl```) - https://linkedin.github.io/rest.li/pdl_schema\n* IDL (```.restspec.json```) - https://linkedin.github.io/rest.li/spec/restspec_format\n* Rest.li Wire Protocol - https://linkedin.github.io/rest.li/spec/protocol\n\nImplementers are encouraged to explore the\ntest data and reference documentation in tandem to learn the details of Rest.li.\n\n### Design Principles\n\nThere are two main ways of interacting with the Rest.li Test Suite Specification: \n\n1. Expanding or updating the Rest.li Test Suite Specification for new Rest.li features or updated Rest.li behavior.\n2. Following the spec to add tests for a Rest.li client binding implemented in a new language.\n\nThese distinct ways of using the Rest.li Test Suite Specification point to an important distinction between language-independent\nand language-specific components within this test suite. The Rest.li Test Suite Specification consists of\nlanguage-independent data and test guidelines, meant to be used for standardizing cross-platform testing. \nIt cannot immediately be used to test a specific language implementation of Rest.li client. As a test developer,\nyou follow the Rest.li Test Suite Specification to write tests in your desired language. \n\nThroughout the documentation, we use \"Rest.li Test Suite Specification\" to refer to the language-independent\ncomponents of the project. We use \"Java TestNG Tests\" or \"Python pytest Tests\" for its language-dependent components, which\nfollow the Test Suite Specification.\n\n### Components \nThis test suite is composed of:\n\n* Rest.li Test Suite Specification  \n    * Data files in language-neutral formats such as ```.json```, ```.pdsc```,```.restspec.json```.\n    * A ```manifest.json``` file containing all the test data provided in this suite to help drive automated test execution.\n    * Guidelines on how to use the spec to validate a Rest.li client implementation.\n* A Java TestNG suite that uses the spec's test data to validate the Java Rest.li client implementation.\n* A Rest.li Java server for wire protocol data generation and potential integration testing\n\n\nThe test suite is structured as follows: \n```\n.\n|—— client-testsuite\n|   |—— manifest.json\n|   |—— data/\n|   |—— requests/\n|   |—— requests-v2/\n|   |—— responses/\n|   |—— responses-v2/\n|   |—— restspecs/\n|   |—— schemas/\n|   |—— snapshots/\n|   |—— src/\n|   |—— ...\n|—— restli-testsuite-server\n|   |—— src\n|   |—— ...\n|—— build.gradle\n|... \n```\n\n\n#### Rest.li Test Suite Specification\n\n##### manifest.json\nThis manifest file provides machine readable information about all the automated tests included in this spec.\nIt is shared across languages, and does not need to be changed when adding a test suite for a new language. \nIt should be changed when expanding the Rest.li Test Suite Specification.\n\nThis file is intended to help drive portions of the test suite execution for each language. Although it does help,\nwriting additional assertions by hand will still be needed in most languages to validate correctness of in-memory\nrepresentations and language bindings.\n\nThe file is broken down into a few main sections:\n\n* jsonTestData - list of JSON data files.\n* schemaTestData - list of Rest.li data schema files (```.pdsc```) as well as JSON data files matching the schemas.\n* wireProtocolTestData - list of Rest.li interface definition files (```.restspec.json```) as well as test HTTP requests and responses,\n  in the form of files, for operations supported by the interface definition.\n\n##### Test data folders\nThese folders contain testing data used by the automated tests in the spec. Most of them were generated by Java Rest.li,\nand should be used when following the spec to test your Rest.li client implementation. \n\nThe following folders are included:\n* **data**: input JSON data for testing schemas and json serialization\n* **requests**: incoming HTTP requests for wire protocol tests (Rest.li protocol 1.0)\n* **requests-v2**: incoming HTTP requests for wire protocol tests (Rest.li protocol 2.0)\n* **responses**: expected HTTP responses for wire protocol tests (Rest.li protocol 1.0)\n* **responses-v2**: expected HTTP responses for wire protocol tests (Rest.li protocol 2.0)\n* **restspecs**: IDL (Interface Description Language) generated from Java resources. These should be used by Rest.li implementations\nto make request builders.\n* **snapshots**: snapshots for resource compatibility checker\n\n#### Java TestNG Tests\nIn ```src/test/java```, you can find the Java test suite, which uses the TestNG testing framework. \nYou can add test suites for new languages in ```src/test/```. The Java suite is separated into two folders:\n* **test**: Java files containing the tests (```TestRestClientAgainstStandardTestSuite``` and ```TestRestClientWithManualAssertions```), and utility methods (```StandardTestSuiteBase```) for running tests. \n\n* **testsuite**: Java files for building and loading requests and responses.\n\n#### Sample Rest.li server \nThe ```restli-testsuite-server``` directory contains code for a Rest.li Java server. Based on java resources defined there,\nJava Rest.li will generate language-independent restspecs in the ```client-testsuite/restspecs``` folder. Using these restspecs,\nother Rest.li implementations can generate request builders without modifying server code.\nWhen adding a new test, you may want to update or add a resource to ```restli-testsuite-server/src/main/java/testsuite```.\nSee the section on wire protocol tests in [How To Run the Java TestNG Tests and Expand the Test Suite Specification](testsuite_how_to.md).\n\n## Rest.li Test Suite Specification Coverage\n\nThe test suite spec is intended to cover three categories of Rest.li client behavior: **JSON serialization, data template\ngeneration, and wire protocol**. Each test category is described in more detail below. \n  \n### JSON Tests\nThese are tests for serialization and deserialization. Tests cover basic JSON and JSON corner cases such as large\nnumbers, special characters and encodings.\n\n\n| JSON feature | Details |\n|--------------|---------|\n| Basic&nbsp;Types| string, number, boolean, null, object, array|\n| Empty&nbsp;Collections| empty object, empty array, object with empty arrays, object with empty objects, array of empty object, array of empty arrays| \n| Large&nbsp;Numbers |int32 and int64|\n| Special&nbsp;Characters| periods in key|\n| Unicode| Chinese character and e with an accent mark|\n\n\n### Data Schema Tests\nThese are tests for data template generation from schema. Tests cover schema types (records, unions, enums, typerefs,\n...), primitive types, optionals and defaults. Backward compatibility rules are also covered.\n                                                            \n\n| Schema feature | Details|\n|------------------|--------|\n| Primitive&nbsp;Types|int, long, float, double, bytes, string |\n| Complex&nbsp;Types | array of maps, map of ints, record |\n| Complex&nbsp;Type: Unions | union of complex types, union of primitives, union of same types |\n| Enums | with properties and with alias|\n| Fixed&nbsp;Type | |\n| Typerefs | for string, array, chained typeref, array, map, field, union|\n| Include | include, and include with include |\n| Default&nbsp;Fixup | to see if required fields are \"fixed up\" with defaults|\n| Optional&nbsp;Fields | |\n\n### Wire Protocol Tests\nThese are tests for building requests and decoding responses. Tests cover serializing/deserializing of URLs, \nheaders and bodies, escaping/encoding, batch formats, projections, and partial updates.\n\nWe test for well-formed requests by comparing the built HTTP request\nwith the expected HTTP request in the ```requests/``` or ```requests-v2/``` folder. We compare url, method, headers, and body.\nWe test that Rest.li can decode a Rest.li response from an HTTP response by checking the decoded Rest.li response for\nthe correct values. We compare the response's status and error message with the status and error message specified by \n```manifest.json```. The body of the response is tested through manual assertions that check for the correct values.\n\n#### Basic Resource Method Tests for Requests/Responses\n\n| Rest.li Method | Collection | Simple | Association | Action Set|\n|--------|------|----------------|----------------|------------------|\n| get | x | x | x | |\n| batch-get | x |  | x | |\n| finder | x | | x | |\n| batch-finder | o | | o | |\n| create | x | | | |\n| create&nbsp;with&nbsp;returned&nbsp;entity | x | | | |\n| batch-create | x | | | |\n| update | x | x | x | |\n| partial update| x | |o| |\n| batch-update| x |  | x | |\n| batch-partial-update| x| | o | |\n| delete | x | x | x | | \n| batch-delete | x | | x | | \n| action | o |  o | o | x | \n\n\"x\" - test is included in Rest.li Test Suite Specification  \n\"o\" - test is not included but method should be supported by the resource  \n\" \" - test is not included and method should NOT be supported by resource\n\n\n#### Resource Key and Parameter Tests\n\n| Key Feature | Rest.li Method used|\n|-------------|---------------|\n|Key&nbsp;with&nbsp;Union&nbsp;of&nbsp;Simple&nbsp;and&nbsp;Complex&nbsp;Members | get | \n|Query&nbsp;Params (int, string, long, string array, message array, string map, primitive union, complex types union, optional string, url typeref)| get | \n|Complex&nbsp;Key (simple record) | get, create, delete, update batch-create, batch-delete, batch-get, batch-update, partial-update |\n|Special&nbsp;Chars&nbsp;in&nbsp;ComplexKey&nbsp;Strings | get, batch-get|\n \n\n#### Error tests\n\n| Error | Resource used | Rest.li Method | Details |\n|-----|-----|----|----|\n|404 | Collection | get | Send empty get request|\n|400 | Collection | update | Request has a missing required field | \n|500 | Collection | create | Create request with id field |\n|Error&nbsp;Details | Collection | create | CreateResponse with Error Details| \n|Batch&nbsp;Results&nbsp;with&nbsp;Errors | Collection | batch_update | Batch update with one good and two bad requests| \n\n\n#### Misc. Tests\n\n| Feature | Resource tested| Method used|\n|---------|---------|------|\n| Typeref | Collection, Association | get |\n| Subresource | Collection, Association | get |\n| Projection | Collection, Association, ComplexKeyResource | get | \n\n## Next Steps\n\n### Improvements to Test Suite Specification\n* Add test for record with lowercase name \n* Add test for tunneled query params\n* Add test that includes unicode (non-ascii) characters on wire\n* Extend spec to include cross-language server testing\n\n### Gaps in example Java Tests\n* Enrich manual assertions for wire protocol and schema tests\n* Fill in the gaps listed in the Base Resource Method Tests table \n\n### Future Work\n* Expand reference implementations of spec to different languages, such as mobile.\n\n## Troubleshooting\n\nFor questions or troubleshooting, refer to [Test Suite Troubleshooting](testsuite_troubleshooting.md). \n",
      tags: null,
      id: 56
    });
    
    

  
    index.add({
      title: "Test suite - Troubleshooting",
      category: null,
      content: "Test Suite Troubleshooting\n================\n\n### My language-specific test suite doesn't run at all:\nWhile the tests specified by the test suite spec are helpful for finding incompatibilities between Rest.li\nimplementations, some incompatibilities prevent the project from building. When adding a new language or expanding the\nRest.li Test Suite Specification, it may be necessary to omit some of the spec's tests, schemas, or resources so that\nthe rest of the suite can run properly. The omitted features should be noted as incompatibilities.\n\n##### DataTemplate generation fails\nA .pdsc schema may not be supported by your Rest.li implementation. Ignore this schema when generating code so that the \ncode generation does not fail and stop. \n\n\n##### RequestBuilder generation fails\nIn other languages, requestBuilders are generated from language-independent restspecs for a particular resources. \nThis generation can fail (causing your build to fail) if the resource is not supported in your Rest.li implementation. \nIf this is the case, you can skip requestBuilder generation for that resource, and skip wire protocol tests that use that\nrequestBuilder. \n\n##### Building requests in RequestResponseTestCases (or equivalent file) fails\nRequests are built before running wire protocol tests, so an error here will prevent all the tests from running. Make \nsure the attributes of the request you are building are actually supported by your language. \n\n### My language-specific test suite runs but certain tests fail:\n\n##### Consistent Behavior Incompatibilities\nFailing tests can indicate that your Rest.li implementation has some incompatibilities with Java Rest.li. However, some\nincompatibilities have no effect on Rest.li performance. For example, consider wire protocol requests.\nWhen comparing the flat HTTP request with your Rest.li built request, you should not look for a carbon copy. Some differences\nare acceptable. For instance, order may be different, or your implementation may use an optional header that is not included\nin the flat HTTP request.\nIf your wire protocol tests fail due to acceptable differences, you may wish to note the incompatibility and \nmake your tests more lenient. \n \n##### Inconsistent Behavior Incompatibilities\nFailing tests can also indicate incompatibilities that affect behavior. \nFor instance, the keywithunion wire protocol test fails because Python Rest.li does not support complex key\nwith params, while Java Rest.li does. This test should fail, as it indicates a gap in Python Rest.li's implementation of\nthe Rest.li Protocol.\n\n### I want to test an internal Rest.li implementation:\nThis project is open source, which may cause integration problems if your implementation is internal.\nInstead of adding a language-specific test suite to the project, you may need to import the Rest.li Test Suite Specification\nfolders into an internal project. Please refer to Design and Code Structure in \n[test suite overview](testsuite_overview.md) to know which language-independent files and folders are part of the test\nsuite spec.",
      tags: null,
      id: 57
    });
    
    

  
    index.add({
      title: "Unstructured Data in Rest.li Quick Start",
      category: null,
      content: "#  Unstructured Data in Rest.li Quick Start\n\n## Contents\n\n - [Introduction](#introduction)\n - [Serve Unstructured Data](#serve-unstructured-data)\n - [Consume Unstructured Data](#consume-unstructured-data)\n - [Recap](#recap)\n\n## Introduction\nThis tutorial demonstrates how to serve unstructured binary data, such as Binary Large Object (BLOB), in a Rest.li server. It will show how to define a Rest.li resource that responds with fortune reports (in PDF format) for GET requests and how to consume the GET response from a HTTP client.\n\nThis tutorial assumes that you already have a working Rest.li server. Otherwise, follow the [Rest.li Quick Start Guide](/rest.li/get_started/quick_start) before you continue.\n\n## Serve Unstructured Data\nWe start by defining a resource class on the server side by extending the provided CollectionUnstructuredDataResourceTemplate with the generic type of our resource key as String. Notice that, different from a regular Rest.li resource interface/template that also requires a value type, an unstructured data resource doesn’t require one. Next, we annotate the resource with @RestLiCollection and specify the required resource name and namespace:\n\n```\n@RestLiCollection(name = \"fortuneReports\", namespace = \"com.example.fortune\")\npublic class FortuneReportsResource extends CollectionUnstructuredDataResourceTemplate\n{\n  @Override\n  public void get(String key, @UnstructuredDataWriterParam UnstructuredDataWriter writer)\n  {\n    byte[] fortuneReportPDF = fortuneReportDB.get(key);  // Fetch the data from source\n    writer.setContentType(\"application/pdf\");            // Set the proper MIME content-type\n    writer.getOutputStream().write(fortuneReportPDF);    // Output the data into response\n  }\n}\n```\n\nWe then implement the GET method simply by overriding from the template class. We obtain the requested fortune report PDF from source (in bytes) and use the UnstructuredDataWriter instance given by the method parameter to return the response.\n\nUnstructuredDataWriter provides a setter for the required Content-Type header and an OutputStream instance for writing the binary data that goes into the response content. The final response will then be sent to the client after the GET method is successfully returned.\n\n## Consume Unstructured Data\nThe response wire format contains the Content-Type header and content payload exactly as they were specified in the GET method without any alterations from the Rest.li framework. This means the response can be consumed directly by any HTTP client with no special client-side handling required.\n\nExample: Using the GET endpoint in a HTML anchor\n\n```\n\n```\n\nWhen the link is clicked, the browser receive the PDF response and render the PDF inline or as file-download depends on the resource implementation.\n\nExample: Calling a local deployed GET endpoint with curl\n```\n$ curl -v http://localhost:1338/fortuneReports/1\n...\nHTTP/1.1 200 OK\nContent-Type: application/pdf\nContent-Length: 5\nContent: >>\n```\n\nCurrently, the Rest.li client doesn’t have support for unstructured data resource. Request builders aren’t generated for unstructured data resources.\n\n\n## Recap\nAs you can see, serving unstructured data in Rest.li is very easy. Defining a resource for unstructured data is similar to how you define a regular Rest.li resource for Records with one exception. Instead of returning records, you respond with writing the data to an OutputStream.\n\nYou can learn more about unstructured data support in [Rest.li User Guide](/rest.li/user_guide/restli_server).",
      tags: null,
      id: 58
    });
    
    

  
    index.add({
      title: "Unstructured data (blob) user guide",
      category: null,
      content: "# Unstructured Data (BLOB) User Guide\n\n## Contents\n\n* [Summary](#summary)\n* [About Unstructured Data](#about)\n* [Features Overview](#features)\n* [Unstructured Data Resources](#create)\n* [Consume Unstructured Data](#consume)\n* [FAQs](#faq)\n\n\n## Summary\n\nRest.li applications are built around *Resources*. The key ingredient to creating a resource is _data model_, whose internal structure is defined by [Pegasus Data Schema](/rest.li/pdl_schema) in key-values style. A fundamental presumption was that such structure exists for every Rest.li data model. However, it's not the case for _unstructured_ data such as images or PDFs, which are usually consumed in raw binary forms without a containing data structure.\n\nThis user guide is about working with _unstructured data_ in Rest.li framework. This is _not_ a comprehensive guide to building Rest.li resources in general, which is already covered in great details at [Writing Resources](/rest.li/user_guide/restli_server#writing-resources). This guide focuses on the differences of unstructured data resource.\n\nSee also [Unstructured Data in Rest.li Quick Start](/rest.li/start/unstructured).\n\n## About Unstructured Data\n\nTo Rest.li, the key difference about unstructured data is that they don't have any defined schema and don't have to be represented by a single generated class in Rest.li like schema-base data does (RecordTemplate). Unstructured data can be handled in the rawest form as a bytes array or a more advanced form as InputStream/ByteBuffer in Java for example.\n\nAdditionally, there are several other differences that set them apart from the typical structured data:\n\n* **Different Wire Protocol.** Rest.li transports _structured_ data as JSON content. But _unstructured_ data should be transported in their own MIME type with the body contains only the binaries.\n* **Larger Data Size.** Unstructured data are usually larger in sheer size. Buffering the entire payload in system memory may not be a good idea and not as necessary as for structured data (in order for the codec to work).\n* **Different Data Handling.** Application logic is much less likely to have reasons to peek into or even mutate unstructured data on the fly. Once the data is minted or fetched, they should remain immutable during transportation. (except maybe for special handling like compression/decompression which is taken care by the framework anyway)\n* **Different Types of Client.** The query for unstructured data is usually initiated by end-user clients in some native manner. A good example is that a web browser could initiate a binary upload/download without invoking any JavaScript logic.\n* **Breakable.** Unstructured data can be broken down and processed as a series of byte chunks. This is perfect for streaming which is necessary to reduce the memory footprint.\n\n\n## Features Overview\n\nBy default, unstructured data enjoys the same level of support as structured data in Rest.li: they can be modeled as various resource types and most resource-supporting features and tooling should work. **However**, because of the lack of RecordTemplate-based data model, any feature that works on the structure of the resource value, such as Field Projections, Entity Validation etc, do _not_ apply to unstructured data resources, although they will continue to work with structured data resources that live in the same Rest.li application.\n\n**Features Highlights:**\n\n* Model as Collection, Association or Simple (Singleton) with both Sync and Async I/O\n* Model as sub-resource of structured data resources\n* Download/Get, Post/Upload, Put and Delete methods\n* Rest.li and R2 Filters\n* Unstructured Data Streaming\n* Generated Rest APIs documentation (limited)\n\n**Not Supported Features:**\n\n* Model as Action or Free-Form resource\n* Model as parent-resource\n* APIs of BATCH_*, FINDERS, PARTIAL_UPDATES\n* Field projections (skipped)\n* Decoration\n\n**Streaming Support**\n\nIn this context, _streaming_ means the ability to transport and process _unstructured data_ in small chunks without the need to buffering the whole content in memory. It sounds appealing, but it introduces complexities for the app developers that might be unnecessary in most simple use cases. Therefore, Rest.li supports both non-streaming and streaming method.\n\n\n## Unstructured Data Resources\n\n\n### Base Resource Interfaces\n\nBase interface determines the resource type, the resource key and value type. Unstructured data has its own set of base interfaces. The main difference is the absence of the resource value type. Each resource type has two variants: Non-Streaming and Streaming version. Non-streaming comes with synchronous and asynchronous style, while there is no such distinction for streaming.\n\n**Non-Streaming base interfaces**\n- Collection\n    - `UnstructuredDataCollectionResource`\n    - Async\n        - `UnstructuredDataCollectionResourceAsync`\n        - `UnstructuredDataCollectionResourceTask`\n        - `UnstructuredDataCollectionResourcePromise`\n- Association\n    - `UnstructuredDataAssociationResource`\n    - Async\n        - `UnstructuredDataAssociationResourceAsync`\n        - `UnstructuredDataAssociationResourceTask`\n        - `UnstructuredDataAssociationResourcePromise`\n- Simple\n    - `UnstructuredDataSimpleResource`\n    - Async\n        - `UnstructuredDataSimpleResourceAsync`\n        - `UnstructuredDataSimpleResourceTask`\n        - `UnstructuredDataSimpleResourcePromise`\n\n**Streaming base interfaces**\n* `UnstructuredDataCollectionResourceReactive`\n* `UnstructuredDataAssociationResourceReactive` \n* `UnstructuredDataSimpleResourceReactive`\n\n\n### Working with Streaming Resources\n\n**Highlights:**\n* Streaming resources are _relatively_ more complicated to write, maintain and debug. It's recommended for high performance demand. Still, performance gain with using streaming is not guaranteed. Benchmarking is the best way to tell. \n* Creating a streaming resource doesn't automatically make it end-to-end streaming across network nodes. It only guarantees that no buffering will happen within the Rest.li application that host this resource. For example, the data source must provide a way to allow partial data fetching, same to the other end: the destination node must be able to consume the data partially.\n* Rest.li streaming adapts the [EntityStream interface](https://github.com/linkedin/rest.li/wiki/EntityStream)\n\n\n#### Resource Definition\n\nThe definition of streaming unstructured data resource is similar to a regular resource. However, no value type is needed.\n\n```java\n@RestLiCollection(name = \"resumes\", namespace = \"com.mycompany\")\npublic class ResumesResource extends UnstructuredDataCollectionResourceReactiveTemplate { ... }\n```\n\n#### Download/Get API\n\nThe interface of streaming resource is similar to the [asynchronous resources](/rest.li/Asynchronous-Servers-and-Clients-in-Rest_li) with a callback parameter that's used to return the result.\n\n```java\n@Override\npublic void get(String resumeId, @CallbackParam Callback callback) {\n    Writer writer = new SingletonWriter(ByteString.copy(UNSTRUCTURED_DATA_BYTES));\n    callback.onSuccess(new UnstructuredDataReactiveResult(EntityStreams.newEntityStream(writer), MIME_TYPE));\n}\n```\n\n**Get Response**\n\n_UnstructuredDataReactiveResult_ represents the download response which encapsulates the unstructured data EntityStream as well as the metadata needed to return a successful response. Its merely a container and could be subclassed if desires.\n\n**Writing Unstructured Data**\n\nStreaming requires the data to be read/write in continuous chunks manner. Simple bytes array or InputStream won't do the job. Rest.li adopt the [EntityStream interface](https://github.com/linkedin/rest.li/wiki/EntityStream)\n\nByteString is essentially Rest.li's immutable bytes array implementation and is used here to represent a single _chunk_. EntityStream is the interface that provides the chunks when they are requested. Note that the chunk size is not enforced, however, it's recommended to make the size reasonable and consistent.\n\n*Writing Unstructured Data w. R2 Writer*\n\nRest.li's R2 layer has its own similar EntityStream implementation. If a writer is already provided, it can be easily converted to Rest.Li Writer using [EntityStreamAdapters](https://github.com/linkedin/rest.li/blob/master/r2-core/src/main/java/com/linkedin/r2/message/stream/entitystream/adapter/EntityStreamAdapters.java) util. \n\n```java\nWriter dataWriter = new ResumeDataWriter(id);\ncom.linkedin.entitystream.Writer writer = EntityStreamAdapters.toGenericWriterx(dataWriter);\n```\n\n**Setting the Content-Type**\n\nA content-specific [MIME content-type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types) is required for the unstructured response to be handled correctly by its clients. It is required as part of the _UnstructuredDataReactiveResult_ and is used _as it-is_ in the HTTP response header. No validation is done by Rest.li.\n\n**Setting Additional Headers**\n\nMore headers/metadata can be set using the ResourceContext. Here is an example to add a 'disposition' header to the response:\n\n```java\ngetContext().setResponseHeader(\"Content-Disposition\", \"attachment; filename=\\\"filename.jpg\\\"\");\n```\n\n\n### Rest.li Filters and Unstructured Data\n\nRest.li [filters](/rest.li/Rest_li-Filters) currently don't support access to the unstructured data payload. Any existing or new filter that tries to access the payload will get an empty record. (No, they won't just fail.)\n\n\n### Resource IDL for Unstructured Data\n\nResource IDLs are also generated for unstructured data resources, with a few minor differences in the generated IDL and Restspec files:\n\n* A new \"entityType\" field to indicate the resource entity type being UNSTRUCTURED_DATA\n* The existing \"schema\" field is empty when the \"entityType\" field is UNSTRUCTURED_DATA\n\n\n\n### Online Documentation\n\nRest.li generates online [API documentation](/rest.li/user_guide/restli_server#online-documentation) for every resource. It also works for unstructured data resource, however, in the API page, unstructured data is currently treated as an empty missing model.\n\n\n## Consume Unstructured Data\n\n**Highlights**\n* Unstructured data resources are designed to be invoked by any HTTP client natively without the need of decoding/unwrapping by a _rich client_ like Rest.li's RestClient. In fact, no request builders are generated at all for unstructured data resources. \n* Use a D2 client directly for use cases that need the dynamic host feature from RestClient.\n\n\n### Response Anatomy\n\nA simple unstructured data GET response:\n\n```sh\ncurl 'http://myhost/resumes/1'\n\nHTTP/1.1 200 OK\nContent-Type: application/pdf\nContent:\n>>\n```\n\n\n### Using an Http Client\n\nOne common Http client is a native browser (not the JavaScript client _lives_ in a browser). Unstructured data resource endpoints can be used in place wherever a standard web resource link is expected.\n\n```html\n\n  Download Resume\n\n```\n\n### Using a D2 Client\n\n[D2](/rest.li/Dynamic_Discovery) is what powers the host finding capability of RestClient under the hood. With a [D2 client](/rest.li/start/d2_quick_start#step-3-create-a-client), you can send a request without having to specify the actual hostname of your resources:\n\n```java\nURI uri = URI.create(\"d2://resumes/1\");\nStreamRequest req = new StreamRequestBuilder(uri).build(...);\nd2Client.streamRequest(req, responseCallback)\n...\n```\n\n### Error Handlings\n\nA request to unstructured data resource could fail in the [same ways](/rest.li/user_guide/restli_server#returning-errors) a regular resource does. Moreover, even when a request such as Get is successful, the data flow could still be interrupted or timeout. When that happens, you could receive a successful HTTP status like 200 and still get an incomplete response or long hanging that results in a client timeout.\n\n\n## FAQs\n\n* *Q: Is there a size limit on how large an unstructured data could be served by Rest.li resource?*\nA: No. But practically, the size will be cap base on your server's timeout value. If you are seeing incompleted content on the client, it could be caused by an undersize server timeout value.\n\n* *Q: What should be a reasonable server timeout?*\nA: It depends on if the Rest.li application hosts a mix of structured and unstructured resources. Currently, Rest.li only allow one timeout setting for the entire app. You may not want a long timeout for APIs that serve small structured data. On the other hand, a short timeout for APIs that serve large unstructured data.\n\n* *Q: Should I create a streaming or non-streaming resource for my unstructured data?*\nA: First of all, streaming doesn't come for free and true end-to-end streaming also depends on your other nodes in the network, so make sure you understand what you are getting into. Secondly, the performance depends on many factors such as the size of the data and I/O performances etc.\n\n* *Q: What is reactive streaming and how can I leverage it?*\nA: [EntityStream](https://github.com/linkedin/rest.li/wiki/EntityStream)\n",
      tags: null,
      id: 59
    });
    
    

  

  


var store = [
  

  
    {
      "title": "Asynchronous Servers and Clients In Rest.li",
      "excerpt": "Rest.li is asynchronous and non-blocking under the hood. This section covers asynchronous servers and slients in Rest.li",
      "link": "/Asynchronous-Servers-and-Clients-in-Rest_li",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Attachment Streaming",
      "excerpt": null,
      "link": "/Attachment-Streaming",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Components",
      "excerpt": "Rest.li is the top layer of a larger software stack code named 'pegasus'.  Pegasus is comprised of the following major components. Data, generator, r2, d2 and restli",
      "link": "/Components",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li compression",
      "excerpt": "Rest.li compression",
      "link": "/Compression",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "D2 Zookeeper configuration properties",
      "excerpt": "D2 Zookeeper configuration properties",
      "link": "/D2-Zookeeper-Properties",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li FAQ",
      "excerpt": "Rest.li FAQ",
      "link": "/Data-FAQ",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Dynamic Discovery (D2)",
      "excerpt": "Dynamic Discovery (D2) is a layer of indirection similar to DNS for the rest.li framework.",
      "link": "/Dynamic_Discovery",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Guice dependency injection with Rest.li",
      "excerpt": "Guice dependency injection can be used with Rest.li",
      "link": "/Guice-Dependency-Injection",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Projections in Java",
      "excerpt": "Projections are a way for a client to request only specific fields from an object instead of the entire object. Using projections when the client needs a few fields from an object is a good way to self-document the co...",
      "link": "/How-to-use-projections-in-Java",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Projections",
      "excerpt": "Rest.li projection",
      "link": "/Projections",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Request Response API (R2)",
      "excerpt": "R2 is the request / response API underlying Rest.li.  It includes abstractions for REST requests and responses, filter chains for customized processing, and transport abstraction. It is designed so that it can be easi...",
      "link": "/Request-Response-API-(R2)",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Request Response Framework",
      "excerpt": "Pegasus's request/response framework, often called R2,  includes abstractions for REST and RPC requests and responses, filter chains for customized processing, and transport abstraction.",
      "link": "/Request-Response-Framework",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li 2.0 response API",
      "excerpt": "Rest.li 2.0 response API",
      "link": "/Rest_li-2_0-response-API",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li-2.x-upgrade-instructions",
      "excerpt": null,
      "link": "/Rest_li-2_x-upgrade-instructions",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Filters",
      "excerpt": "Rest.li provides a mechanism to intercept incoming requests and outgoing responses via filters. Each Rest.li filter contains methods that handle both requests and responses.",
      "link": "/Rest_li-Filters",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li with Netty",
      "excerpt": "Rest.li can be run in a variety of HTTP frameworks. Out of the box, Rest.li supports both Netty and Servlet containers, such as Jetty.",
      "link": "/Rest_li-with-Netty",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li with servlet containers",
      "excerpt": "Rest.li may be run in a variety of http frameworks. Out of the box, Rest.li supports both Netty and Servlet containers, such as Jetty.",
      "link": "/Rest_li-with-Servlet-Containers",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li to Avro conversions",
      "excerpt": "Sometimes it is necessary to convert between Avro and Rest.li formats. That is, either converting schemas or converting data. Rest.li provides ways to do this using the data-avro module.",
      "link": "/Rest_li_Avro_conversions",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Scala-Integration",
      "excerpt": null,
      "link": "/Scala-Integration",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Send Rest.li request query in request Body",
      "excerpt": "Rest.li protocol specifies what HTTP method will be used for each type of Rest.li request. However, sometimes due to security constraint or jetty buffer limitation, it may be required to customize the HTTP method used...",
      "link": "/Send-Rest_li-Request-Query-In-Request-Body",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Spring dependency injection with Rest.li",
      "excerpt": "Spring dependency injection can be used with Rest.li.",
      "link": "/Spring-Dependency-Injection",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Validation in Rest.li",
      "excerpt": "Rest.li validation",
      "link": "/Validation-in-Rest_li",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Writing-unit-tests-for-Rest.li-clients-and-servers",
      "excerpt": null,
      "link": "/Writing-unit-tests-for-Rest_li-clients-and-servers",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Introduction of Annotation Processor",
      "excerpt": "Documentation of using annotation processor in Rest.li",
      "link": "/annotation_processor",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "API reference",
      "excerpt": null,
      "link": "/reference/architecture",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Avro Translation",
      "excerpt": "Rest.li Avro translation.",
      "link": "/avro_translation",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Batch Finder Resource Method",
      "excerpt": "This documentation describes how restli framework support batch of search requirement.",
      "link": "/batch_finder_resource_method",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Building Rest.li from Source",
      "excerpt": "How to build Rest.li from Source",
      "link": "/setup/building",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Snapshots and Resource Compatibility Checking",
      "excerpt": "Rest.li uses a form of expanded IDLs, called Snapshots, to keep track of the state of resources and check compatibility between resource iterations.",
      "link": "/modeling/compatibility_check",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "How to contribute to Rest.li",
      "excerpt": "Contributions to Rest.li are welcome!",
      "link": "/contribute/howto",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li D2 Dynamic discovery tutorial",
      "excerpt": "Rest.li D2 Dynamic discovery tutorial",
      "link": "/start/d2_quick_start",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Quick Start Guide",
      "excerpt": "Rest.li Quick Start Guide. Follow the steps below to try Rest.li quickly and get a basic idea of how it works",
      "link": "/get_started/quick_start",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Gradle build integration",
      "excerpt": null,
      "link": "/setup/gradle",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li client user guide",
      "excerpt": "The Rest.li client framework provides support for accessing resources defined using Rest.li. The client framework consists of a request builder and a Rest client",
      "link": "/user_guide/restli_client",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li server user guide",
      "excerpt": "This document describes Rest.li support for implementing servers.",
      "link": "/user_guide/restli_server",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li architecture user guide",
      "excerpt": "This document describes how to use Rest.li to build RESTful clients and servers. The first section introduces key architectural elements and provides an overview of the development process. The remainder of the docume...",
      "link": "/user_guide/server_architecture",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Configuring Service Errors in Java",
      "excerpt": "This page describes how to configure service errors for a resource in Java.",
      "link": "/user_guide/service_errors_java",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "How Data is Represented in Memory",
      "excerpt": "Rest.li how data is represented in memory.",
      "link": "/how_data_is_represented_in_memory",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "How Data is Serialized for Transport",
      "excerpt": "Rest.li how data is serialized for transport.",
      "link": "/how_data_is_serialized_for_transport",
      "image": null,
      "date": null,
      "category": null
    },
  

  

  
    {
      "title": "Rest.li - A framework for building RESTful architectures at scale",
      "excerpt": null,
      "link": "/",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Installation",
      "excerpt": null,
      "link": "/setup/",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Java Binding",
      "excerpt": "Rest.li Java binding.",
      "link": "/java_binding",
      "image": null,
      "date": null,
      "category": null
    },
  

  

  
    {
      "title": "Configure Max Batch Size in Java",
      "excerpt": "This page describes how to set up max batch size limitation for resource batch methods in Java.",
      "link": "/max_batch_size",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Modeling Resources with Rest.li",
      "excerpt": "This document will describe how to model a resource using Rest.li.",
      "link": "/modeling/modeling",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.Li PathSpecs",
      "excerpt": "Rest.Li PathSpecs",
      "link": "/pathspec",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Migrating from PDSC to PDL",
      "excerpt": "Guide for migrating from PDSC to PDL with a side-by-side comparison showing notable differences.",
      "link": "/pdl_migration",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "PDL Schema",
      "excerpt": "Documentation of Pegasus schemas and specification of the PDL syntax.",
      "link": "/pdl_schema",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "PDSC Syntax",
      "excerpt": "Rest.li PDSC Syntax.",
      "link": "/pdsc_syntax",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li Protocol",
      "excerpt": "Rest.li Protocol",
      "link": "/spec/protocol",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Return entity in Rest.li",
      "excerpt": "This page describes returning the entity for resource methods that are not originally intended to return the entity.",
      "link": "/spec/return_entity",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Service Errors",
      "excerpt": "This page describes how service errors are returned by Rest.li and how they are documented in a resource's IDL.",
      "link": "/spec/service_errors",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Rest.li restspec (IDL) format",
      "excerpt": "Rest.li restspec (IDL) format",
      "link": "/spec/restspec_format",
      "image": null,
      "date": null,
      "category": null
    },
  

  

  

  
    {
      "title": "Quickstart - A Tutorial Introduction to Rest.li",
      "excerpt": "Quickstart - A step by step tutorial introduction to Rest.li. Create a Rest.li Server and Client.",
      "link": "/start/step_by_step",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Multi-language Compatibility Matrix",
      "excerpt": null,
      "link": "/multi_language_compatibility_matrix",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Test Suite - How To",
      "excerpt": null,
      "link": "/testsuite_how_to",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "How To - Add new language",
      "excerpt": null,
      "link": "/howto_add_new_language",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Test Suite",
      "excerpt": null,
      "link": "/test_suite",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Test suite - Troubleshooting",
      "excerpt": null,
      "link": "/testsuite_troubleshooting",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Unstructured Data in Rest.li Quick Start",
      "excerpt": "This tutorial demonstrates how to serve unstructured binary data, such as Binary Large Object (BLOB), in a Rest.li server. It will show how to define a Rest.li resource that responds with fortune reports (in PDF forma...",
      "link": "/start/unstructured",
      "image": null,
      "date": null,
      "category": null
    },
  

  
    {
      "title": "Unstructured data (blob) user guide",
      "excerpt": "This user guide is about working with unstructured data (BLOB) in Rest.li framework.",
      "link": "/user_guide/unstructured_data",
      "image": null,
      "date": null,
      "category": null
    },
  

  

  
]

$(document).ready(() => {
  const urlParams = new URLSearchParams(window.location.search);
  const query = urlParams.get('term');

  if (query) {
    // Fill the query value back into the search bar
    $('#search-input').val(query);

    // Perform the search using the query param
    performSearch(query);
  }
});

function performSearch(query){
  const results = index.search(query);
  const resultsDiv = $('#results');
  resultsDiv.empty();

  if (results && results.length !== 0) {
    // Populate the search results list
    const itemMapperFunction = (item) => {
      const storeItem = store[item.ref];
      return `<li class="result-item"><h3><a href="/rest.li${storeItem.link}" class="post-title">${storeItem.title}</a></h3>` +
             `<p class="markdown-body">${storeItem.excerpt || ''}</p></li>`;
    };
    resultsDiv.append(`<h1>Search results for <b>${query}</b></h1><div class="result"><ul>${results.map(itemMapperFunction).join('')}</ul></div>`);
  } else {
    // Indicate that no results were found
    resultsDiv.prepend(`<h1>No results for <b>${query}</b></h1>`);
  }
}
